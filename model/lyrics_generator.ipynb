{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle # to save/load objects\n",
    "import sys\n",
    "sys.path.append(\"../helper/\") # go to parent dir\n",
    "from functions_preprocess import *\n",
    "from functions_model import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Here, I'm getting, cleaning and processing the data. I intentionally mixed the bands with different characteristics. I thought this may prevent overfitting and provide better word pool. Data has the almost entire discography of the bands, even demos for some.  \n",
    "\n",
    "I'm also creating two dictionaries here, `vocab_to_int` and `int_to_vocab`. The model need numerical representation of the characters to be able to compute weights, biases, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memesljuirmoblteovanpaicmaanexdr ready with 3079342 chars\n"
     ]
    }
   ],
   "source": [
    "# prepare training data\n",
    "artist_list = ['metallica', 'megadeth', 'slayer', 'judaspriest', 'ironmaiden', 'motorhead', \n",
    "               'blacksabbath', 'testament', 'overkill', 'anthrax', 'pantera', 'icedearth',\n",
    "               'manowar', 'annihilator', 'exodus', 'dreamtheater']\n",
    "text = ''\n",
    "folder_name = ''\n",
    "for i in artist_list:\n",
    "    text = text + combine_songs(i)\n",
    "    folder_name = folder_name + i[:2] # I know this's not the greatest folder name\n",
    "    \n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(folder_name, \"ready with\", len(chars), \"chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving\n",
    "I need to save the objects I created so far. Remember, no orders in Python dictionaries. This means this notebook creates different dictionaries in each session. Because I need the use my model later, I have to save these lookup dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the objects\n",
    "f = open('checkpoints/{}/vars.pckl'.format(folder_name), 'wb')\n",
    "pickle.dump([text, vocab, vocab_to_int, int_to_vocab, chars, artist_list], f, protocol=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly encourage you to check the function `split_data` in `functions_model.py`. The target is the next character in the sequence.  \n",
    "`x = chars[: n_batches*slice_size]`  \n",
    "`y = chars[1: n_batches*slice_size + 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training and validation sets\n",
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200) # default fraction for trainning is 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40  Iteration 1/11040 Training loss: 4.4761 0.9269 sec/batch\n",
      "Epoch 1/40  Iteration 2/11040 Training loss: 4.4324 0.4878 sec/batch\n",
      "Epoch 1/40  Iteration 3/11040 Training loss: 4.2446 0.4667 sec/batch\n",
      "Epoch 1/40  Iteration 4/11040 Training loss: 4.3949 0.4858 sec/batch\n",
      "Epoch 1/40  Iteration 5/11040 Training loss: 4.3521 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 6/11040 Training loss: 4.2877 0.4763 sec/batch\n",
      "Epoch 1/40  Iteration 7/11040 Training loss: 4.2215 0.4735 sec/batch\n",
      "Epoch 1/40  Iteration 8/11040 Training loss: 4.1513 0.4875 sec/batch\n",
      "Epoch 1/40  Iteration 9/11040 Training loss: 4.0862 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 10/11040 Training loss: 4.0323 0.4958 sec/batch\n",
      "Epoch 1/40  Iteration 11/11040 Training loss: 3.9828 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 12/11040 Training loss: 3.9386 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 13/11040 Training loss: 3.9028 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 14/11040 Training loss: 3.8712 0.4815 sec/batch\n",
      "Epoch 1/40  Iteration 15/11040 Training loss: 3.8411 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 16/11040 Training loss: 3.8139 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 17/11040 Training loss: 3.7889 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 18/11040 Training loss: 3.7658 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 19/11040 Training loss: 3.7447 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 20/11040 Training loss: 3.7252 0.4815 sec/batch\n",
      "Epoch 1/40  Iteration 21/11040 Training loss: 3.7072 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 22/11040 Training loss: 3.6899 0.4805 sec/batch\n",
      "Epoch 1/40  Iteration 23/11040 Training loss: 3.6748 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 24/11040 Training loss: 3.6614 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 25/11040 Training loss: 3.6478 0.4795 sec/batch\n",
      "Epoch 1/40  Iteration 26/11040 Training loss: 3.6345 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 27/11040 Training loss: 3.6215 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 28/11040 Training loss: 3.6103 0.4813 sec/batch\n",
      "Epoch 1/40  Iteration 29/11040 Training loss: 3.5996 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 30/11040 Training loss: 3.5896 0.4833 sec/batch\n",
      "Epoch 1/40  Iteration 31/11040 Training loss: 3.5803 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 32/11040 Training loss: 3.5711 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 33/11040 Training loss: 3.5623 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 34/11040 Training loss: 3.5538 0.4834 sec/batch\n",
      "Epoch 1/40  Iteration 35/11040 Training loss: 3.5461 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 36/11040 Training loss: 3.5385 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 37/11040 Training loss: 3.5316 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 38/11040 Training loss: 3.5248 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 39/11040 Training loss: 3.5179 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 40/11040 Training loss: 3.5121 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 41/11040 Training loss: 3.5060 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 42/11040 Training loss: 3.5000 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 43/11040 Training loss: 3.4947 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 44/11040 Training loss: 3.4899 0.4834 sec/batch\n",
      "Epoch 1/40  Iteration 45/11040 Training loss: 3.4847 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 46/11040 Training loss: 3.4800 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 47/11040 Training loss: 3.4753 0.4755 sec/batch\n",
      "Epoch 1/40  Iteration 48/11040 Training loss: 3.4704 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 49/11040 Training loss: 3.4657 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 50/11040 Training loss: 3.4613 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 51/11040 Training loss: 3.4575 0.4813 sec/batch\n",
      "Epoch 1/40  Iteration 52/11040 Training loss: 3.4542 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 53/11040 Training loss: 3.4516 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 54/11040 Training loss: 3.4488 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 55/11040 Training loss: 3.4458 0.4855 sec/batch\n",
      "Epoch 1/40  Iteration 56/11040 Training loss: 3.4424 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 57/11040 Training loss: 3.4396 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 58/11040 Training loss: 3.4368 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 59/11040 Training loss: 3.4341 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 60/11040 Training loss: 3.4311 0.4813 sec/batch\n",
      "Epoch 1/40  Iteration 61/11040 Training loss: 3.4282 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 62/11040 Training loss: 3.4250 0.4807 sec/batch\n",
      "Epoch 1/40  Iteration 63/11040 Training loss: 3.4219 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 64/11040 Training loss: 3.4191 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 65/11040 Training loss: 3.4163 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 66/11040 Training loss: 3.4133 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 67/11040 Training loss: 3.4105 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 68/11040 Training loss: 3.4079 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 69/11040 Training loss: 3.4055 0.4834 sec/batch\n",
      "Epoch 1/40  Iteration 70/11040 Training loss: 3.4031 0.4793 sec/batch\n",
      "Epoch 1/40  Iteration 71/11040 Training loss: 3.4003 0.4775 sec/batch\n",
      "Epoch 1/40  Iteration 72/11040 Training loss: 3.3977 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 73/11040 Training loss: 3.3950 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 74/11040 Training loss: 3.3928 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 75/11040 Training loss: 3.3906 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 76/11040 Training loss: 3.3883 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 77/11040 Training loss: 3.3858 0.4835 sec/batch\n",
      "Epoch 1/40  Iteration 78/11040 Training loss: 3.3835 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 79/11040 Training loss: 3.3810 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 80/11040 Training loss: 3.3784 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 81/11040 Training loss: 3.3759 0.4813 sec/batch\n",
      "Epoch 1/40  Iteration 82/11040 Training loss: 3.3733 0.4777 sec/batch\n",
      "Epoch 1/40  Iteration 83/11040 Training loss: 3.3710 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 84/11040 Training loss: 3.3686 0.4795 sec/batch\n",
      "Epoch 1/40  Iteration 85/11040 Training loss: 3.3664 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 86/11040 Training loss: 3.3641 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 87/11040 Training loss: 3.3619 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 88/11040 Training loss: 3.3597 0.4805 sec/batch\n",
      "Epoch 1/40  Iteration 89/11040 Training loss: 3.3575 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 90/11040 Training loss: 3.3555 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 91/11040 Training loss: 3.3536 0.4833 sec/batch\n",
      "Epoch 1/40  Iteration 92/11040 Training loss: 3.3517 0.4806 sec/batch\n",
      "Epoch 1/40  Iteration 93/11040 Training loss: 3.3496 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 94/11040 Training loss: 3.3480 0.4863 sec/batch\n",
      "Epoch 1/40  Iteration 95/11040 Training loss: 3.3460 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 96/11040 Training loss: 3.3440 0.4803 sec/batch\n",
      "Epoch 1/40  Iteration 97/11040 Training loss: 3.3422 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 98/11040 Training loss: 3.3404 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 99/11040 Training loss: 3.3385 0.4783 sec/batch\n",
      "Epoch 1/40  Iteration 100/11040 Training loss: 3.3365 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 101/11040 Training loss: 3.3347 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 102/11040 Training loss: 3.3329 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 103/11040 Training loss: 3.3308 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 104/11040 Training loss: 3.3292 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 105/11040 Training loss: 3.3274 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 106/11040 Training loss: 3.3257 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 107/11040 Training loss: 3.3242 0.4806 sec/batch\n",
      "Epoch 1/40  Iteration 108/11040 Training loss: 3.3225 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 109/11040 Training loss: 3.3208 0.4834 sec/batch\n",
      "Epoch 1/40  Iteration 110/11040 Training loss: 3.3191 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 111/11040 Training loss: 3.3178 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 112/11040 Training loss: 3.3160 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 113/11040 Training loss: 3.3147 0.4746 sec/batch\n",
      "Epoch 1/40  Iteration 114/11040 Training loss: 3.3132 0.4793 sec/batch\n",
      "Epoch 1/40  Iteration 115/11040 Training loss: 3.3120 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 116/11040 Training loss: 3.3109 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 117/11040 Training loss: 3.3097 0.4735 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40  Iteration 118/11040 Training loss: 3.3084 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 119/11040 Training loss: 3.3071 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 120/11040 Training loss: 3.3060 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 121/11040 Training loss: 3.3049 0.4766 sec/batch\n",
      "Epoch 1/40  Iteration 122/11040 Training loss: 3.3037 0.4745 sec/batch\n",
      "Epoch 1/40  Iteration 123/11040 Training loss: 3.3025 0.4793 sec/batch\n",
      "Epoch 1/40  Iteration 124/11040 Training loss: 3.3013 0.4763 sec/batch\n",
      "Epoch 1/40  Iteration 125/11040 Training loss: 3.3001 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 126/11040 Training loss: 3.2987 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 127/11040 Training loss: 3.2974 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 128/11040 Training loss: 3.2960 0.4766 sec/batch\n",
      "Epoch 1/40  Iteration 129/11040 Training loss: 3.2947 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 130/11040 Training loss: 3.2933 0.4733 sec/batch\n",
      "Epoch 1/40  Iteration 131/11040 Training loss: 3.2919 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 132/11040 Training loss: 3.2904 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 133/11040 Training loss: 3.2890 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 134/11040 Training loss: 3.2875 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 135/11040 Training loss: 3.2861 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 136/11040 Training loss: 3.2849 0.4825 sec/batch\n",
      "Epoch 1/40  Iteration 137/11040 Training loss: 3.2833 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 138/11040 Training loss: 3.2817 0.4770 sec/batch\n",
      "Epoch 1/40  Iteration 139/11040 Training loss: 3.2801 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 140/11040 Training loss: 3.2786 0.4772 sec/batch\n",
      "Epoch 1/40  Iteration 141/11040 Training loss: 3.2772 0.4776 sec/batch\n",
      "Epoch 1/40  Iteration 142/11040 Training loss: 3.2757 0.5084 sec/batch\n",
      "Epoch 1/40  Iteration 143/11040 Training loss: 3.2742 0.4904 sec/batch\n",
      "Epoch 1/40  Iteration 144/11040 Training loss: 3.2724 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 145/11040 Training loss: 3.2706 0.4775 sec/batch\n",
      "Epoch 1/40  Iteration 146/11040 Training loss: 3.2689 0.4756 sec/batch\n",
      "Epoch 1/40  Iteration 147/11040 Training loss: 3.2673 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 148/11040 Training loss: 3.2656 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 149/11040 Training loss: 3.2638 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 150/11040 Training loss: 3.2621 0.4775 sec/batch\n",
      "Epoch 1/40  Iteration 151/11040 Training loss: 3.2605 0.4753 sec/batch\n",
      "Epoch 1/40  Iteration 152/11040 Training loss: 3.2588 0.4783 sec/batch\n",
      "Epoch 1/40  Iteration 153/11040 Training loss: 3.2570 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 154/11040 Training loss: 3.2551 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 155/11040 Training loss: 3.2533 0.4793 sec/batch\n",
      "Epoch 1/40  Iteration 156/11040 Training loss: 3.2515 0.4793 sec/batch\n",
      "Epoch 1/40  Iteration 157/11040 Training loss: 3.2495 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 158/11040 Training loss: 3.2477 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 159/11040 Training loss: 3.2460 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 160/11040 Training loss: 3.2442 0.4745 sec/batch\n",
      "Epoch 1/40  Iteration 161/11040 Training loss: 3.2424 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 162/11040 Training loss: 3.2404 0.4734 sec/batch\n",
      "Epoch 1/40  Iteration 163/11040 Training loss: 3.2385 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 164/11040 Training loss: 3.2366 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 165/11040 Training loss: 3.2347 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 166/11040 Training loss: 3.2329 0.4733 sec/batch\n",
      "Epoch 1/40  Iteration 167/11040 Training loss: 3.2309 0.4743 sec/batch\n",
      "Epoch 1/40  Iteration 168/11040 Training loss: 3.2288 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 169/11040 Training loss: 3.2265 0.4763 sec/batch\n",
      "Epoch 1/40  Iteration 170/11040 Training loss: 3.2244 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 171/11040 Training loss: 3.2223 0.4775 sec/batch\n",
      "Epoch 1/40  Iteration 172/11040 Training loss: 3.2201 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 173/11040 Training loss: 3.2180 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 174/11040 Training loss: 3.2159 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 175/11040 Training loss: 3.2137 0.4795 sec/batch\n",
      "Epoch 1/40  Iteration 176/11040 Training loss: 3.2115 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 177/11040 Training loss: 3.2092 0.4786 sec/batch\n",
      "Epoch 1/40  Iteration 178/11040 Training loss: 3.2070 0.4753 sec/batch\n",
      "Epoch 1/40  Iteration 179/11040 Training loss: 3.2048 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 180/11040 Training loss: 3.2024 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 181/11040 Training loss: 3.2000 0.4753 sec/batch\n",
      "Epoch 1/40  Iteration 182/11040 Training loss: 3.1978 0.4783 sec/batch\n",
      "Epoch 1/40  Iteration 183/11040 Training loss: 3.1953 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 184/11040 Training loss: 3.1930 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 185/11040 Training loss: 3.1905 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 186/11040 Training loss: 3.1882 0.4786 sec/batch\n",
      "Epoch 1/40  Iteration 187/11040 Training loss: 3.1857 0.4824 sec/batch\n",
      "Epoch 1/40  Iteration 188/11040 Training loss: 3.1833 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 189/11040 Training loss: 3.1807 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 190/11040 Training loss: 3.1780 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 191/11040 Training loss: 3.1755 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 192/11040 Training loss: 3.1727 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 193/11040 Training loss: 3.1700 0.4747 sec/batch\n",
      "Epoch 1/40  Iteration 194/11040 Training loss: 3.1673 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 195/11040 Training loss: 3.1647 0.4755 sec/batch\n",
      "Epoch 1/40  Iteration 196/11040 Training loss: 3.1621 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 197/11040 Training loss: 3.1594 0.4754 sec/batch\n",
      "Epoch 1/40  Iteration 198/11040 Training loss: 3.1567 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 199/11040 Training loss: 3.1541 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 200/11040 Training loss: 3.1513 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 201/11040 Training loss: 3.1486 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 202/11040 Training loss: 3.1459 0.4775 sec/batch\n",
      "Epoch 1/40  Iteration 203/11040 Training loss: 3.1432 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 204/11040 Training loss: 3.1405 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 205/11040 Training loss: 3.1376 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 206/11040 Training loss: 3.1348 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 207/11040 Training loss: 3.1320 0.4786 sec/batch\n",
      "Epoch 1/40  Iteration 208/11040 Training loss: 3.1293 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 209/11040 Training loss: 3.1265 0.4744 sec/batch\n",
      "Epoch 1/40  Iteration 210/11040 Training loss: 3.1237 0.4765 sec/batch\n",
      "Epoch 1/40  Iteration 211/11040 Training loss: 3.1209 0.4783 sec/batch\n",
      "Epoch 1/40  Iteration 212/11040 Training loss: 3.1181 0.4813 sec/batch\n",
      "Epoch 1/40  Iteration 213/11040 Training loss: 3.1153 0.4793 sec/batch\n",
      "Epoch 1/40  Iteration 214/11040 Training loss: 3.1126 0.4885 sec/batch\n",
      "Epoch 1/40  Iteration 215/11040 Training loss: 3.1099 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 216/11040 Training loss: 3.1072 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 217/11040 Training loss: 3.1044 0.4805 sec/batch\n",
      "Epoch 1/40  Iteration 218/11040 Training loss: 3.1016 0.4813 sec/batch\n",
      "Epoch 1/40  Iteration 219/11040 Training loss: 3.0988 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 220/11040 Training loss: 3.0963 0.4777 sec/batch\n",
      "Epoch 1/40  Iteration 221/11040 Training loss: 3.0937 0.4834 sec/batch\n",
      "Epoch 1/40  Iteration 222/11040 Training loss: 3.0910 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 223/11040 Training loss: 3.0884 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 224/11040 Training loss: 3.0858 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 225/11040 Training loss: 3.0832 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 226/11040 Training loss: 3.0806 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 227/11040 Training loss: 3.0778 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 228/11040 Training loss: 3.0750 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 229/11040 Training loss: 3.0724 0.4857 sec/batch\n",
      "Epoch 1/40  Iteration 230/11040 Training loss: 3.0698 0.4774 sec/batch\n",
      "Epoch 1/40  Iteration 231/11040 Training loss: 3.0672 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 232/11040 Training loss: 3.0645 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 233/11040 Training loss: 3.0618 0.4784 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40  Iteration 234/11040 Training loss: 3.0591 0.4795 sec/batch\n",
      "Epoch 1/40  Iteration 235/11040 Training loss: 3.0564 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 236/11040 Training loss: 3.0537 0.4785 sec/batch\n",
      "Epoch 1/40  Iteration 237/11040 Training loss: 3.0510 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 238/11040 Training loss: 3.0484 0.4815 sec/batch\n",
      "Epoch 1/40  Iteration 239/11040 Training loss: 3.0459 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 240/11040 Training loss: 3.0433 0.4794 sec/batch\n",
      "Epoch 1/40  Iteration 241/11040 Training loss: 3.0407 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 242/11040 Training loss: 3.0381 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 243/11040 Training loss: 3.0354 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 244/11040 Training loss: 3.0327 0.4773 sec/batch\n",
      "Epoch 1/40  Iteration 245/11040 Training loss: 3.0301 0.4783 sec/batch\n",
      "Epoch 1/40  Iteration 246/11040 Training loss: 3.0276 0.4844 sec/batch\n",
      "Epoch 1/40  Iteration 247/11040 Training loss: 3.0251 0.4775 sec/batch\n",
      "Epoch 1/40  Iteration 248/11040 Training loss: 3.0227 0.4743 sec/batch\n",
      "Epoch 1/40  Iteration 249/11040 Training loss: 3.0202 0.4834 sec/batch\n",
      "Epoch 1/40  Iteration 250/11040 Training loss: 3.0178 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 251/11040 Training loss: 3.0152 0.4815 sec/batch\n",
      "Epoch 1/40  Iteration 252/11040 Training loss: 3.0127 0.4783 sec/batch\n",
      "Epoch 1/40  Iteration 253/11040 Training loss: 3.0103 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 254/11040 Training loss: 3.0080 0.4825 sec/batch\n",
      "Epoch 1/40  Iteration 255/11040 Training loss: 3.0055 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 256/11040 Training loss: 3.0031 0.4795 sec/batch\n",
      "Epoch 1/40  Iteration 257/11040 Training loss: 3.0006 0.4784 sec/batch\n",
      "Epoch 1/40  Iteration 258/11040 Training loss: 2.9982 0.4804 sec/batch\n",
      "Epoch 1/40  Iteration 259/11040 Training loss: 2.9957 0.4803 sec/batch\n",
      "Epoch 1/40  Iteration 260/11040 Training loss: 2.9933 0.4814 sec/batch\n",
      "Epoch 1/40  Iteration 261/11040 Training loss: 2.9909 0.4763 sec/batch\n",
      "Epoch 1/40  Iteration 262/11040 Training loss: 2.9886 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 263/11040 Training loss: 2.9864 0.4764 sec/batch\n",
      "Epoch 1/40  Iteration 264/11040 Training loss: 2.9841 0.4730 sec/batch\n",
      "Epoch 1/40  Iteration 265/11040 Training loss: 2.9817 0.4851 sec/batch\n",
      "Epoch 1/40  Iteration 266/11040 Training loss: 2.9795 0.4636 sec/batch\n",
      "Epoch 1/40  Iteration 267/11040 Training loss: 2.9771 0.4755 sec/batch\n",
      "Epoch 1/40  Iteration 268/11040 Training loss: 2.9748 0.4749 sec/batch\n",
      "Epoch 1/40  Iteration 269/11040 Training loss: 2.9725 0.4718 sec/batch\n",
      "Epoch 1/40  Iteration 270/11040 Training loss: 2.9704 0.4697 sec/batch\n",
      "Epoch 1/40  Iteration 271/11040 Training loss: 2.9683 0.4781 sec/batch\n",
      "Epoch 1/40  Iteration 272/11040 Training loss: 2.9662 0.4676 sec/batch\n",
      "Epoch 1/40  Iteration 273/11040 Training loss: 2.9640 0.4637 sec/batch\n",
      "Epoch 1/40  Iteration 274/11040 Training loss: 2.9617 0.4763 sec/batch\n",
      "Epoch 1/40  Iteration 275/11040 Training loss: 2.9594 0.4629 sec/batch\n",
      "Epoch 1/40  Iteration 276/11040 Training loss: 2.9571 0.4770 sec/batch\n",
      "Epoch 2/40  Iteration 277/11040 Training loss: 2.4108 0.4624 sec/batch\n",
      "Epoch 2/40  Iteration 278/11040 Training loss: 2.3705 0.4676 sec/batch\n",
      "Epoch 2/40  Iteration 279/11040 Training loss: 2.3583 0.4868 sec/batch\n",
      "Epoch 2/40  Iteration 280/11040 Training loss: 2.3641 0.4652 sec/batch\n",
      "Epoch 2/40  Iteration 281/11040 Training loss: 2.3612 0.4888 sec/batch\n",
      "Epoch 2/40  Iteration 282/11040 Training loss: 2.3550 0.4632 sec/batch\n",
      "Epoch 2/40  Iteration 283/11040 Training loss: 2.3463 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 284/11040 Training loss: 2.3448 0.4753 sec/batch\n",
      "Epoch 2/40  Iteration 285/11040 Training loss: 2.3418 0.4648 sec/batch\n",
      "Epoch 2/40  Iteration 286/11040 Training loss: 2.3425 0.4805 sec/batch\n",
      "Epoch 2/40  Iteration 287/11040 Training loss: 2.3424 0.4824 sec/batch\n",
      "Epoch 2/40  Iteration 288/11040 Training loss: 2.3409 0.4741 sec/batch\n",
      "Epoch 2/40  Iteration 289/11040 Training loss: 2.3401 0.4718 sec/batch\n",
      "Epoch 2/40  Iteration 290/11040 Training loss: 2.3386 0.4882 sec/batch\n",
      "Epoch 2/40  Iteration 291/11040 Training loss: 2.3364 0.4641 sec/batch\n",
      "Epoch 2/40  Iteration 292/11040 Training loss: 2.3349 0.4872 sec/batch\n",
      "Epoch 2/40  Iteration 293/11040 Training loss: 2.3321 0.4643 sec/batch\n",
      "Epoch 2/40  Iteration 294/11040 Training loss: 2.3310 0.4590 sec/batch\n",
      "Epoch 2/40  Iteration 295/11040 Training loss: 2.3297 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 296/11040 Training loss: 2.3283 0.4755 sec/batch\n",
      "Epoch 2/40  Iteration 297/11040 Training loss: 2.3283 0.4651 sec/batch\n",
      "Epoch 2/40  Iteration 298/11040 Training loss: 2.3278 0.4781 sec/batch\n",
      "Epoch 2/40  Iteration 299/11040 Training loss: 2.3275 0.4738 sec/batch\n",
      "Epoch 2/40  Iteration 300/11040 Training loss: 2.3283 0.4801 sec/batch\n",
      "Epoch 2/40  Iteration 301/11040 Training loss: 2.3267 0.4737 sec/batch\n",
      "Epoch 2/40  Iteration 302/11040 Training loss: 2.3242 0.4680 sec/batch\n",
      "Epoch 2/40  Iteration 303/11040 Training loss: 2.3215 0.4877 sec/batch\n",
      "Epoch 2/40  Iteration 304/11040 Training loss: 2.3198 0.4640 sec/batch\n",
      "Epoch 2/40  Iteration 305/11040 Training loss: 2.3189 0.4726 sec/batch\n",
      "Epoch 2/40  Iteration 306/11040 Training loss: 2.3186 0.4802 sec/batch\n",
      "Epoch 2/40  Iteration 307/11040 Training loss: 2.3175 0.4718 sec/batch\n",
      "Epoch 2/40  Iteration 308/11040 Training loss: 2.3158 0.4838 sec/batch\n",
      "Epoch 2/40  Iteration 309/11040 Training loss: 2.3152 0.4668 sec/batch\n",
      "Epoch 2/40  Iteration 310/11040 Training loss: 2.3148 0.4877 sec/batch\n",
      "Epoch 2/40  Iteration 311/11040 Training loss: 2.3145 0.4821 sec/batch\n",
      "Epoch 2/40  Iteration 312/11040 Training loss: 2.3135 0.4739 sec/batch\n",
      "Epoch 2/40  Iteration 313/11040 Training loss: 2.3129 0.4786 sec/batch\n",
      "Epoch 2/40  Iteration 314/11040 Training loss: 2.3122 0.4738 sec/batch\n",
      "Epoch 2/40  Iteration 315/11040 Training loss: 2.3110 0.4769 sec/batch\n",
      "Epoch 2/40  Iteration 316/11040 Training loss: 2.3109 0.4595 sec/batch\n",
      "Epoch 2/40  Iteration 317/11040 Training loss: 2.3098 0.4651 sec/batch\n",
      "Epoch 2/40  Iteration 318/11040 Training loss: 2.3083 0.4777 sec/batch\n",
      "Epoch 2/40  Iteration 319/11040 Training loss: 2.3080 0.4741 sec/batch\n",
      "Epoch 2/40  Iteration 320/11040 Training loss: 2.3077 0.4712 sec/batch\n",
      "Epoch 2/40  Iteration 321/11040 Training loss: 2.3070 0.4777 sec/batch\n",
      "Epoch 2/40  Iteration 322/11040 Training loss: 2.3065 0.4809 sec/batch\n",
      "Epoch 2/40  Iteration 323/11040 Training loss: 2.3054 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 324/11040 Training loss: 2.3037 0.4738 sec/batch\n",
      "Epoch 2/40  Iteration 325/11040 Training loss: 2.3020 0.4745 sec/batch\n",
      "Epoch 2/40  Iteration 326/11040 Training loss: 2.3015 0.4749 sec/batch\n",
      "Epoch 2/40  Iteration 327/11040 Training loss: 2.3010 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 328/11040 Training loss: 2.2997 0.4810 sec/batch\n",
      "Epoch 2/40  Iteration 329/11040 Training loss: 2.2991 0.4751 sec/batch\n",
      "Epoch 2/40  Iteration 330/11040 Training loss: 2.2980 0.4783 sec/batch\n",
      "Epoch 2/40  Iteration 331/11040 Training loss: 2.2970 0.4687 sec/batch\n",
      "Epoch 2/40  Iteration 332/11040 Training loss: 2.2961 0.4858 sec/batch\n",
      "Epoch 2/40  Iteration 333/11040 Training loss: 2.2949 0.4897 sec/batch\n",
      "Epoch 2/40  Iteration 334/11040 Training loss: 2.2942 0.4755 sec/batch\n",
      "Epoch 2/40  Iteration 335/11040 Training loss: 2.2939 0.4778 sec/batch\n",
      "Epoch 2/40  Iteration 336/11040 Training loss: 2.2929 0.4733 sec/batch\n",
      "Epoch 2/40  Iteration 337/11040 Training loss: 2.2920 0.4753 sec/batch\n",
      "Epoch 2/40  Iteration 338/11040 Training loss: 2.2905 0.4881 sec/batch\n",
      "Epoch 2/40  Iteration 339/11040 Training loss: 2.2894 0.4766 sec/batch\n",
      "Epoch 2/40  Iteration 340/11040 Training loss: 2.2887 0.4786 sec/batch\n",
      "Epoch 2/40  Iteration 341/11040 Training loss: 2.2878 0.4719 sec/batch\n",
      "Epoch 2/40  Iteration 342/11040 Training loss: 2.2866 0.4869 sec/batch\n",
      "Epoch 2/40  Iteration 343/11040 Training loss: 2.2853 0.4763 sec/batch\n",
      "Epoch 2/40  Iteration 344/11040 Training loss: 2.2841 0.4733 sec/batch\n",
      "Epoch 2/40  Iteration 345/11040 Training loss: 2.2832 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 346/11040 Training loss: 2.2827 0.4816 sec/batch\n",
      "Epoch 2/40  Iteration 347/11040 Training loss: 2.2815 0.4675 sec/batch\n",
      "Epoch 2/40  Iteration 348/11040 Training loss: 2.2804 0.4674 sec/batch\n",
      "Epoch 2/40  Iteration 349/11040 Training loss: 2.2790 0.4888 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40  Iteration 350/11040 Training loss: 2.2782 0.4841 sec/batch\n",
      "Epoch 2/40  Iteration 351/11040 Training loss: 2.2771 0.4788 sec/batch\n",
      "Epoch 2/40  Iteration 352/11040 Training loss: 2.2761 0.4790 sec/batch\n",
      "Epoch 2/40  Iteration 353/11040 Training loss: 2.2751 0.4816 sec/batch\n",
      "Epoch 2/40  Iteration 354/11040 Training loss: 2.2743 0.4773 sec/batch\n",
      "Epoch 2/40  Iteration 355/11040 Training loss: 2.2730 0.4656 sec/batch\n",
      "Epoch 2/40  Iteration 356/11040 Training loss: 2.2717 0.4750 sec/batch\n",
      "Epoch 2/40  Iteration 357/11040 Training loss: 2.2702 0.4809 sec/batch\n",
      "Epoch 2/40  Iteration 358/11040 Training loss: 2.2687 0.4752 sec/batch\n",
      "Epoch 2/40  Iteration 359/11040 Training loss: 2.2674 0.4832 sec/batch\n",
      "Epoch 2/40  Iteration 360/11040 Training loss: 2.2658 0.4768 sec/batch\n",
      "Epoch 2/40  Iteration 361/11040 Training loss: 2.2645 0.4655 sec/batch\n",
      "Epoch 2/40  Iteration 362/11040 Training loss: 2.2634 0.4751 sec/batch\n",
      "Epoch 2/40  Iteration 363/11040 Training loss: 2.2622 0.4859 sec/batch\n",
      "Epoch 2/40  Iteration 364/11040 Training loss: 2.2607 0.4793 sec/batch\n",
      "Epoch 2/40  Iteration 365/11040 Training loss: 2.2597 0.4762 sec/batch\n",
      "Epoch 2/40  Iteration 366/11040 Training loss: 2.2588 0.4705 sec/batch\n",
      "Epoch 2/40  Iteration 367/11040 Training loss: 2.2582 0.4900 sec/batch\n",
      "Epoch 2/40  Iteration 368/11040 Training loss: 2.2573 0.4763 sec/batch\n",
      "Epoch 2/40  Iteration 369/11040 Training loss: 2.2563 0.4878 sec/batch\n",
      "Epoch 2/40  Iteration 370/11040 Training loss: 2.2557 0.4835 sec/batch\n",
      "Epoch 2/40  Iteration 371/11040 Training loss: 2.2552 0.5018 sec/batch\n",
      "Epoch 2/40  Iteration 372/11040 Training loss: 2.2544 0.4777 sec/batch\n",
      "Epoch 2/40  Iteration 373/11040 Training loss: 2.2541 0.4778 sec/batch\n",
      "Epoch 2/40  Iteration 374/11040 Training loss: 2.2533 0.4786 sec/batch\n",
      "Epoch 2/40  Iteration 375/11040 Training loss: 2.2524 0.4757 sec/batch\n",
      "Epoch 2/40  Iteration 376/11040 Training loss: 2.2513 0.4857 sec/batch\n",
      "Epoch 2/40  Iteration 377/11040 Training loss: 2.2505 0.4907 sec/batch\n",
      "Epoch 2/40  Iteration 378/11040 Training loss: 2.2498 0.4645 sec/batch\n",
      "Epoch 2/40  Iteration 379/11040 Training loss: 2.2487 0.4765 sec/batch\n",
      "Epoch 2/40  Iteration 380/11040 Training loss: 2.2481 0.4751 sec/batch\n",
      "Epoch 2/40  Iteration 381/11040 Training loss: 2.2470 0.4746 sec/batch\n",
      "Epoch 2/40  Iteration 382/11040 Training loss: 2.2464 0.4706 sec/batch\n",
      "Epoch 2/40  Iteration 383/11040 Training loss: 2.2460 0.4662 sec/batch\n",
      "Epoch 2/40  Iteration 384/11040 Training loss: 2.2455 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 385/11040 Training loss: 2.2449 0.4913 sec/batch\n",
      "Epoch 2/40  Iteration 386/11040 Training loss: 2.2442 0.4749 sec/batch\n",
      "Epoch 2/40  Iteration 387/11040 Training loss: 2.2437 0.4896 sec/batch\n",
      "Epoch 2/40  Iteration 388/11040 Training loss: 2.2428 0.4724 sec/batch\n",
      "Epoch 2/40  Iteration 389/11040 Training loss: 2.2423 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 390/11040 Training loss: 2.2417 0.4859 sec/batch\n",
      "Epoch 2/40  Iteration 391/11040 Training loss: 2.2412 0.4802 sec/batch\n",
      "Epoch 2/40  Iteration 392/11040 Training loss: 2.2408 0.4749 sec/batch\n",
      "Epoch 2/40  Iteration 393/11040 Training loss: 2.2403 0.4813 sec/batch\n",
      "Epoch 2/40  Iteration 394/11040 Training loss: 2.2399 0.4839 sec/batch\n",
      "Epoch 2/40  Iteration 395/11040 Training loss: 2.2393 0.4883 sec/batch\n",
      "Epoch 2/40  Iteration 396/11040 Training loss: 2.2388 0.4780 sec/batch\n",
      "Epoch 2/40  Iteration 397/11040 Training loss: 2.2382 0.4719 sec/batch\n",
      "Epoch 2/40  Iteration 398/11040 Training loss: 2.2376 0.4911 sec/batch\n",
      "Epoch 2/40  Iteration 399/11040 Training loss: 2.2371 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 400/11040 Training loss: 2.2366 0.4858 sec/batch\n",
      "Epoch 2/40  Iteration 401/11040 Training loss: 2.2359 0.4859 sec/batch\n",
      "Epoch 2/40  Iteration 402/11040 Training loss: 2.2353 0.4894 sec/batch\n",
      "Epoch 2/40  Iteration 403/11040 Training loss: 2.2346 0.4838 sec/batch\n",
      "Epoch 2/40  Iteration 404/11040 Training loss: 2.2338 0.4694 sec/batch\n",
      "Epoch 2/40  Iteration 405/11040 Training loss: 2.2330 0.4839 sec/batch\n",
      "Epoch 2/40  Iteration 406/11040 Training loss: 2.2323 0.4733 sec/batch\n",
      "Epoch 2/40  Iteration 407/11040 Training loss: 2.2312 0.4707 sec/batch\n",
      "Epoch 2/40  Iteration 408/11040 Training loss: 2.2303 0.4783 sec/batch\n",
      "Epoch 2/40  Iteration 409/11040 Training loss: 2.2295 0.4787 sec/batch\n",
      "Epoch 2/40  Iteration 410/11040 Training loss: 2.2286 0.4760 sec/batch\n",
      "Epoch 2/40  Iteration 411/11040 Training loss: 2.2281 0.4808 sec/batch\n",
      "Epoch 2/40  Iteration 412/11040 Training loss: 2.2278 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 413/11040 Training loss: 2.2271 0.4862 sec/batch\n",
      "Epoch 2/40  Iteration 414/11040 Training loss: 2.2265 0.4798 sec/batch\n",
      "Epoch 2/40  Iteration 415/11040 Training loss: 2.2258 0.4757 sec/batch\n",
      "Epoch 2/40  Iteration 416/11040 Training loss: 2.2251 0.4947 sec/batch\n",
      "Epoch 2/40  Iteration 417/11040 Training loss: 2.2245 0.4709 sec/batch\n",
      "Epoch 2/40  Iteration 418/11040 Training loss: 2.2236 0.4752 sec/batch\n",
      "Epoch 2/40  Iteration 419/11040 Training loss: 2.2228 0.4649 sec/batch\n",
      "Epoch 2/40  Iteration 420/11040 Training loss: 2.2219 0.4758 sec/batch\n",
      "Epoch 2/40  Iteration 421/11040 Training loss: 2.2211 0.4920 sec/batch\n",
      "Epoch 2/40  Iteration 422/11040 Training loss: 2.2203 0.4748 sec/batch\n",
      "Epoch 2/40  Iteration 423/11040 Training loss: 2.2195 0.4762 sec/batch\n",
      "Epoch 2/40  Iteration 424/11040 Training loss: 2.2186 0.4795 sec/batch\n",
      "Epoch 2/40  Iteration 425/11040 Training loss: 2.2177 0.4762 sec/batch\n",
      "Epoch 2/40  Iteration 426/11040 Training loss: 2.2170 0.5018 sec/batch\n",
      "Epoch 2/40  Iteration 427/11040 Training loss: 2.2165 0.4629 sec/batch\n",
      "Epoch 2/40  Iteration 428/11040 Training loss: 2.2158 0.4745 sec/batch\n",
      "Epoch 2/40  Iteration 429/11040 Training loss: 2.2151 0.4849 sec/batch\n",
      "Epoch 2/40  Iteration 430/11040 Training loss: 2.2143 0.4653 sec/batch\n",
      "Epoch 2/40  Iteration 431/11040 Training loss: 2.2136 0.4742 sec/batch\n",
      "Epoch 2/40  Iteration 432/11040 Training loss: 2.2129 0.4814 sec/batch\n",
      "Epoch 2/40  Iteration 433/11040 Training loss: 2.2121 0.4912 sec/batch\n",
      "Epoch 2/40  Iteration 434/11040 Training loss: 2.2115 0.4865 sec/batch\n",
      "Epoch 2/40  Iteration 435/11040 Training loss: 2.2111 0.4792 sec/batch\n",
      "Epoch 2/40  Iteration 436/11040 Training loss: 2.2105 0.4762 sec/batch\n",
      "Epoch 2/40  Iteration 437/11040 Training loss: 2.2101 0.4801 sec/batch\n",
      "Epoch 2/40  Iteration 438/11040 Training loss: 2.2095 0.4748 sec/batch\n",
      "Epoch 2/40  Iteration 439/11040 Training loss: 2.2089 0.4875 sec/batch\n",
      "Epoch 2/40  Iteration 440/11040 Training loss: 2.2084 0.4625 sec/batch\n",
      "Epoch 2/40  Iteration 441/11040 Training loss: 2.2078 0.4766 sec/batch\n",
      "Epoch 2/40  Iteration 442/11040 Training loss: 2.2073 0.4920 sec/batch\n",
      "Epoch 2/40  Iteration 443/11040 Training loss: 2.2068 0.4738 sec/batch\n",
      "Epoch 2/40  Iteration 444/11040 Training loss: 2.2061 0.4750 sec/batch\n",
      "Epoch 2/40  Iteration 445/11040 Training loss: 2.2051 0.4661 sec/batch\n",
      "Epoch 2/40  Iteration 446/11040 Training loss: 2.2044 0.4755 sec/batch\n",
      "Epoch 2/40  Iteration 447/11040 Training loss: 2.2038 0.4912 sec/batch\n",
      "Epoch 2/40  Iteration 448/11040 Training loss: 2.2030 0.4750 sec/batch\n",
      "Epoch 2/40  Iteration 449/11040 Training loss: 2.2023 0.4827 sec/batch\n",
      "Epoch 2/40  Iteration 450/11040 Training loss: 2.2015 0.4824 sec/batch\n",
      "Epoch 2/40  Iteration 451/11040 Training loss: 2.2007 0.4896 sec/batch\n",
      "Epoch 2/40  Iteration 452/11040 Training loss: 2.2000 0.4750 sec/batch\n",
      "Epoch 2/40  Iteration 453/11040 Training loss: 2.1991 0.4811 sec/batch\n",
      "Epoch 2/40  Iteration 454/11040 Training loss: 2.1985 0.4753 sec/batch\n",
      "Epoch 2/40  Iteration 455/11040 Training loss: 2.1980 0.4890 sec/batch\n",
      "Epoch 2/40  Iteration 456/11040 Training loss: 2.1973 0.4777 sec/batch\n",
      "Epoch 2/40  Iteration 457/11040 Training loss: 2.1966 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 458/11040 Training loss: 2.1959 0.4801 sec/batch\n",
      "Epoch 2/40  Iteration 459/11040 Training loss: 2.1951 0.4914 sec/batch\n",
      "Epoch 2/40  Iteration 460/11040 Training loss: 2.1945 0.4854 sec/batch\n",
      "Epoch 2/40  Iteration 461/11040 Training loss: 2.1936 0.4796 sec/batch\n",
      "Epoch 2/40  Iteration 462/11040 Training loss: 2.1931 0.4747 sec/batch\n",
      "Epoch 2/40  Iteration 463/11040 Training loss: 2.1926 0.4819 sec/batch\n",
      "Epoch 2/40  Iteration 464/11040 Training loss: 2.1919 0.4750 sec/batch\n",
      "Epoch 2/40  Iteration 465/11040 Training loss: 2.1911 0.4823 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40  Iteration 466/11040 Training loss: 2.1903 0.4676 sec/batch\n",
      "Epoch 2/40  Iteration 467/11040 Training loss: 2.1896 0.4772 sec/batch\n",
      "Epoch 2/40  Iteration 468/11040 Training loss: 2.1888 0.4795 sec/batch\n",
      "Epoch 2/40  Iteration 469/11040 Training loss: 2.1881 0.4904 sec/batch\n",
      "Epoch 2/40  Iteration 470/11040 Training loss: 2.1875 0.4871 sec/batch\n",
      "Epoch 2/40  Iteration 471/11040 Training loss: 2.1870 0.4788 sec/batch\n",
      "Epoch 2/40  Iteration 472/11040 Training loss: 2.1864 0.4830 sec/batch\n",
      "Epoch 2/40  Iteration 473/11040 Training loss: 2.1857 0.4915 sec/batch\n",
      "Epoch 2/40  Iteration 474/11040 Training loss: 2.1851 0.4780 sec/batch\n",
      "Epoch 2/40  Iteration 475/11040 Training loss: 2.1847 0.4727 sec/batch\n",
      "Epoch 2/40  Iteration 476/11040 Training loss: 2.1841 0.4810 sec/batch\n",
      "Epoch 2/40  Iteration 477/11040 Training loss: 2.1834 0.4764 sec/batch\n",
      "Epoch 2/40  Iteration 478/11040 Training loss: 2.1827 0.4806 sec/batch\n",
      "Epoch 2/40  Iteration 479/11040 Training loss: 2.1822 0.4684 sec/batch\n",
      "Epoch 2/40  Iteration 480/11040 Training loss: 2.1817 0.4752 sec/batch\n",
      "Epoch 2/40  Iteration 481/11040 Training loss: 2.1809 0.4808 sec/batch\n",
      "Epoch 2/40  Iteration 482/11040 Training loss: 2.1801 0.4766 sec/batch\n",
      "Epoch 2/40  Iteration 483/11040 Training loss: 2.1794 0.4898 sec/batch\n",
      "Epoch 2/40  Iteration 484/11040 Training loss: 2.1788 0.4794 sec/batch\n",
      "Epoch 2/40  Iteration 485/11040 Training loss: 2.1784 0.4767 sec/batch\n",
      "Epoch 2/40  Iteration 486/11040 Training loss: 2.1777 0.4818 sec/batch\n",
      "Epoch 2/40  Iteration 487/11040 Training loss: 2.1769 0.4752 sec/batch\n",
      "Epoch 2/40  Iteration 488/11040 Training loss: 2.1763 0.4751 sec/batch\n",
      "Epoch 2/40  Iteration 489/11040 Training loss: 2.1756 0.4806 sec/batch\n",
      "Epoch 2/40  Iteration 490/11040 Training loss: 2.1750 0.4741 sec/batch\n",
      "Epoch 2/40  Iteration 491/11040 Training loss: 2.1745 0.4800 sec/batch\n",
      "Epoch 2/40  Iteration 492/11040 Training loss: 2.1739 0.4872 sec/batch\n",
      "Epoch 2/40  Iteration 493/11040 Training loss: 2.1732 0.4892 sec/batch\n",
      "Epoch 2/40  Iteration 494/11040 Training loss: 2.1725 0.4789 sec/batch\n",
      "Epoch 2/40  Iteration 495/11040 Training loss: 2.1719 0.4909 sec/batch\n",
      "Epoch 2/40  Iteration 496/11040 Training loss: 2.1714 0.4961 sec/batch\n",
      "Epoch 2/40  Iteration 497/11040 Training loss: 2.1708 0.4899 sec/batch\n",
      "Epoch 2/40  Iteration 498/11040 Training loss: 2.1701 0.4949 sec/batch\n",
      "Epoch 2/40  Iteration 499/11040 Training loss: 2.1695 0.4699 sec/batch\n",
      "Epoch 2/40  Iteration 500/11040 Training loss: 2.1690 0.4852 sec/batch\n",
      "Validation loss: 1.94441 Saving checkpoint!\n",
      "Epoch 2/40  Iteration 501/11040 Training loss: 2.1687 0.4755 sec/batch\n",
      "Epoch 2/40  Iteration 502/11040 Training loss: 2.1681 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 503/11040 Training loss: 2.1674 0.4776 sec/batch\n",
      "Epoch 2/40  Iteration 504/11040 Training loss: 2.1667 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 505/11040 Training loss: 2.1662 0.4735 sec/batch\n",
      "Epoch 2/40  Iteration 506/11040 Training loss: 2.1656 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 507/11040 Training loss: 2.1651 0.4745 sec/batch\n",
      "Epoch 2/40  Iteration 508/11040 Training loss: 2.1644 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 509/11040 Training loss: 2.1636 0.4734 sec/batch\n",
      "Epoch 2/40  Iteration 510/11040 Training loss: 2.1629 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 511/11040 Training loss: 2.1623 0.4755 sec/batch\n",
      "Epoch 2/40  Iteration 512/11040 Training loss: 2.1615 0.4755 sec/batch\n",
      "Epoch 2/40  Iteration 513/11040 Training loss: 2.1607 0.4773 sec/batch\n",
      "Epoch 2/40  Iteration 514/11040 Training loss: 2.1601 0.4744 sec/batch\n",
      "Epoch 2/40  Iteration 515/11040 Training loss: 2.1596 0.4795 sec/batch\n",
      "Epoch 2/40  Iteration 516/11040 Training loss: 2.1589 0.4763 sec/batch\n",
      "Epoch 2/40  Iteration 517/11040 Training loss: 2.1583 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 518/11040 Training loss: 2.1576 0.4763 sec/batch\n",
      "Epoch 2/40  Iteration 519/11040 Training loss: 2.1568 0.4804 sec/batch\n",
      "Epoch 2/40  Iteration 520/11040 Training loss: 2.1561 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 521/11040 Training loss: 2.1554 0.4814 sec/batch\n",
      "Epoch 2/40  Iteration 522/11040 Training loss: 2.1548 0.4784 sec/batch\n",
      "Epoch 2/40  Iteration 523/11040 Training loss: 2.1541 0.4804 sec/batch\n",
      "Epoch 2/40  Iteration 524/11040 Training loss: 2.1536 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 525/11040 Training loss: 2.1530 0.4784 sec/batch\n",
      "Epoch 2/40  Iteration 526/11040 Training loss: 2.1524 0.4814 sec/batch\n",
      "Epoch 2/40  Iteration 527/11040 Training loss: 2.1517 0.4794 sec/batch\n",
      "Epoch 2/40  Iteration 528/11040 Training loss: 2.1510 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 529/11040 Training loss: 2.1505 0.4784 sec/batch\n",
      "Epoch 2/40  Iteration 530/11040 Training loss: 2.1500 0.4776 sec/batch\n",
      "Epoch 2/40  Iteration 531/11040 Training loss: 2.1494 0.4784 sec/batch\n",
      "Epoch 2/40  Iteration 532/11040 Training loss: 2.1487 0.4774 sec/batch\n",
      "Epoch 2/40  Iteration 533/11040 Training loss: 2.1482 0.4776 sec/batch\n",
      "Epoch 2/40  Iteration 534/11040 Training loss: 2.1476 0.4744 sec/batch\n",
      "Epoch 2/40  Iteration 535/11040 Training loss: 2.1470 0.4763 sec/batch\n",
      "Epoch 2/40  Iteration 536/11040 Training loss: 2.1463 0.4745 sec/batch\n",
      "Epoch 2/40  Iteration 537/11040 Training loss: 2.1457 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 538/11040 Training loss: 2.1452 0.4794 sec/batch\n",
      "Epoch 2/40  Iteration 539/11040 Training loss: 2.1448 0.4804 sec/batch\n",
      "Epoch 2/40  Iteration 540/11040 Training loss: 2.1442 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 541/11040 Training loss: 2.1436 0.4763 sec/batch\n",
      "Epoch 2/40  Iteration 542/11040 Training loss: 2.1431 0.4743 sec/batch\n",
      "Epoch 2/40  Iteration 543/11040 Training loss: 2.1426 0.4734 sec/batch\n",
      "Epoch 2/40  Iteration 544/11040 Training loss: 2.1420 0.4743 sec/batch\n",
      "Epoch 2/40  Iteration 545/11040 Training loss: 2.1416 0.4764 sec/batch\n",
      "Epoch 2/40  Iteration 546/11040 Training loss: 2.1412 0.4754 sec/batch\n",
      "Epoch 2/40  Iteration 547/11040 Training loss: 2.1408 0.4765 sec/batch\n",
      "Epoch 2/40  Iteration 548/11040 Training loss: 2.1404 0.4794 sec/batch\n",
      "Epoch 2/40  Iteration 549/11040 Training loss: 2.1399 0.4736 sec/batch\n",
      "Epoch 2/40  Iteration 550/11040 Training loss: 2.1394 0.4764 sec/batch\n",
      "Epoch 2/40  Iteration 551/11040 Training loss: 2.1387 0.4724 sec/batch\n",
      "Epoch 2/40  Iteration 552/11040 Training loss: 2.1381 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 553/11040 Training loss: 2.0527 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 554/11040 Training loss: 2.0031 0.4739 sec/batch\n",
      "Epoch 3/40  Iteration 555/11040 Training loss: 1.9970 0.4756 sec/batch\n",
      "Epoch 3/40  Iteration 556/11040 Training loss: 2.0030 0.4734 sec/batch\n",
      "Epoch 3/40  Iteration 557/11040 Training loss: 2.0043 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 558/11040 Training loss: 1.9973 0.4904 sec/batch\n",
      "Epoch 3/40  Iteration 559/11040 Training loss: 1.9889 0.5095 sec/batch\n",
      "Epoch 3/40  Iteration 560/11040 Training loss: 1.9870 0.4974 sec/batch\n",
      "Epoch 3/40  Iteration 561/11040 Training loss: 1.9840 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 562/11040 Training loss: 1.9854 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 563/11040 Training loss: 1.9857 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 564/11040 Training loss: 1.9849 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 565/11040 Training loss: 1.9833 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 566/11040 Training loss: 1.9813 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 567/11040 Training loss: 1.9796 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 568/11040 Training loss: 1.9786 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 569/11040 Training loss: 1.9752 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 570/11040 Training loss: 1.9737 0.4735 sec/batch\n",
      "Epoch 3/40  Iteration 571/11040 Training loss: 1.9723 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 572/11040 Training loss: 1.9722 0.4735 sec/batch\n",
      "Epoch 3/40  Iteration 573/11040 Training loss: 1.9723 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 574/11040 Training loss: 1.9724 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 575/11040 Training loss: 1.9725 0.4745 sec/batch\n",
      "Epoch 3/40  Iteration 576/11040 Training loss: 1.9732 0.4745 sec/batch\n",
      "Epoch 3/40  Iteration 577/11040 Training loss: 1.9717 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 578/11040 Training loss: 1.9699 0.4786 sec/batch\n",
      "Epoch 3/40  Iteration 579/11040 Training loss: 1.9682 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 580/11040 Training loss: 1.9674 0.4764 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40  Iteration 581/11040 Training loss: 1.9673 0.4733 sec/batch\n",
      "Epoch 3/40  Iteration 582/11040 Training loss: 1.9676 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 583/11040 Training loss: 1.9667 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 584/11040 Training loss: 1.9656 0.4755 sec/batch\n",
      "Epoch 3/40  Iteration 585/11040 Training loss: 1.9659 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 586/11040 Training loss: 1.9658 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 587/11040 Training loss: 1.9656 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 588/11040 Training loss: 1.9654 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 589/11040 Training loss: 1.9651 0.4765 sec/batch\n",
      "Epoch 3/40  Iteration 590/11040 Training loss: 1.9646 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 591/11040 Training loss: 1.9640 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 592/11040 Training loss: 1.9644 0.4734 sec/batch\n",
      "Epoch 3/40  Iteration 593/11040 Training loss: 1.9637 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 594/11040 Training loss: 1.9627 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 595/11040 Training loss: 1.9629 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 596/11040 Training loss: 1.9630 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 597/11040 Training loss: 1.9628 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 598/11040 Training loss: 1.9624 0.4745 sec/batch\n",
      "Epoch 3/40  Iteration 599/11040 Training loss: 1.9620 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 600/11040 Training loss: 1.9609 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 601/11040 Training loss: 1.9593 0.4734 sec/batch\n",
      "Epoch 3/40  Iteration 602/11040 Training loss: 1.9592 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 603/11040 Training loss: 1.9587 0.4724 sec/batch\n",
      "Epoch 3/40  Iteration 604/11040 Training loss: 1.9580 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 605/11040 Training loss: 1.9580 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 606/11040 Training loss: 1.9575 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 607/11040 Training loss: 1.9572 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 608/11040 Training loss: 1.9569 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 609/11040 Training loss: 1.9562 0.4724 sec/batch\n",
      "Epoch 3/40  Iteration 610/11040 Training loss: 1.9559 0.4775 sec/batch\n",
      "Epoch 3/40  Iteration 611/11040 Training loss: 1.9558 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 612/11040 Training loss: 1.9550 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 613/11040 Training loss: 1.9546 0.4736 sec/batch\n",
      "Epoch 3/40  Iteration 614/11040 Training loss: 1.9535 0.4743 sec/batch\n",
      "Epoch 3/40  Iteration 615/11040 Training loss: 1.9527 0.4785 sec/batch\n",
      "Epoch 3/40  Iteration 616/11040 Training loss: 1.9525 0.4741 sec/batch\n",
      "Epoch 3/40  Iteration 617/11040 Training loss: 1.9519 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 618/11040 Training loss: 1.9511 0.4755 sec/batch\n",
      "Epoch 3/40  Iteration 619/11040 Training loss: 1.9502 0.4775 sec/batch\n",
      "Epoch 3/40  Iteration 620/11040 Training loss: 1.9498 0.4766 sec/batch\n",
      "Epoch 3/40  Iteration 621/11040 Training loss: 1.9495 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 622/11040 Training loss: 1.9497 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 623/11040 Training loss: 1.9488 0.4743 sec/batch\n",
      "Epoch 3/40  Iteration 624/11040 Training loss: 1.9482 0.4757 sec/batch\n",
      "Epoch 3/40  Iteration 625/11040 Training loss: 1.9474 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 626/11040 Training loss: 1.9470 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 627/11040 Training loss: 1.9464 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 628/11040 Training loss: 1.9459 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 629/11040 Training loss: 1.9453 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 630/11040 Training loss: 1.9450 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 631/11040 Training loss: 1.9440 0.4803 sec/batch\n",
      "Epoch 3/40  Iteration 632/11040 Training loss: 1.9433 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 633/11040 Training loss: 1.9424 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 634/11040 Training loss: 1.9414 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 635/11040 Training loss: 1.9406 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 636/11040 Training loss: 1.9397 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 637/11040 Training loss: 1.9387 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 638/11040 Training loss: 1.9382 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 639/11040 Training loss: 1.9376 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 640/11040 Training loss: 1.9366 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 641/11040 Training loss: 1.9361 0.4785 sec/batch\n",
      "Epoch 3/40  Iteration 642/11040 Training loss: 1.9357 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 643/11040 Training loss: 1.9357 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 644/11040 Training loss: 1.9353 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 645/11040 Training loss: 1.9348 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 646/11040 Training loss: 1.9346 0.4776 sec/batch\n",
      "Epoch 3/40  Iteration 647/11040 Training loss: 1.9347 0.4825 sec/batch\n",
      "Epoch 3/40  Iteration 648/11040 Training loss: 1.9344 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 649/11040 Training loss: 1.9348 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 650/11040 Training loss: 1.9347 0.4756 sec/batch\n",
      "Epoch 3/40  Iteration 651/11040 Training loss: 1.9343 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 652/11040 Training loss: 1.9338 0.4747 sec/batch\n",
      "Epoch 3/40  Iteration 653/11040 Training loss: 1.9335 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 654/11040 Training loss: 1.9331 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 655/11040 Training loss: 1.9322 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 656/11040 Training loss: 1.9320 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 657/11040 Training loss: 1.9313 0.4768 sec/batch\n",
      "Epoch 3/40  Iteration 658/11040 Training loss: 1.9310 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 659/11040 Training loss: 1.9312 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 660/11040 Training loss: 1.9314 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 661/11040 Training loss: 1.9313 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 662/11040 Training loss: 1.9311 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 663/11040 Training loss: 1.9308 0.4795 sec/batch\n",
      "Epoch 3/40  Iteration 664/11040 Training loss: 1.9306 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 665/11040 Training loss: 1.9305 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 666/11040 Training loss: 1.9303 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 667/11040 Training loss: 1.9304 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 668/11040 Training loss: 1.9305 0.4765 sec/batch\n",
      "Epoch 3/40  Iteration 669/11040 Training loss: 1.9304 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 670/11040 Training loss: 1.9304 0.4795 sec/batch\n",
      "Epoch 3/40  Iteration 671/11040 Training loss: 1.9302 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 672/11040 Training loss: 1.9303 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 673/11040 Training loss: 1.9300 0.4803 sec/batch\n",
      "Epoch 3/40  Iteration 674/11040 Training loss: 1.9298 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 675/11040 Training loss: 1.9299 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 676/11040 Training loss: 1.9300 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 677/11040 Training loss: 1.9296 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 678/11040 Training loss: 1.9295 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 679/11040 Training loss: 1.9293 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 680/11040 Training loss: 1.9289 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 681/11040 Training loss: 1.9285 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 682/11040 Training loss: 1.9282 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 683/11040 Training loss: 1.9277 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 684/11040 Training loss: 1.9272 0.4755 sec/batch\n",
      "Epoch 3/40  Iteration 685/11040 Training loss: 1.9268 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 686/11040 Training loss: 1.9263 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 687/11040 Training loss: 1.9263 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 688/11040 Training loss: 1.9264 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 689/11040 Training loss: 1.9260 0.4795 sec/batch\n",
      "Epoch 3/40  Iteration 690/11040 Training loss: 1.9258 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 691/11040 Training loss: 1.9256 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 692/11040 Training loss: 1.9253 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 693/11040 Training loss: 1.9249 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 694/11040 Training loss: 1.9246 0.4775 sec/batch\n",
      "Epoch 3/40  Iteration 695/11040 Training loss: 1.9241 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 696/11040 Training loss: 1.9236 0.4794 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40  Iteration 697/11040 Training loss: 1.9231 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 698/11040 Training loss: 1.9228 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 699/11040 Training loss: 1.9224 0.4765 sec/batch\n",
      "Epoch 3/40  Iteration 700/11040 Training loss: 1.9219 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 701/11040 Training loss: 1.9213 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 702/11040 Training loss: 1.9210 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 703/11040 Training loss: 1.9208 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 704/11040 Training loss: 1.9205 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 705/11040 Training loss: 1.9200 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 706/11040 Training loss: 1.9197 0.4776 sec/batch\n",
      "Epoch 3/40  Iteration 707/11040 Training loss: 1.9194 0.4785 sec/batch\n",
      "Epoch 3/40  Iteration 708/11040 Training loss: 1.9190 0.4746 sec/batch\n",
      "Epoch 3/40  Iteration 709/11040 Training loss: 1.9185 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 710/11040 Training loss: 1.9183 0.4766 sec/batch\n",
      "Epoch 3/40  Iteration 711/11040 Training loss: 1.9183 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 712/11040 Training loss: 1.9181 0.4785 sec/batch\n",
      "Epoch 3/40  Iteration 713/11040 Training loss: 1.9180 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 714/11040 Training loss: 1.9178 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 715/11040 Training loss: 1.9176 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 716/11040 Training loss: 1.9175 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 717/11040 Training loss: 1.9172 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 718/11040 Training loss: 1.9172 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 719/11040 Training loss: 1.9170 0.4765 sec/batch\n",
      "Epoch 3/40  Iteration 720/11040 Training loss: 1.9166 0.4775 sec/batch\n",
      "Epoch 3/40  Iteration 721/11040 Training loss: 1.9161 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 722/11040 Training loss: 1.9158 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 723/11040 Training loss: 1.9156 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 724/11040 Training loss: 1.9154 0.4896 sec/batch\n",
      "Epoch 3/40  Iteration 725/11040 Training loss: 1.9151 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 726/11040 Training loss: 1.9148 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 727/11040 Training loss: 1.9144 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 728/11040 Training loss: 1.9140 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 729/11040 Training loss: 1.9136 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 730/11040 Training loss: 1.9133 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 731/11040 Training loss: 1.9132 0.4743 sec/batch\n",
      "Epoch 3/40  Iteration 732/11040 Training loss: 1.9130 0.4755 sec/batch\n",
      "Epoch 3/40  Iteration 733/11040 Training loss: 1.9126 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 734/11040 Training loss: 1.9123 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 735/11040 Training loss: 1.9117 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 736/11040 Training loss: 1.9115 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 737/11040 Training loss: 1.9111 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 738/11040 Training loss: 1.9109 0.4793 sec/batch\n",
      "Epoch 3/40  Iteration 739/11040 Training loss: 1.9108 0.4733 sec/batch\n",
      "Epoch 3/40  Iteration 740/11040 Training loss: 1.9106 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 741/11040 Training loss: 1.9102 0.4743 sec/batch\n",
      "Epoch 3/40  Iteration 742/11040 Training loss: 1.9097 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 743/11040 Training loss: 1.9094 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 744/11040 Training loss: 1.9090 0.4723 sec/batch\n",
      "Epoch 3/40  Iteration 745/11040 Training loss: 1.9086 0.4816 sec/batch\n",
      "Epoch 3/40  Iteration 746/11040 Training loss: 1.9083 0.4805 sec/batch\n",
      "Epoch 3/40  Iteration 747/11040 Training loss: 1.9082 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 748/11040 Training loss: 1.9079 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 749/11040 Training loss: 1.9076 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 750/11040 Training loss: 1.9073 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 751/11040 Training loss: 1.9071 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 752/11040 Training loss: 1.9069 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 753/11040 Training loss: 1.9065 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 754/11040 Training loss: 1.9062 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 755/11040 Training loss: 1.9060 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 756/11040 Training loss: 1.9059 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 757/11040 Training loss: 1.9055 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 758/11040 Training loss: 1.9051 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 759/11040 Training loss: 1.9047 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 760/11040 Training loss: 1.9045 0.4823 sec/batch\n",
      "Epoch 3/40  Iteration 761/11040 Training loss: 1.9042 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 762/11040 Training loss: 1.9040 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 763/11040 Training loss: 1.9035 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 764/11040 Training loss: 1.9032 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 765/11040 Training loss: 1.9029 0.4803 sec/batch\n",
      "Epoch 3/40  Iteration 766/11040 Training loss: 1.9026 0.4793 sec/batch\n",
      "Epoch 3/40  Iteration 767/11040 Training loss: 1.9023 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 768/11040 Training loss: 1.9020 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 769/11040 Training loss: 1.9017 0.4803 sec/batch\n",
      "Epoch 3/40  Iteration 770/11040 Training loss: 1.9012 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 771/11040 Training loss: 1.9009 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 772/11040 Training loss: 1.9007 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 773/11040 Training loss: 1.9005 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 774/11040 Training loss: 1.9000 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 775/11040 Training loss: 1.8997 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 776/11040 Training loss: 1.8995 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 777/11040 Training loss: 1.8994 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 778/11040 Training loss: 1.8992 0.4755 sec/batch\n",
      "Epoch 3/40  Iteration 779/11040 Training loss: 1.8987 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 780/11040 Training loss: 1.8983 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 781/11040 Training loss: 1.8981 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 782/11040 Training loss: 1.8980 0.4755 sec/batch\n",
      "Epoch 3/40  Iteration 783/11040 Training loss: 1.8977 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 784/11040 Training loss: 1.8974 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 785/11040 Training loss: 1.8969 0.4823 sec/batch\n",
      "Epoch 3/40  Iteration 786/11040 Training loss: 1.8965 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 787/11040 Training loss: 1.8962 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 788/11040 Training loss: 1.8958 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 789/11040 Training loss: 1.8953 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 790/11040 Training loss: 1.8950 0.4783 sec/batch\n",
      "Epoch 3/40  Iteration 791/11040 Training loss: 1.8947 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 792/11040 Training loss: 1.8943 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 793/11040 Training loss: 1.8940 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 794/11040 Training loss: 1.8936 0.4775 sec/batch\n",
      "Epoch 3/40  Iteration 795/11040 Training loss: 1.8931 0.4765 sec/batch\n",
      "Epoch 3/40  Iteration 796/11040 Training loss: 1.8927 0.4745 sec/batch\n",
      "Epoch 3/40  Iteration 797/11040 Training loss: 1.8922 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 798/11040 Training loss: 1.8919 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 799/11040 Training loss: 1.8916 0.4753 sec/batch\n",
      "Epoch 3/40  Iteration 800/11040 Training loss: 1.8913 0.4743 sec/batch\n",
      "Epoch 3/40  Iteration 801/11040 Training loss: 1.8910 0.4784 sec/batch\n",
      "Epoch 3/40  Iteration 802/11040 Training loss: 1.8907 0.4754 sec/batch\n",
      "Epoch 3/40  Iteration 803/11040 Training loss: 1.8903 0.4804 sec/batch\n",
      "Epoch 3/40  Iteration 804/11040 Training loss: 1.8899 0.4798 sec/batch\n",
      "Epoch 3/40  Iteration 805/11040 Training loss: 1.8897 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 806/11040 Training loss: 1.8895 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 807/11040 Training loss: 1.8891 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 808/11040 Training loss: 1.8888 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 809/11040 Training loss: 1.8885 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 810/11040 Training loss: 1.8883 0.4743 sec/batch\n",
      "Epoch 3/40  Iteration 811/11040 Training loss: 1.8879 0.4824 sec/batch\n",
      "Epoch 3/40  Iteration 812/11040 Training loss: 1.8875 0.4753 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40  Iteration 813/11040 Training loss: 1.8872 0.4813 sec/batch\n",
      "Epoch 3/40  Iteration 814/11040 Training loss: 1.8870 0.4803 sec/batch\n",
      "Epoch 3/40  Iteration 815/11040 Training loss: 1.8868 0.4763 sec/batch\n",
      "Epoch 3/40  Iteration 816/11040 Training loss: 1.8865 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 817/11040 Training loss: 1.8862 0.4793 sec/batch\n",
      "Epoch 3/40  Iteration 818/11040 Training loss: 1.8860 0.4814 sec/batch\n",
      "Epoch 3/40  Iteration 819/11040 Training loss: 1.8857 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 820/11040 Training loss: 1.8854 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 821/11040 Training loss: 1.8852 0.4773 sec/batch\n",
      "Epoch 3/40  Iteration 822/11040 Training loss: 1.8851 0.4774 sec/batch\n",
      "Epoch 3/40  Iteration 823/11040 Training loss: 1.8851 0.4764 sec/batch\n",
      "Epoch 3/40  Iteration 824/11040 Training loss: 1.8850 0.4785 sec/batch\n",
      "Epoch 3/40  Iteration 825/11040 Training loss: 1.8847 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 826/11040 Training loss: 1.8844 0.4744 sec/batch\n",
      "Epoch 3/40  Iteration 827/11040 Training loss: 1.8840 0.4794 sec/batch\n",
      "Epoch 3/40  Iteration 828/11040 Training loss: 1.8836 0.4795 sec/batch\n",
      "Epoch 4/40  Iteration 829/11040 Training loss: 1.8736 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 830/11040 Training loss: 1.8226 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 831/11040 Training loss: 1.8160 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 832/11040 Training loss: 1.8198 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 833/11040 Training loss: 1.8200 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 834/11040 Training loss: 1.8130 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 835/11040 Training loss: 1.8059 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 836/11040 Training loss: 1.8038 0.4743 sec/batch\n",
      "Epoch 4/40  Iteration 837/11040 Training loss: 1.8007 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 838/11040 Training loss: 1.8018 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 839/11040 Training loss: 1.8025 0.4754 sec/batch\n",
      "Epoch 4/40  Iteration 840/11040 Training loss: 1.8024 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 841/11040 Training loss: 1.8013 0.4815 sec/batch\n",
      "Epoch 4/40  Iteration 842/11040 Training loss: 1.7983 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 843/11040 Training loss: 1.7976 0.4805 sec/batch\n",
      "Epoch 4/40  Iteration 844/11040 Training loss: 1.7977 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 845/11040 Training loss: 1.7955 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 846/11040 Training loss: 1.7938 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 847/11040 Training loss: 1.7930 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 848/11040 Training loss: 1.7933 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 849/11040 Training loss: 1.7934 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 850/11040 Training loss: 1.7939 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 851/11040 Training loss: 1.7940 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 852/11040 Training loss: 1.7953 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 853/11040 Training loss: 1.7942 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 854/11040 Training loss: 1.7924 0.4754 sec/batch\n",
      "Epoch 4/40  Iteration 855/11040 Training loss: 1.7906 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 856/11040 Training loss: 1.7900 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 857/11040 Training loss: 1.7901 0.4833 sec/batch\n",
      "Epoch 4/40  Iteration 858/11040 Training loss: 1.7907 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 859/11040 Training loss: 1.7902 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 860/11040 Training loss: 1.7896 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 861/11040 Training loss: 1.7900 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 862/11040 Training loss: 1.7899 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 863/11040 Training loss: 1.7897 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 864/11040 Training loss: 1.7898 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 865/11040 Training loss: 1.7895 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 866/11040 Training loss: 1.7892 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 867/11040 Training loss: 1.7887 0.4744 sec/batch\n",
      "Epoch 4/40  Iteration 868/11040 Training loss: 1.7892 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 869/11040 Training loss: 1.7883 0.4755 sec/batch\n",
      "Epoch 4/40  Iteration 870/11040 Training loss: 1.7877 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 871/11040 Training loss: 1.7879 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 872/11040 Training loss: 1.7881 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 873/11040 Training loss: 1.7881 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 874/11040 Training loss: 1.7878 0.4755 sec/batch\n",
      "Epoch 4/40  Iteration 875/11040 Training loss: 1.7874 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 876/11040 Training loss: 1.7864 0.4785 sec/batch\n",
      "Epoch 4/40  Iteration 877/11040 Training loss: 1.7851 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 878/11040 Training loss: 1.7852 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 879/11040 Training loss: 1.7847 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 880/11040 Training loss: 1.7841 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 881/11040 Training loss: 1.7843 0.4813 sec/batch\n",
      "Epoch 4/40  Iteration 882/11040 Training loss: 1.7839 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 883/11040 Training loss: 1.7837 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 884/11040 Training loss: 1.7838 0.4743 sec/batch\n",
      "Epoch 4/40  Iteration 885/11040 Training loss: 1.7833 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 886/11040 Training loss: 1.7833 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 887/11040 Training loss: 1.7833 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 888/11040 Training loss: 1.7827 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 889/11040 Training loss: 1.7825 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 890/11040 Training loss: 1.7814 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 891/11040 Training loss: 1.7809 0.4815 sec/batch\n",
      "Epoch 4/40  Iteration 892/11040 Training loss: 1.7807 0.4805 sec/batch\n",
      "Epoch 4/40  Iteration 893/11040 Training loss: 1.7803 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 894/11040 Training loss: 1.7794 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 895/11040 Training loss: 1.7787 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 896/11040 Training loss: 1.7786 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 897/11040 Training loss: 1.7784 0.4796 sec/batch\n",
      "Epoch 4/40  Iteration 898/11040 Training loss: 1.7786 0.4734 sec/batch\n",
      "Epoch 4/40  Iteration 899/11040 Training loss: 1.7780 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 900/11040 Training loss: 1.7773 0.4745 sec/batch\n",
      "Epoch 4/40  Iteration 901/11040 Training loss: 1.7765 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 902/11040 Training loss: 1.7763 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 903/11040 Training loss: 1.7759 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 904/11040 Training loss: 1.7756 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 905/11040 Training loss: 1.7753 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 906/11040 Training loss: 1.7751 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 907/11040 Training loss: 1.7744 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 908/11040 Training loss: 1.7739 0.4853 sec/batch\n",
      "Epoch 4/40  Iteration 909/11040 Training loss: 1.7732 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 910/11040 Training loss: 1.7726 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 911/11040 Training loss: 1.7721 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 912/11040 Training loss: 1.7714 0.4833 sec/batch\n",
      "Epoch 4/40  Iteration 913/11040 Training loss: 1.7708 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 914/11040 Training loss: 1.7705 0.4743 sec/batch\n",
      "Epoch 4/40  Iteration 915/11040 Training loss: 1.7701 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 916/11040 Training loss: 1.7692 0.4834 sec/batch\n",
      "Epoch 4/40  Iteration 917/11040 Training loss: 1.7691 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 918/11040 Training loss: 1.7690 0.4834 sec/batch\n",
      "Epoch 4/40  Iteration 919/11040 Training loss: 1.7693 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 920/11040 Training loss: 1.7692 0.4824 sec/batch\n",
      "Epoch 4/40  Iteration 921/11040 Training loss: 1.7688 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 922/11040 Training loss: 1.7688 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 923/11040 Training loss: 1.7692 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 924/11040 Training loss: 1.7692 0.4815 sec/batch\n",
      "Epoch 4/40  Iteration 925/11040 Training loss: 1.7697 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 926/11040 Training loss: 1.7698 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 927/11040 Training loss: 1.7696 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 928/11040 Training loss: 1.7693 0.4805 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40  Iteration 929/11040 Training loss: 1.7692 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 930/11040 Training loss: 1.7689 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 931/11040 Training loss: 1.7681 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 932/11040 Training loss: 1.7682 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 933/11040 Training loss: 1.7677 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 934/11040 Training loss: 1.7675 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 935/11040 Training loss: 1.7679 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 936/11040 Training loss: 1.7681 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 937/11040 Training loss: 1.7683 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 938/11040 Training loss: 1.7681 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 939/11040 Training loss: 1.7681 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 940/11040 Training loss: 1.7680 0.4743 sec/batch\n",
      "Epoch 4/40  Iteration 941/11040 Training loss: 1.7681 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 942/11040 Training loss: 1.7682 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 943/11040 Training loss: 1.7687 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 944/11040 Training loss: 1.7688 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 945/11040 Training loss: 1.7689 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 946/11040 Training loss: 1.7692 0.4825 sec/batch\n",
      "Epoch 4/40  Iteration 947/11040 Training loss: 1.7692 0.4755 sec/batch\n",
      "Epoch 4/40  Iteration 948/11040 Training loss: 1.7694 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 949/11040 Training loss: 1.7693 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 950/11040 Training loss: 1.7692 0.4754 sec/batch\n",
      "Epoch 4/40  Iteration 951/11040 Training loss: 1.7695 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 952/11040 Training loss: 1.7697 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 953/11040 Training loss: 1.7696 0.4806 sec/batch\n",
      "Epoch 4/40  Iteration 954/11040 Training loss: 1.7696 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 955/11040 Training loss: 1.7695 0.4813 sec/batch\n",
      "Epoch 4/40  Iteration 956/11040 Training loss: 1.7693 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 957/11040 Training loss: 1.7691 0.4807 sec/batch\n",
      "Epoch 4/40  Iteration 958/11040 Training loss: 1.7691 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 959/11040 Training loss: 1.7686 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 960/11040 Training loss: 1.7683 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 961/11040 Training loss: 1.7681 0.4813 sec/batch\n",
      "Epoch 4/40  Iteration 962/11040 Training loss: 1.7678 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 963/11040 Training loss: 1.7680 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 964/11040 Training loss: 1.7682 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 965/11040 Training loss: 1.7680 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 966/11040 Training loss: 1.7681 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 967/11040 Training loss: 1.7679 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 968/11040 Training loss: 1.7678 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 969/11040 Training loss: 1.7676 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 970/11040 Training loss: 1.7675 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 971/11040 Training loss: 1.7671 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 972/11040 Training loss: 1.7667 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 973/11040 Training loss: 1.7664 0.4924 sec/batch\n",
      "Epoch 4/40  Iteration 974/11040 Training loss: 1.7663 0.4787 sec/batch\n",
      "Epoch 4/40  Iteration 975/11040 Training loss: 1.7660 0.4795 sec/batch\n",
      "Epoch 4/40  Iteration 976/11040 Training loss: 1.7657 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 977/11040 Training loss: 1.7652 0.4765 sec/batch\n",
      "Epoch 4/40  Iteration 978/11040 Training loss: 1.7651 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 979/11040 Training loss: 1.7650 0.4765 sec/batch\n",
      "Epoch 4/40  Iteration 980/11040 Training loss: 1.7649 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 981/11040 Training loss: 1.7645 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 982/11040 Training loss: 1.7643 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 983/11040 Training loss: 1.7642 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 984/11040 Training loss: 1.7639 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 985/11040 Training loss: 1.7636 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 986/11040 Training loss: 1.7635 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 987/11040 Training loss: 1.7636 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 988/11040 Training loss: 1.7635 0.4813 sec/batch\n",
      "Epoch 4/40  Iteration 989/11040 Training loss: 1.7636 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 990/11040 Training loss: 1.7634 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 991/11040 Training loss: 1.7634 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 992/11040 Training loss: 1.7634 0.4743 sec/batch\n",
      "Epoch 4/40  Iteration 993/11040 Training loss: 1.7633 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 994/11040 Training loss: 1.7634 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 995/11040 Training loss: 1.7635 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 996/11040 Training loss: 1.7632 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 997/11040 Training loss: 1.7629 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 998/11040 Training loss: 1.7627 0.4765 sec/batch\n",
      "Epoch 4/40  Iteration 999/11040 Training loss: 1.7627 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1000/11040 Training loss: 1.7626 0.4774 sec/batch\n",
      "Validation loss: 1.63715 Saving checkpoint!\n",
      "Epoch 4/40  Iteration 1001/11040 Training loss: 1.7628 0.4713 sec/batch\n",
      "Epoch 4/40  Iteration 1002/11040 Training loss: 1.7625 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 1003/11040 Training loss: 1.7623 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 1004/11040 Training loss: 1.7620 0.4753 sec/batch\n",
      "Epoch 4/40  Iteration 1005/11040 Training loss: 1.7618 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 1006/11040 Training loss: 1.7616 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 1007/11040 Training loss: 1.7617 0.4744 sec/batch\n",
      "Epoch 4/40  Iteration 1008/11040 Training loss: 1.7616 0.4763 sec/batch\n",
      "Epoch 4/40  Iteration 1009/11040 Training loss: 1.7613 0.4755 sec/batch\n",
      "Epoch 4/40  Iteration 1010/11040 Training loss: 1.7611 0.4743 sec/batch\n",
      "Epoch 4/40  Iteration 1011/11040 Training loss: 1.7607 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1012/11040 Training loss: 1.7606 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1013/11040 Training loss: 1.7604 0.4754 sec/batch\n",
      "Epoch 4/40  Iteration 1014/11040 Training loss: 1.7604 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 1015/11040 Training loss: 1.7604 0.4795 sec/batch\n",
      "Epoch 4/40  Iteration 1016/11040 Training loss: 1.7603 0.4754 sec/batch\n",
      "Epoch 4/40  Iteration 1017/11040 Training loss: 1.7600 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 1018/11040 Training loss: 1.7597 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1019/11040 Training loss: 1.7593 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1020/11040 Training loss: 1.7591 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 1021/11040 Training loss: 1.7587 0.4767 sec/batch\n",
      "Epoch 4/40  Iteration 1022/11040 Training loss: 1.7586 0.4744 sec/batch\n",
      "Epoch 4/40  Iteration 1023/11040 Training loss: 1.7586 0.4767 sec/batch\n",
      "Epoch 4/40  Iteration 1024/11040 Training loss: 1.7584 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1025/11040 Training loss: 1.7583 0.4745 sec/batch\n",
      "Epoch 4/40  Iteration 1026/11040 Training loss: 1.7581 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 1027/11040 Training loss: 1.7581 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1028/11040 Training loss: 1.7579 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1029/11040 Training loss: 1.7578 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1030/11040 Training loss: 1.7576 0.4824 sec/batch\n",
      "Epoch 4/40  Iteration 1031/11040 Training loss: 1.7575 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1032/11040 Training loss: 1.7575 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1033/11040 Training loss: 1.7572 0.4815 sec/batch\n",
      "Epoch 4/40  Iteration 1034/11040 Training loss: 1.7569 0.4785 sec/batch\n",
      "Epoch 4/40  Iteration 1035/11040 Training loss: 1.7567 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1036/11040 Training loss: 1.7565 0.4764 sec/batch\n",
      "Epoch 4/40  Iteration 1037/11040 Training loss: 1.7564 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 1038/11040 Training loss: 1.7562 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1039/11040 Training loss: 1.7558 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1040/11040 Training loss: 1.7557 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1041/11040 Training loss: 1.7554 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1042/11040 Training loss: 1.7553 0.4813 sec/batch\n",
      "Epoch 4/40  Iteration 1043/11040 Training loss: 1.7550 0.4793 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40  Iteration 1044/11040 Training loss: 1.7549 0.4824 sec/batch\n",
      "Epoch 4/40  Iteration 1045/11040 Training loss: 1.7546 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 1046/11040 Training loss: 1.7543 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1047/11040 Training loss: 1.7541 0.4787 sec/batch\n",
      "Epoch 4/40  Iteration 1048/11040 Training loss: 1.7540 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 1049/11040 Training loss: 1.7539 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1050/11040 Training loss: 1.7535 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 1051/11040 Training loss: 1.7533 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1052/11040 Training loss: 1.7532 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1053/11040 Training loss: 1.7532 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1054/11040 Training loss: 1.7530 0.4796 sec/batch\n",
      "Epoch 4/40  Iteration 1055/11040 Training loss: 1.7527 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1056/11040 Training loss: 1.7523 0.4796 sec/batch\n",
      "Epoch 4/40  Iteration 1057/11040 Training loss: 1.7522 0.4793 sec/batch\n",
      "Epoch 4/40  Iteration 1058/11040 Training loss: 1.7521 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1059/11040 Training loss: 1.7519 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1060/11040 Training loss: 1.7517 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 1061/11040 Training loss: 1.7514 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1062/11040 Training loss: 1.7510 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1063/11040 Training loss: 1.7508 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1064/11040 Training loss: 1.7505 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1065/11040 Training loss: 1.7502 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1066/11040 Training loss: 1.7499 0.4818 sec/batch\n",
      "Epoch 4/40  Iteration 1067/11040 Training loss: 1.7497 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 1068/11040 Training loss: 1.7495 0.4803 sec/batch\n",
      "Epoch 4/40  Iteration 1069/11040 Training loss: 1.7492 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1070/11040 Training loss: 1.7490 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1071/11040 Training loss: 1.7486 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1072/11040 Training loss: 1.7482 0.4856 sec/batch\n",
      "Epoch 4/40  Iteration 1073/11040 Training loss: 1.7479 0.4795 sec/batch\n",
      "Epoch 4/40  Iteration 1074/11040 Training loss: 1.7477 0.4784 sec/batch\n",
      "Epoch 4/40  Iteration 1075/11040 Training loss: 1.7474 0.4805 sec/batch\n",
      "Epoch 4/40  Iteration 1076/11040 Training loss: 1.7472 0.4845 sec/batch\n",
      "Epoch 4/40  Iteration 1077/11040 Training loss: 1.7470 0.4834 sec/batch\n",
      "Epoch 4/40  Iteration 1078/11040 Training loss: 1.7467 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1079/11040 Training loss: 1.7464 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1080/11040 Training loss: 1.7462 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1081/11040 Training loss: 1.7460 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1082/11040 Training loss: 1.7459 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 1083/11040 Training loss: 1.7457 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1084/11040 Training loss: 1.7455 0.4773 sec/batch\n",
      "Epoch 4/40  Iteration 1085/11040 Training loss: 1.7453 0.4775 sec/batch\n",
      "Epoch 4/40  Iteration 1086/11040 Training loss: 1.7452 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 1087/11040 Training loss: 1.7449 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1088/11040 Training loss: 1.7446 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1089/11040 Training loss: 1.7445 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1090/11040 Training loss: 1.7444 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1091/11040 Training loss: 1.7442 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1092/11040 Training loss: 1.7441 0.4805 sec/batch\n",
      "Epoch 4/40  Iteration 1093/11040 Training loss: 1.7439 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1094/11040 Training loss: 1.7438 0.4823 sec/batch\n",
      "Epoch 4/40  Iteration 1095/11040 Training loss: 1.7436 0.4824 sec/batch\n",
      "Epoch 4/40  Iteration 1096/11040 Training loss: 1.7435 0.4823 sec/batch\n",
      "Epoch 4/40  Iteration 1097/11040 Training loss: 1.7434 0.4855 sec/batch\n",
      "Epoch 4/40  Iteration 1098/11040 Training loss: 1.7433 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1099/11040 Training loss: 1.7434 0.4783 sec/batch\n",
      "Epoch 4/40  Iteration 1100/11040 Training loss: 1.7434 0.4794 sec/batch\n",
      "Epoch 4/40  Iteration 1101/11040 Training loss: 1.7433 0.4814 sec/batch\n",
      "Epoch 4/40  Iteration 1102/11040 Training loss: 1.7431 0.4774 sec/batch\n",
      "Epoch 4/40  Iteration 1103/11040 Training loss: 1.7428 0.4804 sec/batch\n",
      "Epoch 4/40  Iteration 1104/11040 Training loss: 1.7426 0.4814 sec/batch\n",
      "Epoch 5/40  Iteration 1105/11040 Training loss: 1.7576 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1106/11040 Training loss: 1.7068 0.4816 sec/batch\n",
      "Epoch 5/40  Iteration 1107/11040 Training loss: 1.7056 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1108/11040 Training loss: 1.7076 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1109/11040 Training loss: 1.7105 0.4834 sec/batch\n",
      "Epoch 5/40  Iteration 1110/11040 Training loss: 1.7043 0.4815 sec/batch\n",
      "Epoch 5/40  Iteration 1111/11040 Training loss: 1.6967 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1112/11040 Training loss: 1.6947 0.4855 sec/batch\n",
      "Epoch 5/40  Iteration 1113/11040 Training loss: 1.6915 0.4793 sec/batch\n",
      "Epoch 5/40  Iteration 1114/11040 Training loss: 1.6921 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1115/11040 Training loss: 1.6915 0.4815 sec/batch\n",
      "Epoch 5/40  Iteration 1116/11040 Training loss: 1.6912 0.4793 sec/batch\n",
      "Epoch 5/40  Iteration 1117/11040 Training loss: 1.6898 0.4814 sec/batch\n",
      "Epoch 5/40  Iteration 1118/11040 Training loss: 1.6874 0.4784 sec/batch\n",
      "Epoch 5/40  Iteration 1119/11040 Training loss: 1.6867 0.4814 sec/batch\n",
      "Epoch 5/40  Iteration 1120/11040 Training loss: 1.6872 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1121/11040 Training loss: 1.6853 0.4807 sec/batch\n",
      "Epoch 5/40  Iteration 1122/11040 Training loss: 1.6834 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1123/11040 Training loss: 1.6824 0.4805 sec/batch\n",
      "Epoch 5/40  Iteration 1124/11040 Training loss: 1.6825 0.4814 sec/batch\n",
      "Epoch 5/40  Iteration 1125/11040 Training loss: 1.6830 0.4795 sec/batch\n",
      "Epoch 5/40  Iteration 1126/11040 Training loss: 1.6837 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1127/11040 Training loss: 1.6837 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1128/11040 Training loss: 1.6854 0.4824 sec/batch\n",
      "Epoch 5/40  Iteration 1129/11040 Training loss: 1.6840 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1130/11040 Training loss: 1.6827 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1131/11040 Training loss: 1.6813 0.4793 sec/batch\n",
      "Epoch 5/40  Iteration 1132/11040 Training loss: 1.6811 0.4784 sec/batch\n",
      "Epoch 5/40  Iteration 1133/11040 Training loss: 1.6812 0.4816 sec/batch\n",
      "Epoch 5/40  Iteration 1134/11040 Training loss: 1.6815 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1135/11040 Training loss: 1.6809 0.4793 sec/batch\n",
      "Epoch 5/40  Iteration 1136/11040 Training loss: 1.6804 0.4785 sec/batch\n",
      "Epoch 5/40  Iteration 1137/11040 Training loss: 1.6808 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1138/11040 Training loss: 1.6806 0.4814 sec/batch\n",
      "Epoch 5/40  Iteration 1139/11040 Training loss: 1.6806 0.4765 sec/batch\n",
      "Epoch 5/40  Iteration 1140/11040 Training loss: 1.6804 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1141/11040 Training loss: 1.6800 0.4784 sec/batch\n",
      "Epoch 5/40  Iteration 1142/11040 Training loss: 1.6798 0.4784 sec/batch\n",
      "Epoch 5/40  Iteration 1143/11040 Training loss: 1.6791 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1144/11040 Training loss: 1.6797 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1145/11040 Training loss: 1.6788 0.4784 sec/batch\n",
      "Epoch 5/40  Iteration 1146/11040 Training loss: 1.6780 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1147/11040 Training loss: 1.6783 0.4777 sec/batch\n",
      "Epoch 5/40  Iteration 1148/11040 Training loss: 1.6785 0.4795 sec/batch\n",
      "Epoch 5/40  Iteration 1149/11040 Training loss: 1.6784 0.4790 sec/batch\n",
      "Epoch 5/40  Iteration 1150/11040 Training loss: 1.6782 0.4775 sec/batch\n",
      "Epoch 5/40  Iteration 1151/11040 Training loss: 1.6779 0.5064 sec/batch\n",
      "Epoch 5/40  Iteration 1152/11040 Training loss: 1.6772 0.4795 sec/batch\n",
      "Epoch 5/40  Iteration 1153/11040 Training loss: 1.6761 0.4774 sec/batch\n",
      "Epoch 5/40  Iteration 1154/11040 Training loss: 1.6762 0.4729 sec/batch\n",
      "Epoch 5/40  Iteration 1155/11040 Training loss: 1.6757 0.4752 sec/batch\n",
      "Epoch 5/40  Iteration 1156/11040 Training loss: 1.6751 0.4811 sec/batch\n",
      "Epoch 5/40  Iteration 1157/11040 Training loss: 1.6753 0.4743 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40  Iteration 1158/11040 Training loss: 1.6749 0.4673 sec/batch\n",
      "Epoch 5/40  Iteration 1159/11040 Training loss: 1.6748 0.4713 sec/batch\n",
      "Epoch 5/40  Iteration 1160/11040 Training loss: 1.6747 0.4752 sec/batch\n",
      "Epoch 5/40  Iteration 1161/11040 Training loss: 1.6744 0.4752 sec/batch\n",
      "Epoch 5/40  Iteration 1162/11040 Training loss: 1.6743 0.4699 sec/batch\n",
      "Epoch 5/40  Iteration 1163/11040 Training loss: 1.6746 0.4709 sec/batch\n",
      "Epoch 5/40  Iteration 1164/11040 Training loss: 1.6742 0.4836 sec/batch\n",
      "Epoch 5/40  Iteration 1165/11040 Training loss: 1.6739 0.4747 sec/batch\n",
      "Epoch 5/40  Iteration 1166/11040 Training loss: 1.6731 0.4626 sec/batch\n",
      "Epoch 5/40  Iteration 1167/11040 Training loss: 1.6728 0.4785 sec/batch\n",
      "Epoch 5/40  Iteration 1168/11040 Training loss: 1.6726 0.4749 sec/batch\n",
      "Epoch 5/40  Iteration 1169/11040 Training loss: 1.6724 0.4768 sec/batch\n",
      "Epoch 5/40  Iteration 1170/11040 Training loss: 1.6717 0.4644 sec/batch\n",
      "Epoch 5/40  Iteration 1171/11040 Training loss: 1.6711 0.4763 sec/batch\n",
      "Epoch 5/40  Iteration 1172/11040 Training loss: 1.6711 0.4826 sec/batch\n",
      "Epoch 5/40  Iteration 1173/11040 Training loss: 1.6707 0.4742 sec/batch\n",
      "Epoch 5/40  Iteration 1174/11040 Training loss: 1.6710 0.4792 sec/batch\n",
      "Epoch 5/40  Iteration 1175/11040 Training loss: 1.6705 0.4637 sec/batch\n",
      "Epoch 5/40  Iteration 1176/11040 Training loss: 1.6700 0.4757 sec/batch\n",
      "Epoch 5/40  Iteration 1177/11040 Training loss: 1.6692 0.4886 sec/batch\n",
      "Epoch 5/40  Iteration 1178/11040 Training loss: 1.6692 0.4770 sec/batch\n",
      "Epoch 5/40  Iteration 1179/11040 Training loss: 1.6690 0.4746 sec/batch\n",
      "Epoch 5/40  Iteration 1180/11040 Training loss: 1.6689 0.4797 sec/batch\n",
      "Epoch 5/40  Iteration 1181/11040 Training loss: 1.6687 0.4710 sec/batch\n",
      "Epoch 5/40  Iteration 1182/11040 Training loss: 1.6687 0.4917 sec/batch\n",
      "Epoch 5/40  Iteration 1183/11040 Training loss: 1.6681 0.4801 sec/batch\n",
      "Epoch 5/40  Iteration 1184/11040 Training loss: 1.6679 0.4748 sec/batch\n",
      "Epoch 5/40  Iteration 1185/11040 Training loss: 1.6672 0.4795 sec/batch\n",
      "Epoch 5/40  Iteration 1186/11040 Training loss: 1.6666 0.4788 sec/batch\n",
      "Epoch 5/40  Iteration 1187/11040 Training loss: 1.6661 0.4602 sec/batch\n",
      "Epoch 5/40  Iteration 1188/11040 Training loss: 1.6655 0.4809 sec/batch\n",
      "Epoch 5/40  Iteration 1189/11040 Training loss: 1.6651 0.4748 sec/batch\n",
      "Epoch 5/40  Iteration 1190/11040 Training loss: 1.6648 0.4762 sec/batch\n",
      "Epoch 5/40  Iteration 1191/11040 Training loss: 1.6644 0.4901 sec/batch\n",
      "Epoch 5/40  Iteration 1192/11040 Training loss: 1.6636 0.4804 sec/batch\n",
      "Epoch 5/40  Iteration 1193/11040 Training loss: 1.6633 0.4869 sec/batch\n",
      "Epoch 5/40  Iteration 1194/11040 Training loss: 1.6634 0.4636 sec/batch\n",
      "Epoch 5/40  Iteration 1195/11040 Training loss: 1.6638 0.4760 sec/batch\n",
      "Epoch 5/40  Iteration 1196/11040 Training loss: 1.6637 0.4798 sec/batch\n",
      "Epoch 5/40  Iteration 1197/11040 Training loss: 1.6634 0.4758 sec/batch\n",
      "Epoch 5/40  Iteration 1198/11040 Training loss: 1.6635 0.4754 sec/batch\n",
      "Epoch 5/40  Iteration 1199/11040 Training loss: 1.6639 0.4788 sec/batch\n",
      "Epoch 5/40  Iteration 1200/11040 Training loss: 1.6641 0.4759 sec/batch\n",
      "Epoch 5/40  Iteration 1201/11040 Training loss: 1.6646 0.4899 sec/batch\n",
      "Epoch 5/40  Iteration 1202/11040 Training loss: 1.6648 0.4905 sec/batch\n",
      "Epoch 5/40  Iteration 1203/11040 Training loss: 1.6646 0.4751 sec/batch\n",
      "Epoch 5/40  Iteration 1204/11040 Training loss: 1.6643 0.4649 sec/batch\n",
      "Epoch 5/40  Iteration 1205/11040 Training loss: 1.6642 0.4763 sec/batch\n",
      "Epoch 5/40  Iteration 1206/11040 Training loss: 1.6640 0.4889 sec/batch\n",
      "Epoch 5/40  Iteration 1207/11040 Training loss: 1.6633 0.4748 sec/batch\n",
      "Epoch 5/40  Iteration 1208/11040 Training loss: 1.6634 0.4761 sec/batch\n",
      "Epoch 5/40  Iteration 1209/11040 Training loss: 1.6630 0.4801 sec/batch\n",
      "Epoch 5/40  Iteration 1210/11040 Training loss: 1.6629 0.4754 sec/batch\n",
      "Epoch 5/40  Iteration 1211/11040 Training loss: 1.6632 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1212/11040 Training loss: 1.6635 0.4654 sec/batch\n",
      "Epoch 5/40  Iteration 1213/11040 Training loss: 1.6636 0.4757 sec/batch\n",
      "Epoch 5/40  Iteration 1214/11040 Training loss: 1.6635 0.4825 sec/batch\n",
      "Epoch 5/40  Iteration 1215/11040 Training loss: 1.6635 0.4755 sec/batch\n",
      "Epoch 5/40  Iteration 1216/11040 Training loss: 1.6635 0.4738 sec/batch\n",
      "Epoch 5/40  Iteration 1217/11040 Training loss: 1.6638 0.4817 sec/batch\n",
      "Epoch 5/40  Iteration 1218/11040 Training loss: 1.6640 0.4752 sec/batch\n",
      "Epoch 5/40  Iteration 1219/11040 Training loss: 1.6645 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1220/11040 Training loss: 1.6648 0.4752 sec/batch\n",
      "Epoch 5/40  Iteration 1221/11040 Training loss: 1.6648 0.4822 sec/batch\n",
      "Epoch 5/40  Iteration 1222/11040 Training loss: 1.6651 0.4835 sec/batch\n",
      "Epoch 5/40  Iteration 1223/11040 Training loss: 1.6652 0.4734 sec/batch\n",
      "Epoch 5/40  Iteration 1224/11040 Training loss: 1.6655 0.4761 sec/batch\n",
      "Epoch 5/40  Iteration 1225/11040 Training loss: 1.6654 0.4789 sec/batch\n",
      "Epoch 5/40  Iteration 1226/11040 Training loss: 1.6654 0.4599 sec/batch\n",
      "Epoch 5/40  Iteration 1227/11040 Training loss: 1.6657 0.4744 sec/batch\n",
      "Epoch 5/40  Iteration 1228/11040 Training loss: 1.6660 0.4817 sec/batch\n",
      "Epoch 5/40  Iteration 1229/11040 Training loss: 1.6658 0.4747 sec/batch\n",
      "Epoch 5/40  Iteration 1230/11040 Training loss: 1.6659 0.4851 sec/batch\n",
      "Epoch 5/40  Iteration 1231/11040 Training loss: 1.6659 0.4657 sec/batch\n",
      "Epoch 5/40  Iteration 1232/11040 Training loss: 1.6657 0.4768 sec/batch\n",
      "Epoch 5/40  Iteration 1233/11040 Training loss: 1.6656 0.4751 sec/batch\n",
      "Epoch 5/40  Iteration 1234/11040 Training loss: 1.6656 0.4741 sec/batch\n",
      "Epoch 5/40  Iteration 1235/11040 Training loss: 1.6653 0.4754 sec/batch\n",
      "Epoch 5/40  Iteration 1236/11040 Training loss: 1.6650 0.4965 sec/batch\n",
      "Epoch 5/40  Iteration 1237/11040 Training loss: 1.6650 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1238/11040 Training loss: 1.6648 0.4766 sec/batch\n",
      "Epoch 5/40  Iteration 1239/11040 Training loss: 1.6651 0.4761 sec/batch\n",
      "Epoch 5/40  Iteration 1240/11040 Training loss: 1.6654 0.4760 sec/batch\n",
      "Epoch 5/40  Iteration 1241/11040 Training loss: 1.6652 0.4821 sec/batch\n",
      "Epoch 5/40  Iteration 1242/11040 Training loss: 1.6654 0.4758 sec/batch\n",
      "Epoch 5/40  Iteration 1243/11040 Training loss: 1.6653 0.4697 sec/batch\n",
      "Epoch 5/40  Iteration 1244/11040 Training loss: 1.6652 0.4859 sec/batch\n",
      "Epoch 5/40  Iteration 1245/11040 Training loss: 1.6652 0.4736 sec/batch\n",
      "Epoch 5/40  Iteration 1246/11040 Training loss: 1.6651 0.4873 sec/batch\n",
      "Epoch 5/40  Iteration 1247/11040 Training loss: 1.6648 0.4744 sec/batch\n",
      "Epoch 5/40  Iteration 1248/11040 Training loss: 1.6646 0.4757 sec/batch\n",
      "Epoch 5/40  Iteration 1249/11040 Training loss: 1.6644 0.4898 sec/batch\n",
      "Epoch 5/40  Iteration 1250/11040 Training loss: 1.6642 0.4715 sec/batch\n",
      "Epoch 5/40  Iteration 1251/11040 Training loss: 1.6641 0.4802 sec/batch\n",
      "Epoch 5/40  Iteration 1252/11040 Training loss: 1.6638 0.4695 sec/batch\n",
      "Epoch 5/40  Iteration 1253/11040 Training loss: 1.6633 0.4760 sec/batch\n",
      "Epoch 5/40  Iteration 1254/11040 Training loss: 1.6632 0.4844 sec/batch\n",
      "Epoch 5/40  Iteration 1255/11040 Training loss: 1.6632 0.4662 sec/batch\n",
      "Epoch 5/40  Iteration 1256/11040 Training loss: 1.6630 0.4738 sec/batch\n",
      "Epoch 5/40  Iteration 1257/11040 Training loss: 1.6627 0.4652 sec/batch\n",
      "Epoch 5/40  Iteration 1258/11040 Training loss: 1.6625 0.4613 sec/batch\n",
      "Epoch 5/40  Iteration 1259/11040 Training loss: 1.6624 0.4782 sec/batch\n",
      "Epoch 5/40  Iteration 1260/11040 Training loss: 1.6622 0.4713 sec/batch\n",
      "Epoch 5/40  Iteration 1261/11040 Training loss: 1.6618 0.4747 sec/batch\n",
      "Epoch 5/40  Iteration 1262/11040 Training loss: 1.6618 0.4845 sec/batch\n",
      "Epoch 5/40  Iteration 1263/11040 Training loss: 1.6619 0.4653 sec/batch\n",
      "Epoch 5/40  Iteration 1264/11040 Training loss: 1.6619 0.4754 sec/batch\n",
      "Epoch 5/40  Iteration 1265/11040 Training loss: 1.6620 0.4654 sec/batch\n",
      "Epoch 5/40  Iteration 1266/11040 Training loss: 1.6619 0.4758 sec/batch\n",
      "Epoch 5/40  Iteration 1267/11040 Training loss: 1.6620 0.4741 sec/batch\n",
      "Epoch 5/40  Iteration 1268/11040 Training loss: 1.6619 0.4754 sec/batch\n",
      "Epoch 5/40  Iteration 1269/11040 Training loss: 1.6619 0.4795 sec/batch\n",
      "Epoch 5/40  Iteration 1270/11040 Training loss: 1.6620 0.4833 sec/batch\n",
      "Epoch 5/40  Iteration 1271/11040 Training loss: 1.6621 0.4635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40  Iteration 1272/11040 Training loss: 1.6619 0.4738 sec/batch\n",
      "Epoch 5/40  Iteration 1273/11040 Training loss: 1.6615 0.4818 sec/batch\n",
      "Epoch 5/40  Iteration 1274/11040 Training loss: 1.6614 0.4737 sec/batch\n",
      "Epoch 5/40  Iteration 1275/11040 Training loss: 1.6615 0.4772 sec/batch\n",
      "Epoch 5/40  Iteration 1276/11040 Training loss: 1.6614 0.4634 sec/batch\n",
      "Epoch 5/40  Iteration 1277/11040 Training loss: 1.6613 0.4721 sec/batch\n",
      "Epoch 5/40  Iteration 1278/11040 Training loss: 1.6611 0.4851 sec/batch\n",
      "Epoch 5/40  Iteration 1279/11040 Training loss: 1.6610 0.4654 sec/batch\n",
      "Epoch 5/40  Iteration 1280/11040 Training loss: 1.6608 0.4909 sec/batch\n",
      "Epoch 5/40  Iteration 1281/11040 Training loss: 1.6607 0.4809 sec/batch\n",
      "Epoch 5/40  Iteration 1282/11040 Training loss: 1.6606 0.4766 sec/batch\n",
      "Epoch 5/40  Iteration 1283/11040 Training loss: 1.6607 0.4736 sec/batch\n",
      "Epoch 5/40  Iteration 1284/11040 Training loss: 1.6607 0.4651 sec/batch\n",
      "Epoch 5/40  Iteration 1285/11040 Training loss: 1.6605 0.4598 sec/batch\n",
      "Epoch 5/40  Iteration 1286/11040 Training loss: 1.6604 0.4749 sec/batch\n",
      "Epoch 5/40  Iteration 1287/11040 Training loss: 1.6600 0.4758 sec/batch\n",
      "Epoch 5/40  Iteration 1288/11040 Training loss: 1.6600 0.4751 sec/batch\n",
      "Epoch 5/40  Iteration 1289/11040 Training loss: 1.6598 0.4808 sec/batch\n",
      "Epoch 5/40  Iteration 1290/11040 Training loss: 1.6599 0.4746 sec/batch\n",
      "Epoch 5/40  Iteration 1291/11040 Training loss: 1.6601 0.4753 sec/batch\n",
      "Epoch 5/40  Iteration 1292/11040 Training loss: 1.6600 0.4676 sec/batch\n",
      "Epoch 5/40  Iteration 1293/11040 Training loss: 1.6598 0.4740 sec/batch\n",
      "Epoch 5/40  Iteration 1294/11040 Training loss: 1.6596 0.4845 sec/batch\n",
      "Epoch 5/40  Iteration 1295/11040 Training loss: 1.6594 0.4753 sec/batch\n",
      "Epoch 5/40  Iteration 1296/11040 Training loss: 1.6592 0.4687 sec/batch\n",
      "Epoch 5/40  Iteration 1297/11040 Training loss: 1.6590 0.4850 sec/batch\n",
      "Epoch 5/40  Iteration 1298/11040 Training loss: 1.6589 0.4747 sec/batch\n",
      "Epoch 5/40  Iteration 1299/11040 Training loss: 1.6590 0.4846 sec/batch\n",
      "Epoch 5/40  Iteration 1300/11040 Training loss: 1.6590 0.4711 sec/batch\n",
      "Epoch 5/40  Iteration 1301/11040 Training loss: 1.6589 0.4854 sec/batch\n",
      "Epoch 5/40  Iteration 1302/11040 Training loss: 1.6588 0.4771 sec/batch\n",
      "Epoch 5/40  Iteration 1303/11040 Training loss: 1.6589 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1304/11040 Training loss: 1.6588 0.4642 sec/batch\n",
      "Epoch 5/40  Iteration 1305/11040 Training loss: 1.6587 0.4809 sec/batch\n",
      "Epoch 5/40  Iteration 1306/11040 Training loss: 1.6586 0.4762 sec/batch\n",
      "Epoch 5/40  Iteration 1307/11040 Training loss: 1.6586 0.4741 sec/batch\n",
      "Epoch 5/40  Iteration 1308/11040 Training loss: 1.6586 0.4879 sec/batch\n",
      "Epoch 5/40  Iteration 1309/11040 Training loss: 1.6584 0.4844 sec/batch\n",
      "Epoch 5/40  Iteration 1310/11040 Training loss: 1.6582 0.4839 sec/batch\n",
      "Epoch 5/40  Iteration 1311/11040 Training loss: 1.6581 0.4666 sec/batch\n",
      "Epoch 5/40  Iteration 1312/11040 Training loss: 1.6580 0.4770 sec/batch\n",
      "Epoch 5/40  Iteration 1313/11040 Training loss: 1.6578 0.4642 sec/batch\n",
      "Epoch 5/40  Iteration 1314/11040 Training loss: 1.6577 0.4745 sec/batch\n",
      "Epoch 5/40  Iteration 1315/11040 Training loss: 1.6574 0.4947 sec/batch\n",
      "Epoch 5/40  Iteration 1316/11040 Training loss: 1.6574 0.4718 sec/batch\n",
      "Epoch 5/40  Iteration 1317/11040 Training loss: 1.6572 0.4743 sec/batch\n",
      "Epoch 5/40  Iteration 1318/11040 Training loss: 1.6571 0.4765 sec/batch\n",
      "Epoch 5/40  Iteration 1319/11040 Training loss: 1.6569 0.4747 sec/batch\n",
      "Epoch 5/40  Iteration 1320/11040 Training loss: 1.6568 0.4763 sec/batch\n",
      "Epoch 5/40  Iteration 1321/11040 Training loss: 1.6565 0.4808 sec/batch\n",
      "Epoch 5/40  Iteration 1322/11040 Training loss: 1.6563 0.4760 sec/batch\n",
      "Epoch 5/40  Iteration 1323/11040 Training loss: 1.6560 0.4731 sec/batch\n",
      "Epoch 5/40  Iteration 1324/11040 Training loss: 1.6560 0.4817 sec/batch\n",
      "Epoch 5/40  Iteration 1325/11040 Training loss: 1.6559 0.4756 sec/batch\n",
      "Epoch 5/40  Iteration 1326/11040 Training loss: 1.6556 0.4901 sec/batch\n",
      "Epoch 5/40  Iteration 1327/11040 Training loss: 1.6554 0.4754 sec/batch\n",
      "Epoch 5/40  Iteration 1328/11040 Training loss: 1.6554 0.4793 sec/batch\n",
      "Epoch 5/40  Iteration 1329/11040 Training loss: 1.6554 0.4708 sec/batch\n",
      "Epoch 5/40  Iteration 1330/11040 Training loss: 1.6553 0.4736 sec/batch\n",
      "Epoch 5/40  Iteration 1331/11040 Training loss: 1.6550 0.4749 sec/batch\n",
      "Epoch 5/40  Iteration 1332/11040 Training loss: 1.6547 0.4664 sec/batch\n",
      "Epoch 5/40  Iteration 1333/11040 Training loss: 1.6547 0.4755 sec/batch\n",
      "Epoch 5/40  Iteration 1334/11040 Training loss: 1.6547 0.4824 sec/batch\n",
      "Epoch 5/40  Iteration 1335/11040 Training loss: 1.6546 0.4764 sec/batch\n",
      "Epoch 5/40  Iteration 1336/11040 Training loss: 1.6544 0.4712 sec/batch\n",
      "Epoch 5/40  Iteration 1337/11040 Training loss: 1.6542 0.4843 sec/batch\n",
      "Epoch 5/40  Iteration 1338/11040 Training loss: 1.6538 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1339/11040 Training loss: 1.6537 0.4907 sec/batch\n",
      "Epoch 5/40  Iteration 1340/11040 Training loss: 1.6535 0.4807 sec/batch\n",
      "Epoch 5/40  Iteration 1341/11040 Training loss: 1.6532 0.4903 sec/batch\n",
      "Epoch 5/40  Iteration 1342/11040 Training loss: 1.6531 0.4880 sec/batch\n",
      "Epoch 5/40  Iteration 1343/11040 Training loss: 1.6530 0.4779 sec/batch\n",
      "Epoch 5/40  Iteration 1344/11040 Training loss: 1.6528 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1345/11040 Training loss: 1.6526 0.4812 sec/batch\n",
      "Epoch 5/40  Iteration 1346/11040 Training loss: 1.6524 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1347/11040 Training loss: 1.6520 0.4808 sec/batch\n",
      "Epoch 5/40  Iteration 1348/11040 Training loss: 1.6517 0.4783 sec/batch\n",
      "Epoch 5/40  Iteration 1349/11040 Training loss: 1.6515 0.4747 sec/batch\n",
      "Epoch 5/40  Iteration 1350/11040 Training loss: 1.6513 0.4818 sec/batch\n",
      "Epoch 5/40  Iteration 1351/11040 Training loss: 1.6512 0.4756 sec/batch\n",
      "Epoch 5/40  Iteration 1352/11040 Training loss: 1.6510 0.4740 sec/batch\n",
      "Epoch 5/40  Iteration 1353/11040 Training loss: 1.6508 0.4650 sec/batch\n",
      "Epoch 5/40  Iteration 1354/11040 Training loss: 1.6507 0.4748 sec/batch\n",
      "Epoch 5/40  Iteration 1355/11040 Training loss: 1.6504 0.4820 sec/batch\n",
      "Epoch 5/40  Iteration 1356/11040 Training loss: 1.6502 0.4676 sec/batch\n",
      "Epoch 5/40  Iteration 1357/11040 Training loss: 1.6501 0.4892 sec/batch\n",
      "Epoch 5/40  Iteration 1358/11040 Training loss: 1.6501 0.4829 sec/batch\n",
      "Epoch 5/40  Iteration 1359/11040 Training loss: 1.6499 0.4757 sec/batch\n",
      "Epoch 5/40  Iteration 1360/11040 Training loss: 1.6497 0.4809 sec/batch\n",
      "Epoch 5/40  Iteration 1361/11040 Training loss: 1.6496 0.4794 sec/batch\n",
      "Epoch 5/40  Iteration 1362/11040 Training loss: 1.6495 0.4724 sec/batch\n",
      "Epoch 5/40  Iteration 1363/11040 Training loss: 1.6493 0.4652 sec/batch\n",
      "Epoch 5/40  Iteration 1364/11040 Training loss: 1.6491 0.4752 sec/batch\n",
      "Epoch 5/40  Iteration 1365/11040 Training loss: 1.6489 0.4750 sec/batch\n",
      "Epoch 5/40  Iteration 1366/11040 Training loss: 1.6489 0.4782 sec/batch\n",
      "Epoch 5/40  Iteration 1367/11040 Training loss: 1.6488 0.4834 sec/batch\n",
      "Epoch 5/40  Iteration 1368/11040 Training loss: 1.6488 0.4864 sec/batch\n",
      "Epoch 5/40  Iteration 1369/11040 Training loss: 1.6486 0.4782 sec/batch\n",
      "Epoch 5/40  Iteration 1370/11040 Training loss: 1.6485 0.4914 sec/batch\n",
      "Epoch 5/40  Iteration 1371/11040 Training loss: 1.6485 0.4833 sec/batch\n",
      "Epoch 5/40  Iteration 1372/11040 Training loss: 1.6483 0.4793 sec/batch\n",
      "Epoch 5/40  Iteration 1373/11040 Training loss: 1.6483 0.4780 sec/batch\n",
      "Epoch 5/40  Iteration 1374/11040 Training loss: 1.6483 0.4892 sec/batch\n",
      "Epoch 5/40  Iteration 1375/11040 Training loss: 1.6485 0.5032 sec/batch\n",
      "Epoch 5/40  Iteration 1376/11040 Training loss: 1.6485 0.4773 sec/batch\n",
      "Epoch 5/40  Iteration 1377/11040 Training loss: 1.6484 0.4905 sec/batch\n",
      "Epoch 5/40  Iteration 1378/11040 Training loss: 1.6483 0.4839 sec/batch\n",
      "Epoch 5/40  Iteration 1379/11040 Training loss: 1.6481 0.4663 sec/batch\n",
      "Epoch 5/40  Iteration 1380/11040 Training loss: 1.6479 0.4965 sec/batch\n",
      "Epoch 6/40  Iteration 1381/11040 Training loss: 1.6891 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1382/11040 Training loss: 1.6365 0.4825 sec/batch\n",
      "Epoch 6/40  Iteration 1383/11040 Training loss: 1.6341 0.4774 sec/batch\n",
      "Epoch 6/40  Iteration 1384/11040 Training loss: 1.6359 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1385/11040 Training loss: 1.6369 0.4960 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40  Iteration 1386/11040 Training loss: 1.6306 0.4648 sec/batch\n",
      "Epoch 6/40  Iteration 1387/11040 Training loss: 1.6215 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1388/11040 Training loss: 1.6197 0.4762 sec/batch\n",
      "Epoch 6/40  Iteration 1389/11040 Training loss: 1.6177 0.4649 sec/batch\n",
      "Epoch 6/40  Iteration 1390/11040 Training loss: 1.6172 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1391/11040 Training loss: 1.6166 0.4640 sec/batch\n",
      "Epoch 6/40  Iteration 1392/11040 Training loss: 1.6168 0.4579 sec/batch\n",
      "Epoch 6/40  Iteration 1393/11040 Training loss: 1.6142 0.4751 sec/batch\n",
      "Epoch 6/40  Iteration 1394/11040 Training loss: 1.6111 0.4827 sec/batch\n",
      "Epoch 6/40  Iteration 1395/11040 Training loss: 1.6105 0.4753 sec/batch\n",
      "Epoch 6/40  Iteration 1396/11040 Training loss: 1.6103 0.4753 sec/batch\n",
      "Epoch 6/40  Iteration 1397/11040 Training loss: 1.6076 0.4753 sec/batch\n",
      "Epoch 6/40  Iteration 1398/11040 Training loss: 1.6064 0.4773 sec/batch\n",
      "Epoch 6/40  Iteration 1399/11040 Training loss: 1.6056 0.4760 sec/batch\n",
      "Epoch 6/40  Iteration 1400/11040 Training loss: 1.6061 0.4744 sec/batch\n",
      "Epoch 6/40  Iteration 1401/11040 Training loss: 1.6071 0.4777 sec/batch\n",
      "Epoch 6/40  Iteration 1402/11040 Training loss: 1.6081 0.4785 sec/batch\n",
      "Epoch 6/40  Iteration 1403/11040 Training loss: 1.6078 0.4795 sec/batch\n",
      "Epoch 6/40  Iteration 1404/11040 Training loss: 1.6094 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1405/11040 Training loss: 1.6082 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1406/11040 Training loss: 1.6067 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1407/11040 Training loss: 1.6055 0.4746 sec/batch\n",
      "Epoch 6/40  Iteration 1408/11040 Training loss: 1.6049 0.4837 sec/batch\n",
      "Epoch 6/40  Iteration 1409/11040 Training loss: 1.6049 0.4694 sec/batch\n",
      "Epoch 6/40  Iteration 1410/11040 Training loss: 1.6054 0.4631 sec/batch\n",
      "Epoch 6/40  Iteration 1411/11040 Training loss: 1.6048 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1412/11040 Training loss: 1.6045 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1413/11040 Training loss: 1.6048 0.4796 sec/batch\n",
      "Epoch 6/40  Iteration 1414/11040 Training loss: 1.6045 0.4735 sec/batch\n",
      "Epoch 6/40  Iteration 1415/11040 Training loss: 1.6046 0.4796 sec/batch\n",
      "Epoch 6/40  Iteration 1416/11040 Training loss: 1.6048 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1417/11040 Training loss: 1.6045 0.4731 sec/batch\n",
      "Epoch 6/40  Iteration 1418/11040 Training loss: 1.6043 0.4760 sec/batch\n",
      "Epoch 6/40  Iteration 1419/11040 Training loss: 1.6037 0.4631 sec/batch\n",
      "Epoch 6/40  Iteration 1420/11040 Training loss: 1.6043 0.4620 sec/batch\n",
      "Epoch 6/40  Iteration 1421/11040 Training loss: 1.6037 0.4726 sec/batch\n",
      "Epoch 6/40  Iteration 1422/11040 Training loss: 1.6032 0.4701 sec/batch\n",
      "Epoch 6/40  Iteration 1423/11040 Training loss: 1.6037 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1424/11040 Training loss: 1.6039 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1425/11040 Training loss: 1.6037 0.4738 sec/batch\n",
      "Epoch 6/40  Iteration 1426/11040 Training loss: 1.6034 0.4793 sec/batch\n",
      "Epoch 6/40  Iteration 1427/11040 Training loss: 1.6030 0.4689 sec/batch\n",
      "Epoch 6/40  Iteration 1428/11040 Training loss: 1.6023 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1429/11040 Training loss: 1.6014 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1430/11040 Training loss: 1.6016 0.4797 sec/batch\n",
      "Epoch 6/40  Iteration 1431/11040 Training loss: 1.6012 0.4792 sec/batch\n",
      "Epoch 6/40  Iteration 1432/11040 Training loss: 1.6007 0.4799 sec/batch\n",
      "Epoch 6/40  Iteration 1433/11040 Training loss: 1.6009 0.4778 sec/batch\n",
      "Epoch 6/40  Iteration 1434/11040 Training loss: 1.6005 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1435/11040 Training loss: 1.6004 0.4843 sec/batch\n",
      "Epoch 6/40  Iteration 1436/11040 Training loss: 1.6002 0.4747 sec/batch\n",
      "Epoch 6/40  Iteration 1437/11040 Training loss: 1.5998 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1438/11040 Training loss: 1.5998 0.4848 sec/batch\n",
      "Epoch 6/40  Iteration 1439/11040 Training loss: 1.6001 0.4737 sec/batch\n",
      "Epoch 6/40  Iteration 1440/11040 Training loss: 1.5996 0.4795 sec/batch\n",
      "Epoch 6/40  Iteration 1441/11040 Training loss: 1.5994 0.4630 sec/batch\n",
      "Epoch 6/40  Iteration 1442/11040 Training loss: 1.5987 0.4841 sec/batch\n",
      "Epoch 6/40  Iteration 1443/11040 Training loss: 1.5984 0.4748 sec/batch\n",
      "Epoch 6/40  Iteration 1444/11040 Training loss: 1.5981 0.4838 sec/batch\n",
      "Epoch 6/40  Iteration 1445/11040 Training loss: 1.5979 0.4799 sec/batch\n",
      "Epoch 6/40  Iteration 1446/11040 Training loss: 1.5974 0.4781 sec/batch\n",
      "Epoch 6/40  Iteration 1447/11040 Training loss: 1.5969 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1448/11040 Training loss: 1.5969 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1449/11040 Training loss: 1.5965 0.4779 sec/batch\n",
      "Epoch 6/40  Iteration 1450/11040 Training loss: 1.5968 0.4849 sec/batch\n",
      "Epoch 6/40  Iteration 1451/11040 Training loss: 1.5962 0.4949 sec/batch\n",
      "Epoch 6/40  Iteration 1452/11040 Training loss: 1.5957 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1453/11040 Training loss: 1.5950 0.4845 sec/batch\n",
      "Epoch 6/40  Iteration 1454/11040 Training loss: 1.5949 0.4741 sec/batch\n",
      "Epoch 6/40  Iteration 1455/11040 Training loss: 1.5946 0.4684 sec/batch\n",
      "Epoch 6/40  Iteration 1456/11040 Training loss: 1.5946 0.4632 sec/batch\n",
      "Epoch 6/40  Iteration 1457/11040 Training loss: 1.5943 0.4688 sec/batch\n",
      "Epoch 6/40  Iteration 1458/11040 Training loss: 1.5943 0.4740 sec/batch\n",
      "Epoch 6/40  Iteration 1459/11040 Training loss: 1.5938 0.4797 sec/batch\n",
      "Epoch 6/40  Iteration 1460/11040 Training loss: 1.5936 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1461/11040 Training loss: 1.5930 0.4690 sec/batch\n",
      "Epoch 6/40  Iteration 1462/11040 Training loss: 1.5926 0.4730 sec/batch\n",
      "Epoch 6/40  Iteration 1463/11040 Training loss: 1.5922 0.4691 sec/batch\n",
      "Epoch 6/40  Iteration 1464/11040 Training loss: 1.5918 0.4742 sec/batch\n",
      "Epoch 6/40  Iteration 1465/11040 Training loss: 1.5913 0.4679 sec/batch\n",
      "Epoch 6/40  Iteration 1466/11040 Training loss: 1.5911 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1467/11040 Training loss: 1.5907 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1468/11040 Training loss: 1.5898 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1469/11040 Training loss: 1.5898 0.4649 sec/batch\n",
      "Epoch 6/40  Iteration 1470/11040 Training loss: 1.5897 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1471/11040 Training loss: 1.5902 0.4623 sec/batch\n",
      "Epoch 6/40  Iteration 1472/11040 Training loss: 1.5901 0.4793 sec/batch\n",
      "Epoch 6/40  Iteration 1473/11040 Training loss: 1.5897 0.4783 sec/batch\n",
      "Epoch 6/40  Iteration 1474/11040 Training loss: 1.5900 0.4851 sec/batch\n",
      "Epoch 6/40  Iteration 1475/11040 Training loss: 1.5905 0.4838 sec/batch\n",
      "Epoch 6/40  Iteration 1476/11040 Training loss: 1.5908 0.4737 sec/batch\n",
      "Epoch 6/40  Iteration 1477/11040 Training loss: 1.5913 0.4738 sec/batch\n",
      "Epoch 6/40  Iteration 1478/11040 Training loss: 1.5915 0.4696 sec/batch\n",
      "Epoch 6/40  Iteration 1479/11040 Training loss: 1.5915 0.4783 sec/batch\n",
      "Epoch 6/40  Iteration 1480/11040 Training loss: 1.5912 0.4797 sec/batch\n",
      "Epoch 6/40  Iteration 1481/11040 Training loss: 1.5911 0.4783 sec/batch\n",
      "Epoch 6/40  Iteration 1482/11040 Training loss: 1.5909 0.4690 sec/batch\n",
      "Epoch 6/40  Iteration 1483/11040 Training loss: 1.5902 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1484/11040 Training loss: 1.5904 0.4801 sec/batch\n",
      "Epoch 6/40  Iteration 1485/11040 Training loss: 1.5901 0.4782 sec/batch\n",
      "Epoch 6/40  Iteration 1486/11040 Training loss: 1.5900 0.4688 sec/batch\n",
      "Epoch 6/40  Iteration 1487/11040 Training loss: 1.5904 0.4644 sec/batch\n",
      "Epoch 6/40  Iteration 1488/11040 Training loss: 1.5906 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1489/11040 Training loss: 1.5908 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1490/11040 Training loss: 1.5908 0.4842 sec/batch\n",
      "Epoch 6/40  Iteration 1491/11040 Training loss: 1.5909 0.4752 sec/batch\n",
      "Epoch 6/40  Iteration 1492/11040 Training loss: 1.5909 0.4833 sec/batch\n",
      "Epoch 6/40  Iteration 1493/11040 Training loss: 1.5911 0.4996 sec/batch\n",
      "Epoch 6/40  Iteration 1494/11040 Training loss: 1.5913 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1495/11040 Training loss: 1.5918 0.4844 sec/batch\n",
      "Epoch 6/40  Iteration 1496/11040 Training loss: 1.5922 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1497/11040 Training loss: 1.5922 0.4629 sec/batch\n",
      "Epoch 6/40  Iteration 1498/11040 Training loss: 1.5926 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1499/11040 Training loss: 1.5928 0.4791 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40  Iteration 1500/11040 Training loss: 1.5931 0.4791 sec/batch\n",
      "Validation loss: 1.50083 Saving checkpoint!\n",
      "Epoch 6/40  Iteration 1501/11040 Training loss: 1.5937 0.4845 sec/batch\n",
      "Epoch 6/40  Iteration 1502/11040 Training loss: 1.5937 0.4825 sec/batch\n",
      "Epoch 6/40  Iteration 1503/11040 Training loss: 1.5941 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1504/11040 Training loss: 1.5945 0.4793 sec/batch\n",
      "Epoch 6/40  Iteration 1505/11040 Training loss: 1.5945 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1506/11040 Training loss: 1.5948 0.4843 sec/batch\n",
      "Epoch 6/40  Iteration 1507/11040 Training loss: 1.5947 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1508/11040 Training loss: 1.5947 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1509/11040 Training loss: 1.5945 0.4797 sec/batch\n",
      "Epoch 6/40  Iteration 1510/11040 Training loss: 1.5946 0.4777 sec/batch\n",
      "Epoch 6/40  Iteration 1511/11040 Training loss: 1.5944 0.4635 sec/batch\n",
      "Epoch 6/40  Iteration 1512/11040 Training loss: 1.5940 0.4782 sec/batch\n",
      "Epoch 6/40  Iteration 1513/11040 Training loss: 1.5940 0.4795 sec/batch\n",
      "Epoch 6/40  Iteration 1514/11040 Training loss: 1.5939 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1515/11040 Training loss: 1.5941 0.5007 sec/batch\n",
      "Epoch 6/40  Iteration 1516/11040 Training loss: 1.5944 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1517/11040 Training loss: 1.5942 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1518/11040 Training loss: 1.5944 0.4639 sec/batch\n",
      "Epoch 6/40  Iteration 1519/11040 Training loss: 1.5945 0.4643 sec/batch\n",
      "Epoch 6/40  Iteration 1520/11040 Training loss: 1.5944 0.4620 sec/batch\n",
      "Epoch 6/40  Iteration 1521/11040 Training loss: 1.5944 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1522/11040 Training loss: 1.5944 0.4745 sec/batch\n",
      "Epoch 6/40  Iteration 1523/11040 Training loss: 1.5941 0.4631 sec/batch\n",
      "Epoch 6/40  Iteration 1524/11040 Training loss: 1.5939 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1525/11040 Training loss: 1.5938 0.4795 sec/batch\n",
      "Epoch 6/40  Iteration 1526/11040 Training loss: 1.5937 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1527/11040 Training loss: 1.5936 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1528/11040 Training loss: 1.5933 0.4799 sec/batch\n",
      "Epoch 6/40  Iteration 1529/11040 Training loss: 1.5929 0.4629 sec/batch\n",
      "Epoch 6/40  Iteration 1530/11040 Training loss: 1.5929 0.4640 sec/batch\n",
      "Epoch 6/40  Iteration 1531/11040 Training loss: 1.5928 0.4631 sec/batch\n",
      "Epoch 6/40  Iteration 1532/11040 Training loss: 1.5928 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1533/11040 Training loss: 1.5925 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1534/11040 Training loss: 1.5925 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1535/11040 Training loss: 1.5924 0.4649 sec/batch\n",
      "Epoch 6/40  Iteration 1536/11040 Training loss: 1.5922 0.4778 sec/batch\n",
      "Epoch 6/40  Iteration 1537/11040 Training loss: 1.5919 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1538/11040 Training loss: 1.5918 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1539/11040 Training loss: 1.5919 0.4800 sec/batch\n",
      "Epoch 6/40  Iteration 1540/11040 Training loss: 1.5919 0.4784 sec/batch\n",
      "Epoch 6/40  Iteration 1541/11040 Training loss: 1.5921 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1542/11040 Training loss: 1.5920 0.4709 sec/batch\n",
      "Epoch 6/40  Iteration 1543/11040 Training loss: 1.5920 0.4870 sec/batch\n",
      "Epoch 6/40  Iteration 1544/11040 Training loss: 1.5920 0.4687 sec/batch\n",
      "Epoch 6/40  Iteration 1545/11040 Training loss: 1.5919 0.4689 sec/batch\n",
      "Epoch 6/40  Iteration 1546/11040 Training loss: 1.5921 0.4635 sec/batch\n",
      "Epoch 6/40  Iteration 1547/11040 Training loss: 1.5922 0.4793 sec/batch\n",
      "Epoch 6/40  Iteration 1548/11040 Training loss: 1.5921 0.4627 sec/batch\n",
      "Epoch 6/40  Iteration 1549/11040 Training loss: 1.5918 0.4638 sec/batch\n",
      "Epoch 6/40  Iteration 1550/11040 Training loss: 1.5918 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1551/11040 Training loss: 1.5919 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1552/11040 Training loss: 1.5919 0.4844 sec/batch\n",
      "Epoch 6/40  Iteration 1553/11040 Training loss: 1.5918 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1554/11040 Training loss: 1.5917 0.4793 sec/batch\n",
      "Epoch 6/40  Iteration 1555/11040 Training loss: 1.5917 0.4582 sec/batch\n",
      "Epoch 6/40  Iteration 1556/11040 Training loss: 1.5915 0.4630 sec/batch\n",
      "Epoch 6/40  Iteration 1557/11040 Training loss: 1.5915 0.4633 sec/batch\n",
      "Epoch 6/40  Iteration 1558/11040 Training loss: 1.5914 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1559/11040 Training loss: 1.5916 0.4798 sec/batch\n",
      "Epoch 6/40  Iteration 1560/11040 Training loss: 1.5916 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1561/11040 Training loss: 1.5915 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1562/11040 Training loss: 1.5914 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1563/11040 Training loss: 1.5912 0.4792 sec/batch\n",
      "Epoch 6/40  Iteration 1564/11040 Training loss: 1.5912 0.4838 sec/batch\n",
      "Epoch 6/40  Iteration 1565/11040 Training loss: 1.5910 0.4797 sec/batch\n",
      "Epoch 6/40  Iteration 1566/11040 Training loss: 1.5911 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1567/11040 Training loss: 1.5913 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1568/11040 Training loss: 1.5913 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1569/11040 Training loss: 1.5912 0.4793 sec/batch\n",
      "Epoch 6/40  Iteration 1570/11040 Training loss: 1.5909 0.4842 sec/batch\n",
      "Epoch 6/40  Iteration 1571/11040 Training loss: 1.5907 0.4744 sec/batch\n",
      "Epoch 6/40  Iteration 1572/11040 Training loss: 1.5906 0.4680 sec/batch\n",
      "Epoch 6/40  Iteration 1573/11040 Training loss: 1.5904 0.4905 sec/batch\n",
      "Epoch 6/40  Iteration 1574/11040 Training loss: 1.5904 0.4686 sec/batch\n",
      "Epoch 6/40  Iteration 1575/11040 Training loss: 1.5906 0.4834 sec/batch\n",
      "Epoch 6/40  Iteration 1576/11040 Training loss: 1.5905 0.4836 sec/batch\n",
      "Epoch 6/40  Iteration 1577/11040 Training loss: 1.5905 0.4702 sec/batch\n",
      "Epoch 6/40  Iteration 1578/11040 Training loss: 1.5904 0.4741 sec/batch\n",
      "Epoch 6/40  Iteration 1579/11040 Training loss: 1.5905 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1580/11040 Training loss: 1.5904 0.4828 sec/batch\n",
      "Epoch 6/40  Iteration 1581/11040 Training loss: 1.5904 0.4870 sec/batch\n",
      "Epoch 6/40  Iteration 1582/11040 Training loss: 1.5904 0.4691 sec/batch\n",
      "Epoch 6/40  Iteration 1583/11040 Training loss: 1.5905 0.4818 sec/batch\n",
      "Epoch 6/40  Iteration 1584/11040 Training loss: 1.5906 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1585/11040 Training loss: 1.5904 0.4795 sec/batch\n",
      "Epoch 6/40  Iteration 1586/11040 Training loss: 1.5903 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1587/11040 Training loss: 1.5902 0.4692 sec/batch\n",
      "Epoch 6/40  Iteration 1588/11040 Training loss: 1.5902 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1589/11040 Training loss: 1.5900 0.4753 sec/batch\n",
      "Epoch 6/40  Iteration 1590/11040 Training loss: 1.5900 0.4671 sec/batch\n",
      "Epoch 6/40  Iteration 1591/11040 Training loss: 1.5898 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1592/11040 Training loss: 1.5898 0.4805 sec/batch\n",
      "Epoch 6/40  Iteration 1593/11040 Training loss: 1.5897 0.4782 sec/batch\n",
      "Epoch 6/40  Iteration 1594/11040 Training loss: 1.5896 0.4942 sec/batch\n",
      "Epoch 6/40  Iteration 1595/11040 Training loss: 1.5895 0.4792 sec/batch\n",
      "Epoch 6/40  Iteration 1596/11040 Training loss: 1.5894 0.4685 sec/batch\n",
      "Epoch 6/40  Iteration 1597/11040 Training loss: 1.5892 0.4797 sec/batch\n",
      "Epoch 6/40  Iteration 1598/11040 Training loss: 1.5889 0.4739 sec/batch\n",
      "Epoch 6/40  Iteration 1599/11040 Training loss: 1.5888 0.4700 sec/batch\n",
      "Epoch 6/40  Iteration 1600/11040 Training loss: 1.5888 0.4723 sec/batch\n",
      "Epoch 6/40  Iteration 1601/11040 Training loss: 1.5887 0.4695 sec/batch\n",
      "Epoch 6/40  Iteration 1602/11040 Training loss: 1.5885 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1603/11040 Training loss: 1.5883 0.4739 sec/batch\n",
      "Epoch 6/40  Iteration 1604/11040 Training loss: 1.5884 0.4696 sec/batch\n",
      "Epoch 6/40  Iteration 1605/11040 Training loss: 1.5884 0.4621 sec/batch\n",
      "Epoch 6/40  Iteration 1606/11040 Training loss: 1.5883 0.4771 sec/batch\n",
      "Epoch 6/40  Iteration 1607/11040 Training loss: 1.5880 0.4655 sec/batch\n",
      "Epoch 6/40  Iteration 1608/11040 Training loss: 1.5878 0.4788 sec/batch\n",
      "Epoch 6/40  Iteration 1609/11040 Training loss: 1.5878 0.4796 sec/batch\n",
      "Epoch 6/40  Iteration 1610/11040 Training loss: 1.5878 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1611/11040 Training loss: 1.5877 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1612/11040 Training loss: 1.5876 0.4848 sec/batch\n",
      "Epoch 6/40  Iteration 1613/11040 Training loss: 1.5873 0.4736 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40  Iteration 1614/11040 Training loss: 1.5870 0.4840 sec/batch\n",
      "Epoch 6/40  Iteration 1615/11040 Training loss: 1.5870 0.4799 sec/batch\n",
      "Epoch 6/40  Iteration 1616/11040 Training loss: 1.5868 0.4799 sec/batch\n",
      "Epoch 6/40  Iteration 1617/11040 Training loss: 1.5866 0.4781 sec/batch\n",
      "Epoch 6/40  Iteration 1618/11040 Training loss: 1.5865 0.4687 sec/batch\n",
      "Epoch 6/40  Iteration 1619/11040 Training loss: 1.5865 0.4783 sec/batch\n",
      "Epoch 6/40  Iteration 1620/11040 Training loss: 1.5863 0.4643 sec/batch\n",
      "Epoch 6/40  Iteration 1621/11040 Training loss: 1.5862 0.4639 sec/batch\n",
      "Epoch 6/40  Iteration 1622/11040 Training loss: 1.5860 0.4626 sec/batch\n",
      "Epoch 6/40  Iteration 1623/11040 Training loss: 1.5858 0.4641 sec/batch\n",
      "Epoch 6/40  Iteration 1624/11040 Training loss: 1.5855 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1625/11040 Training loss: 1.5853 0.4626 sec/batch\n",
      "Epoch 6/40  Iteration 1626/11040 Training loss: 1.5852 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1627/11040 Training loss: 1.5851 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1628/11040 Training loss: 1.5849 0.4844 sec/batch\n",
      "Epoch 6/40  Iteration 1629/11040 Training loss: 1.5848 0.4736 sec/batch\n",
      "Epoch 6/40  Iteration 1630/11040 Training loss: 1.5847 0.4791 sec/batch\n",
      "Epoch 6/40  Iteration 1631/11040 Training loss: 1.5845 0.4753 sec/batch\n",
      "Epoch 6/40  Iteration 1632/11040 Training loss: 1.5843 0.4678 sec/batch\n",
      "Epoch 6/40  Iteration 1633/11040 Training loss: 1.5843 0.4787 sec/batch\n",
      "Epoch 6/40  Iteration 1634/11040 Training loss: 1.5843 0.4693 sec/batch\n",
      "Epoch 6/40  Iteration 1635/11040 Training loss: 1.5842 0.4691 sec/batch\n",
      "Epoch 6/40  Iteration 1636/11040 Training loss: 1.5841 0.4739 sec/batch\n",
      "Epoch 6/40  Iteration 1637/11040 Training loss: 1.5840 0.4626 sec/batch\n",
      "Epoch 6/40  Iteration 1638/11040 Training loss: 1.5839 0.4638 sec/batch\n",
      "Epoch 6/40  Iteration 1639/11040 Training loss: 1.5837 0.4634 sec/batch\n",
      "Epoch 6/40  Iteration 1640/11040 Training loss: 1.5836 0.4631 sec/batch\n",
      "Epoch 6/40  Iteration 1641/11040 Training loss: 1.5835 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1642/11040 Training loss: 1.5836 0.4794 sec/batch\n",
      "Epoch 6/40  Iteration 1643/11040 Training loss: 1.5835 0.4792 sec/batch\n",
      "Epoch 6/40  Iteration 1644/11040 Training loss: 1.5835 0.4796 sec/batch\n",
      "Epoch 6/40  Iteration 1645/11040 Training loss: 1.5834 0.4789 sec/batch\n",
      "Epoch 6/40  Iteration 1646/11040 Training loss: 1.5833 0.4686 sec/batch\n",
      "Epoch 6/40  Iteration 1647/11040 Training loss: 1.5834 0.4637 sec/batch\n",
      "Epoch 6/40  Iteration 1648/11040 Training loss: 1.5833 0.4839 sec/batch\n",
      "Epoch 6/40  Iteration 1649/11040 Training loss: 1.5833 0.4689 sec/batch\n",
      "Epoch 6/40  Iteration 1650/11040 Training loss: 1.5833 0.4786 sec/batch\n",
      "Epoch 6/40  Iteration 1651/11040 Training loss: 1.5835 0.4798 sec/batch\n",
      "Epoch 6/40  Iteration 1652/11040 Training loss: 1.5836 0.4790 sec/batch\n",
      "Epoch 6/40  Iteration 1653/11040 Training loss: 1.5835 0.4834 sec/batch\n",
      "Epoch 6/40  Iteration 1654/11040 Training loss: 1.5835 0.4809 sec/batch\n",
      "Epoch 6/40  Iteration 1655/11040 Training loss: 1.5833 0.4779 sec/batch\n",
      "Epoch 6/40  Iteration 1656/11040 Training loss: 1.5832 0.4785 sec/batch\n",
      "Epoch 7/40  Iteration 1657/11040 Training loss: 1.6260 0.4634 sec/batch\n",
      "Epoch 7/40  Iteration 1658/11040 Training loss: 1.5705 0.4851 sec/batch\n",
      "Epoch 7/40  Iteration 1659/11040 Training loss: 1.5704 0.4781 sec/batch\n",
      "Epoch 7/40  Iteration 1660/11040 Training loss: 1.5715 0.4797 sec/batch\n",
      "Epoch 7/40  Iteration 1661/11040 Training loss: 1.5732 0.4788 sec/batch\n",
      "Epoch 7/40  Iteration 1662/11040 Training loss: 1.5682 0.4804 sec/batch\n",
      "Epoch 7/40  Iteration 1663/11040 Training loss: 1.5589 0.4776 sec/batch\n",
      "Epoch 7/40  Iteration 1664/11040 Training loss: 1.5581 0.4999 sec/batch\n",
      "Epoch 7/40  Iteration 1665/11040 Training loss: 1.5560 0.4809 sec/batch\n",
      "Epoch 7/40  Iteration 1666/11040 Training loss: 1.5560 0.4790 sec/batch\n",
      "Epoch 7/40  Iteration 1667/11040 Training loss: 1.5552 0.4776 sec/batch\n",
      "Epoch 7/40  Iteration 1668/11040 Training loss: 1.5551 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1669/11040 Training loss: 1.5538 0.4793 sec/batch\n",
      "Epoch 7/40  Iteration 1670/11040 Training loss: 1.5514 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1671/11040 Training loss: 1.5511 0.4797 sec/batch\n",
      "Epoch 7/40  Iteration 1672/11040 Training loss: 1.5518 0.4840 sec/batch\n",
      "Epoch 7/40  Iteration 1673/11040 Training loss: 1.5492 0.4740 sec/batch\n",
      "Epoch 7/40  Iteration 1674/11040 Training loss: 1.5477 0.4736 sec/batch\n",
      "Epoch 7/40  Iteration 1675/11040 Training loss: 1.5465 0.4690 sec/batch\n",
      "Epoch 7/40  Iteration 1676/11040 Training loss: 1.5467 0.4843 sec/batch\n",
      "Epoch 7/40  Iteration 1677/11040 Training loss: 1.5480 0.4691 sec/batch\n",
      "Epoch 7/40  Iteration 1678/11040 Training loss: 1.5492 0.4835 sec/batch\n",
      "Epoch 7/40  Iteration 1679/11040 Training loss: 1.5494 0.4743 sec/batch\n",
      "Epoch 7/40  Iteration 1680/11040 Training loss: 1.5512 0.4738 sec/batch\n",
      "Epoch 7/40  Iteration 1681/11040 Training loss: 1.5500 0.4790 sec/batch\n",
      "Epoch 7/40  Iteration 1682/11040 Training loss: 1.5489 0.4802 sec/batch\n",
      "Epoch 7/40  Iteration 1683/11040 Training loss: 1.5480 0.4777 sec/batch\n",
      "Epoch 7/40  Iteration 1684/11040 Training loss: 1.5475 0.4689 sec/batch\n",
      "Epoch 7/40  Iteration 1685/11040 Training loss: 1.5476 0.4846 sec/batch\n",
      "Epoch 7/40  Iteration 1686/11040 Training loss: 1.5479 0.4689 sec/batch\n",
      "Epoch 7/40  Iteration 1687/11040 Training loss: 1.5478 0.4682 sec/batch\n",
      "Epoch 7/40  Iteration 1688/11040 Training loss: 1.5477 0.4893 sec/batch\n",
      "Epoch 7/40  Iteration 1689/11040 Training loss: 1.5480 0.4687 sec/batch\n",
      "Epoch 7/40  Iteration 1690/11040 Training loss: 1.5481 0.4633 sec/batch\n",
      "Epoch 7/40  Iteration 1691/11040 Training loss: 1.5479 0.4751 sec/batch\n",
      "Epoch 7/40  Iteration 1692/11040 Training loss: 1.5483 0.4840 sec/batch\n",
      "Epoch 7/40  Iteration 1693/11040 Training loss: 1.5482 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1694/11040 Training loss: 1.5477 0.4781 sec/batch\n",
      "Epoch 7/40  Iteration 1695/11040 Training loss: 1.5472 0.4796 sec/batch\n",
      "Epoch 7/40  Iteration 1696/11040 Training loss: 1.5476 0.4693 sec/batch\n",
      "Epoch 7/40  Iteration 1697/11040 Training loss: 1.5469 0.4777 sec/batch\n",
      "Epoch 7/40  Iteration 1698/11040 Training loss: 1.5464 0.4644 sec/batch\n",
      "Epoch 7/40  Iteration 1699/11040 Training loss: 1.5469 0.4779 sec/batch\n",
      "Epoch 7/40  Iteration 1700/11040 Training loss: 1.5470 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1701/11040 Training loss: 1.5469 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1702/11040 Training loss: 1.5466 0.4686 sec/batch\n",
      "Epoch 7/40  Iteration 1703/11040 Training loss: 1.5462 0.4741 sec/batch\n",
      "Epoch 7/40  Iteration 1704/11040 Training loss: 1.5457 0.4681 sec/batch\n",
      "Epoch 7/40  Iteration 1705/11040 Training loss: 1.5447 0.4643 sec/batch\n",
      "Epoch 7/40  Iteration 1706/11040 Training loss: 1.5451 0.4626 sec/batch\n",
      "Epoch 7/40  Iteration 1707/11040 Training loss: 1.5448 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1708/11040 Training loss: 1.5443 0.4799 sec/batch\n",
      "Epoch 7/40  Iteration 1709/11040 Training loss: 1.5445 0.4788 sec/batch\n",
      "Epoch 7/40  Iteration 1710/11040 Training loss: 1.5443 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1711/11040 Training loss: 1.5443 0.4790 sec/batch\n",
      "Epoch 7/40  Iteration 1712/11040 Training loss: 1.5443 0.4692 sec/batch\n",
      "Epoch 7/40  Iteration 1713/11040 Training loss: 1.5438 0.4850 sec/batch\n",
      "Epoch 7/40  Iteration 1714/11040 Training loss: 1.5438 0.4727 sec/batch\n",
      "Epoch 7/40  Iteration 1715/11040 Training loss: 1.5442 0.4635 sec/batch\n",
      "Epoch 7/40  Iteration 1716/11040 Training loss: 1.5438 0.4648 sec/batch\n",
      "Epoch 7/40  Iteration 1717/11040 Training loss: 1.5437 0.4739 sec/batch\n",
      "Epoch 7/40  Iteration 1718/11040 Training loss: 1.5432 0.4681 sec/batch\n",
      "Epoch 7/40  Iteration 1719/11040 Training loss: 1.5430 0.4845 sec/batch\n",
      "Epoch 7/40  Iteration 1720/11040 Training loss: 1.5429 0.4687 sec/batch\n",
      "Epoch 7/40  Iteration 1721/11040 Training loss: 1.5427 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1722/11040 Training loss: 1.5421 0.4794 sec/batch\n",
      "Epoch 7/40  Iteration 1723/11040 Training loss: 1.5418 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1724/11040 Training loss: 1.5419 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1725/11040 Training loss: 1.5416 0.4835 sec/batch\n",
      "Epoch 7/40  Iteration 1726/11040 Training loss: 1.5418 0.4735 sec/batch\n",
      "Epoch 7/40  Iteration 1727/11040 Training loss: 1.5414 0.4693 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40  Iteration 1728/11040 Training loss: 1.5408 0.4790 sec/batch\n",
      "Epoch 7/40  Iteration 1729/11040 Training loss: 1.5401 0.4630 sec/batch\n",
      "Epoch 7/40  Iteration 1730/11040 Training loss: 1.5400 0.4592 sec/batch\n",
      "Epoch 7/40  Iteration 1731/11040 Training loss: 1.5396 0.4780 sec/batch\n",
      "Epoch 7/40  Iteration 1732/11040 Training loss: 1.5397 0.4796 sec/batch\n",
      "Epoch 7/40  Iteration 1733/11040 Training loss: 1.5394 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1734/11040 Training loss: 1.5394 0.4777 sec/batch\n",
      "Epoch 7/40  Iteration 1735/11040 Training loss: 1.5389 0.4846 sec/batch\n",
      "Epoch 7/40  Iteration 1736/11040 Training loss: 1.5387 0.4742 sec/batch\n",
      "Epoch 7/40  Iteration 1737/11040 Training loss: 1.5382 0.4837 sec/batch\n",
      "Epoch 7/40  Iteration 1738/11040 Training loss: 1.5380 0.4740 sec/batch\n",
      "Epoch 7/40  Iteration 1739/11040 Training loss: 1.5376 0.4689 sec/batch\n",
      "Epoch 7/40  Iteration 1740/11040 Training loss: 1.5373 0.4797 sec/batch\n",
      "Epoch 7/40  Iteration 1741/11040 Training loss: 1.5369 0.4630 sec/batch\n",
      "Epoch 7/40  Iteration 1742/11040 Training loss: 1.5367 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1743/11040 Training loss: 1.5362 0.4806 sec/batch\n",
      "Epoch 7/40  Iteration 1744/11040 Training loss: 1.5354 0.4774 sec/batch\n",
      "Epoch 7/40  Iteration 1745/11040 Training loss: 1.5354 0.4794 sec/batch\n",
      "Epoch 7/40  Iteration 1746/11040 Training loss: 1.5355 0.4844 sec/batch\n",
      "Epoch 7/40  Iteration 1747/11040 Training loss: 1.5360 0.4804 sec/batch\n",
      "Epoch 7/40  Iteration 1748/11040 Training loss: 1.5359 0.4779 sec/batch\n",
      "Epoch 7/40  Iteration 1749/11040 Training loss: 1.5356 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1750/11040 Training loss: 1.5359 0.4797 sec/batch\n",
      "Epoch 7/40  Iteration 1751/11040 Training loss: 1.5364 0.4782 sec/batch\n",
      "Epoch 7/40  Iteration 1752/11040 Training loss: 1.5366 0.4849 sec/batch\n",
      "Epoch 7/40  Iteration 1753/11040 Training loss: 1.5371 0.4793 sec/batch\n",
      "Epoch 7/40  Iteration 1754/11040 Training loss: 1.5374 0.4786 sec/batch\n",
      "Epoch 7/40  Iteration 1755/11040 Training loss: 1.5374 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1756/11040 Training loss: 1.5371 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1757/11040 Training loss: 1.5371 0.4690 sec/batch\n",
      "Epoch 7/40  Iteration 1758/11040 Training loss: 1.5370 0.4796 sec/batch\n",
      "Epoch 7/40  Iteration 1759/11040 Training loss: 1.5365 0.4629 sec/batch\n",
      "Epoch 7/40  Iteration 1760/11040 Training loss: 1.5367 0.4791 sec/batch\n",
      "Epoch 7/40  Iteration 1761/11040 Training loss: 1.5364 0.4786 sec/batch\n",
      "Epoch 7/40  Iteration 1762/11040 Training loss: 1.5364 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1763/11040 Training loss: 1.5367 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1764/11040 Training loss: 1.5369 0.4840 sec/batch\n",
      "Epoch 7/40  Iteration 1765/11040 Training loss: 1.5372 0.4739 sec/batch\n",
      "Epoch 7/40  Iteration 1766/11040 Training loss: 1.5372 0.4843 sec/batch\n",
      "Epoch 7/40  Iteration 1767/11040 Training loss: 1.5373 0.4793 sec/batch\n",
      "Epoch 7/40  Iteration 1768/11040 Training loss: 1.5374 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1769/11040 Training loss: 1.5377 0.4844 sec/batch\n",
      "Epoch 7/40  Iteration 1770/11040 Training loss: 1.5379 0.4749 sec/batch\n",
      "Epoch 7/40  Iteration 1771/11040 Training loss: 1.5383 0.4829 sec/batch\n",
      "Epoch 7/40  Iteration 1772/11040 Training loss: 1.5387 0.4740 sec/batch\n",
      "Epoch 7/40  Iteration 1773/11040 Training loss: 1.5388 0.4701 sec/batch\n",
      "Epoch 7/40  Iteration 1774/11040 Training loss: 1.5391 0.4726 sec/batch\n",
      "Epoch 7/40  Iteration 1775/11040 Training loss: 1.5393 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1776/11040 Training loss: 1.5396 0.4796 sec/batch\n",
      "Epoch 7/40  Iteration 1777/11040 Training loss: 1.5397 0.4840 sec/batch\n",
      "Epoch 7/40  Iteration 1778/11040 Training loss: 1.5397 0.4760 sec/batch\n",
      "Epoch 7/40  Iteration 1779/11040 Training loss: 1.5401 0.4768 sec/batch\n",
      "Epoch 7/40  Iteration 1780/11040 Training loss: 1.5405 0.4788 sec/batch\n",
      "Epoch 7/40  Iteration 1781/11040 Training loss: 1.5405 0.4794 sec/batch\n",
      "Epoch 7/40  Iteration 1782/11040 Training loss: 1.5408 0.4692 sec/batch\n",
      "Epoch 7/40  Iteration 1783/11040 Training loss: 1.5408 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1784/11040 Training loss: 1.5407 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1785/11040 Training loss: 1.5406 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1786/11040 Training loss: 1.5406 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1787/11040 Training loss: 1.5404 0.4750 sec/batch\n",
      "Epoch 7/40  Iteration 1788/11040 Training loss: 1.5401 0.4628 sec/batch\n",
      "Epoch 7/40  Iteration 1789/11040 Training loss: 1.5401 0.4753 sec/batch\n",
      "Epoch 7/40  Iteration 1790/11040 Training loss: 1.5400 0.4728 sec/batch\n",
      "Epoch 7/40  Iteration 1791/11040 Training loss: 1.5403 0.4782 sec/batch\n",
      "Epoch 7/40  Iteration 1792/11040 Training loss: 1.5405 0.4845 sec/batch\n",
      "Epoch 7/40  Iteration 1793/11040 Training loss: 1.5403 0.4688 sec/batch\n",
      "Epoch 7/40  Iteration 1794/11040 Training loss: 1.5406 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1795/11040 Training loss: 1.5406 0.4834 sec/batch\n",
      "Epoch 7/40  Iteration 1796/11040 Training loss: 1.5406 0.4687 sec/batch\n",
      "Epoch 7/40  Iteration 1797/11040 Training loss: 1.5406 0.4582 sec/batch\n",
      "Epoch 7/40  Iteration 1798/11040 Training loss: 1.5407 0.4686 sec/batch\n",
      "Epoch 7/40  Iteration 1799/11040 Training loss: 1.5405 0.4639 sec/batch\n",
      "Epoch 7/40  Iteration 1800/11040 Training loss: 1.5403 0.4646 sec/batch\n",
      "Epoch 7/40  Iteration 1801/11040 Training loss: 1.5402 0.4780 sec/batch\n",
      "Epoch 7/40  Iteration 1802/11040 Training loss: 1.5401 0.4793 sec/batch\n",
      "Epoch 7/40  Iteration 1803/11040 Training loss: 1.5400 0.4786 sec/batch\n",
      "Epoch 7/40  Iteration 1804/11040 Training loss: 1.5398 0.4790 sec/batch\n",
      "Epoch 7/40  Iteration 1805/11040 Training loss: 1.5395 0.4802 sec/batch\n",
      "Epoch 7/40  Iteration 1806/11040 Training loss: 1.5394 0.4779 sec/batch\n",
      "Epoch 7/40  Iteration 1807/11040 Training loss: 1.5394 0.4797 sec/batch\n",
      "Epoch 7/40  Iteration 1808/11040 Training loss: 1.5394 0.4735 sec/batch\n",
      "Epoch 7/40  Iteration 1809/11040 Training loss: 1.5392 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1810/11040 Training loss: 1.5391 0.4799 sec/batch\n",
      "Epoch 7/40  Iteration 1811/11040 Training loss: 1.5391 0.4788 sec/batch\n",
      "Epoch 7/40  Iteration 1812/11040 Training loss: 1.5389 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1813/11040 Training loss: 1.5386 0.4739 sec/batch\n",
      "Epoch 7/40  Iteration 1814/11040 Training loss: 1.5386 0.4850 sec/batch\n",
      "Epoch 7/40  Iteration 1815/11040 Training loss: 1.5387 0.4838 sec/batch\n",
      "Epoch 7/40  Iteration 1816/11040 Training loss: 1.5388 0.4740 sec/batch\n",
      "Epoch 7/40  Iteration 1817/11040 Training loss: 1.5389 0.4686 sec/batch\n",
      "Epoch 7/40  Iteration 1818/11040 Training loss: 1.5388 0.4790 sec/batch\n",
      "Epoch 7/40  Iteration 1819/11040 Training loss: 1.5389 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1820/11040 Training loss: 1.5390 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1821/11040 Training loss: 1.5390 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1822/11040 Training loss: 1.5392 0.4791 sec/batch\n",
      "Epoch 7/40  Iteration 1823/11040 Training loss: 1.5393 0.4793 sec/batch\n",
      "Epoch 7/40  Iteration 1824/11040 Training loss: 1.5392 0.4769 sec/batch\n",
      "Epoch 7/40  Iteration 1825/11040 Training loss: 1.5389 0.4705 sec/batch\n",
      "Epoch 7/40  Iteration 1826/11040 Training loss: 1.5389 0.4646 sec/batch\n",
      "Epoch 7/40  Iteration 1827/11040 Training loss: 1.5390 0.4630 sec/batch\n",
      "Epoch 7/40  Iteration 1828/11040 Training loss: 1.5390 0.4632 sec/batch\n",
      "Epoch 7/40  Iteration 1829/11040 Training loss: 1.5390 0.4633 sec/batch\n",
      "Epoch 7/40  Iteration 1830/11040 Training loss: 1.5389 0.4787 sec/batch\n",
      "Epoch 7/40  Iteration 1831/11040 Training loss: 1.5388 0.4810 sec/batch\n",
      "Epoch 7/40  Iteration 1832/11040 Training loss: 1.5387 0.4774 sec/batch\n",
      "Epoch 7/40  Iteration 1833/11040 Training loss: 1.5387 0.4843 sec/batch\n",
      "Epoch 7/40  Iteration 1834/11040 Training loss: 1.5386 0.4739 sec/batch\n",
      "Epoch 7/40  Iteration 1835/11040 Training loss: 1.5388 0.4846 sec/batch\n",
      "Epoch 7/40  Iteration 1836/11040 Training loss: 1.5389 0.4835 sec/batch\n",
      "Epoch 7/40  Iteration 1837/11040 Training loss: 1.5388 0.4689 sec/batch\n",
      "Epoch 7/40  Iteration 1838/11040 Training loss: 1.5387 0.4801 sec/batch\n",
      "Epoch 7/40  Iteration 1839/11040 Training loss: 1.5385 0.4778 sec/batch\n",
      "Epoch 7/40  Iteration 1840/11040 Training loss: 1.5386 0.4849 sec/batch\n",
      "Epoch 7/40  Iteration 1841/11040 Training loss: 1.5385 0.4733 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40  Iteration 1842/11040 Training loss: 1.5387 0.4865 sec/batch\n",
      "Epoch 7/40  Iteration 1843/11040 Training loss: 1.5389 0.4735 sec/batch\n",
      "Epoch 7/40  Iteration 1844/11040 Training loss: 1.5389 0.4825 sec/batch\n",
      "Epoch 7/40  Iteration 1845/11040 Training loss: 1.5387 0.4786 sec/batch\n",
      "Epoch 7/40  Iteration 1846/11040 Training loss: 1.5386 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1847/11040 Training loss: 1.5384 0.4796 sec/batch\n",
      "Epoch 7/40  Iteration 1848/11040 Training loss: 1.5383 0.4794 sec/batch\n",
      "Epoch 7/40  Iteration 1849/11040 Training loss: 1.5381 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1850/11040 Training loss: 1.5381 0.4841 sec/batch\n",
      "Epoch 7/40  Iteration 1851/11040 Training loss: 1.5383 0.4739 sec/batch\n",
      "Epoch 7/40  Iteration 1852/11040 Training loss: 1.5383 0.4629 sec/batch\n",
      "Epoch 7/40  Iteration 1853/11040 Training loss: 1.5383 0.4648 sec/batch\n",
      "Epoch 7/40  Iteration 1854/11040 Training loss: 1.5383 0.4681 sec/batch\n",
      "Epoch 7/40  Iteration 1855/11040 Training loss: 1.5385 0.4731 sec/batch\n",
      "Epoch 7/40  Iteration 1856/11040 Training loss: 1.5385 0.4640 sec/batch\n",
      "Epoch 7/40  Iteration 1857/11040 Training loss: 1.5385 0.4633 sec/batch\n",
      "Epoch 7/40  Iteration 1858/11040 Training loss: 1.5385 0.4791 sec/batch\n",
      "Epoch 7/40  Iteration 1859/11040 Training loss: 1.5386 0.4800 sec/batch\n",
      "Epoch 7/40  Iteration 1860/11040 Training loss: 1.5387 0.4692 sec/batch\n",
      "Epoch 7/40  Iteration 1861/11040 Training loss: 1.5386 0.4780 sec/batch\n",
      "Epoch 7/40  Iteration 1862/11040 Training loss: 1.5385 0.4628 sec/batch\n",
      "Epoch 7/40  Iteration 1863/11040 Training loss: 1.5385 0.4798 sec/batch\n",
      "Epoch 7/40  Iteration 1864/11040 Training loss: 1.5384 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1865/11040 Training loss: 1.5383 0.4799 sec/batch\n",
      "Epoch 7/40  Iteration 1866/11040 Training loss: 1.5383 0.4785 sec/batch\n",
      "Epoch 7/40  Iteration 1867/11040 Training loss: 1.5381 0.4834 sec/batch\n",
      "Epoch 7/40  Iteration 1868/11040 Training loss: 1.5381 0.4906 sec/batch\n",
      "Epoch 7/40  Iteration 1869/11040 Training loss: 1.5381 0.4840 sec/batch\n",
      "Epoch 7/40  Iteration 1870/11040 Training loss: 1.5380 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1871/11040 Training loss: 1.5378 0.4845 sec/batch\n",
      "Epoch 7/40  Iteration 1872/11040 Training loss: 1.5378 0.4802 sec/batch\n",
      "Epoch 7/40  Iteration 1873/11040 Training loss: 1.5376 0.4723 sec/batch\n",
      "Epoch 7/40  Iteration 1874/11040 Training loss: 1.5374 0.5005 sec/batch\n",
      "Epoch 7/40  Iteration 1875/11040 Training loss: 1.5372 0.4947 sec/batch\n",
      "Epoch 7/40  Iteration 1876/11040 Training loss: 1.5372 0.4886 sec/batch\n",
      "Epoch 7/40  Iteration 1877/11040 Training loss: 1.5372 0.4794 sec/batch\n",
      "Epoch 7/40  Iteration 1878/11040 Training loss: 1.5369 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1879/11040 Training loss: 1.5368 0.4686 sec/batch\n",
      "Epoch 7/40  Iteration 1880/11040 Training loss: 1.5369 0.4847 sec/batch\n",
      "Epoch 7/40  Iteration 1881/11040 Training loss: 1.5369 0.4788 sec/batch\n",
      "Epoch 7/40  Iteration 1882/11040 Training loss: 1.5369 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1883/11040 Training loss: 1.5367 0.4743 sec/batch\n",
      "Epoch 7/40  Iteration 1884/11040 Training loss: 1.5365 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1885/11040 Training loss: 1.5366 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1886/11040 Training loss: 1.5366 0.4629 sec/batch\n",
      "Epoch 7/40  Iteration 1887/11040 Training loss: 1.5365 0.4799 sec/batch\n",
      "Epoch 7/40  Iteration 1888/11040 Training loss: 1.5365 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1889/11040 Training loss: 1.5363 0.4783 sec/batch\n",
      "Epoch 7/40  Iteration 1890/11040 Training loss: 1.5361 0.4688 sec/batch\n",
      "Epoch 7/40  Iteration 1891/11040 Training loss: 1.5361 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1892/11040 Training loss: 1.5360 0.4794 sec/batch\n",
      "Epoch 7/40  Iteration 1893/11040 Training loss: 1.5358 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1894/11040 Training loss: 1.5358 0.4839 sec/batch\n",
      "Epoch 7/40  Iteration 1895/11040 Training loss: 1.5358 0.4744 sec/batch\n",
      "Epoch 7/40  Iteration 1896/11040 Training loss: 1.5357 0.4840 sec/batch\n",
      "Epoch 7/40  Iteration 1897/11040 Training loss: 1.5356 0.4686 sec/batch\n",
      "Epoch 7/40  Iteration 1898/11040 Training loss: 1.5355 0.4742 sec/batch\n",
      "Epoch 7/40  Iteration 1899/11040 Training loss: 1.5353 0.4791 sec/batch\n",
      "Epoch 7/40  Iteration 1900/11040 Training loss: 1.5351 0.4684 sec/batch\n",
      "Epoch 7/40  Iteration 1901/11040 Training loss: 1.5349 0.4688 sec/batch\n",
      "Epoch 7/40  Iteration 1902/11040 Training loss: 1.5348 0.4891 sec/batch\n",
      "Epoch 7/40  Iteration 1903/11040 Training loss: 1.5347 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1904/11040 Training loss: 1.5346 0.4792 sec/batch\n",
      "Epoch 7/40  Iteration 1905/11040 Training loss: 1.5344 0.4744 sec/batch\n",
      "Epoch 7/40  Iteration 1906/11040 Training loss: 1.5344 0.4838 sec/batch\n",
      "Epoch 7/40  Iteration 1907/11040 Training loss: 1.5342 0.4844 sec/batch\n",
      "Epoch 7/40  Iteration 1908/11040 Training loss: 1.5340 0.4795 sec/batch\n",
      "Epoch 7/40  Iteration 1909/11040 Training loss: 1.5340 0.4894 sec/batch\n",
      "Epoch 7/40  Iteration 1910/11040 Training loss: 1.5341 0.4796 sec/batch\n",
      "Epoch 7/40  Iteration 1911/11040 Training loss: 1.5340 0.4743 sec/batch\n",
      "Epoch 7/40  Iteration 1912/11040 Training loss: 1.5339 0.4731 sec/batch\n",
      "Epoch 7/40  Iteration 1913/11040 Training loss: 1.5338 0.4685 sec/batch\n",
      "Epoch 7/40  Iteration 1914/11040 Training loss: 1.5338 0.4631 sec/batch\n",
      "Epoch 7/40  Iteration 1915/11040 Training loss: 1.5337 0.4795 sec/batch\n",
      "Epoch 7/40  Iteration 1916/11040 Training loss: 1.5335 0.4793 sec/batch\n",
      "Epoch 7/40  Iteration 1917/11040 Training loss: 1.5334 0.4851 sec/batch\n",
      "Epoch 7/40  Iteration 1918/11040 Training loss: 1.5335 0.4779 sec/batch\n",
      "Epoch 7/40  Iteration 1919/11040 Training loss: 1.5335 0.4788 sec/batch\n",
      "Epoch 7/40  Iteration 1920/11040 Training loss: 1.5335 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1921/11040 Training loss: 1.5334 0.4739 sec/batch\n",
      "Epoch 7/40  Iteration 1922/11040 Training loss: 1.5333 0.4842 sec/batch\n",
      "Epoch 7/40  Iteration 1923/11040 Training loss: 1.5333 0.4744 sec/batch\n",
      "Epoch 7/40  Iteration 1924/11040 Training loss: 1.5333 0.4685 sec/batch\n",
      "Epoch 7/40  Iteration 1925/11040 Training loss: 1.5333 0.4654 sec/batch\n",
      "Epoch 7/40  Iteration 1926/11040 Training loss: 1.5333 0.4777 sec/batch\n",
      "Epoch 7/40  Iteration 1927/11040 Training loss: 1.5336 0.4789 sec/batch\n",
      "Epoch 7/40  Iteration 1928/11040 Training loss: 1.5337 0.5009 sec/batch\n",
      "Epoch 7/40  Iteration 1929/11040 Training loss: 1.5337 0.4783 sec/batch\n",
      "Epoch 7/40  Iteration 1930/11040 Training loss: 1.5336 0.4782 sec/batch\n",
      "Epoch 7/40  Iteration 1931/11040 Training loss: 1.5335 0.4844 sec/batch\n",
      "Epoch 7/40  Iteration 1932/11040 Training loss: 1.5334 0.4807 sec/batch\n",
      "Epoch 8/40  Iteration 1933/11040 Training loss: 1.5802 0.4782 sec/batch\n",
      "Epoch 8/40  Iteration 1934/11040 Training loss: 1.5322 0.4695 sec/batch\n",
      "Epoch 8/40  Iteration 1935/11040 Training loss: 1.5298 0.4781 sec/batch\n",
      "Epoch 8/40  Iteration 1936/11040 Training loss: 1.5310 0.4635 sec/batch\n",
      "Epoch 8/40  Iteration 1937/11040 Training loss: 1.5343 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 1938/11040 Training loss: 1.5299 0.4795 sec/batch\n",
      "Epoch 8/40  Iteration 1939/11040 Training loss: 1.5214 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 1940/11040 Training loss: 1.5196 0.4681 sec/batch\n",
      "Epoch 8/40  Iteration 1941/11040 Training loss: 1.5173 0.4796 sec/batch\n",
      "Epoch 8/40  Iteration 1942/11040 Training loss: 1.5184 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 1943/11040 Training loss: 1.5167 0.4670 sec/batch\n",
      "Epoch 8/40  Iteration 1944/11040 Training loss: 1.5163 0.4778 sec/batch\n",
      "Epoch 8/40  Iteration 1945/11040 Training loss: 1.5148 0.4796 sec/batch\n",
      "Epoch 8/40  Iteration 1946/11040 Training loss: 1.5123 0.4677 sec/batch\n",
      "Epoch 8/40  Iteration 1947/11040 Training loss: 1.5116 0.4804 sec/batch\n",
      "Epoch 8/40  Iteration 1948/11040 Training loss: 1.5120 0.4786 sec/batch\n",
      "Epoch 8/40  Iteration 1949/11040 Training loss: 1.5095 0.4891 sec/batch\n",
      "Epoch 8/40  Iteration 1950/11040 Training loss: 1.5081 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 1951/11040 Training loss: 1.5072 0.4896 sec/batch\n",
      "Epoch 8/40  Iteration 1952/11040 Training loss: 1.5073 0.4794 sec/batch\n",
      "Epoch 8/40  Iteration 1953/11040 Training loss: 1.5087 0.4992 sec/batch\n",
      "Epoch 8/40  Iteration 1954/11040 Training loss: 1.5094 0.4794 sec/batch\n",
      "Epoch 8/40  Iteration 1955/11040 Training loss: 1.5096 0.4843 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40  Iteration 1956/11040 Training loss: 1.5110 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 1957/11040 Training loss: 1.5096 0.4803 sec/batch\n",
      "Epoch 8/40  Iteration 1958/11040 Training loss: 1.5087 0.4841 sec/batch\n",
      "Epoch 8/40  Iteration 1959/11040 Training loss: 1.5076 0.4788 sec/batch\n",
      "Epoch 8/40  Iteration 1960/11040 Training loss: 1.5073 0.4792 sec/batch\n",
      "Epoch 8/40  Iteration 1961/11040 Training loss: 1.5070 0.4794 sec/batch\n",
      "Epoch 8/40  Iteration 1962/11040 Training loss: 1.5075 0.4781 sec/batch\n",
      "Epoch 8/40  Iteration 1963/11040 Training loss: 1.5069 0.4689 sec/batch\n",
      "Epoch 8/40  Iteration 1964/11040 Training loss: 1.5066 0.4630 sec/batch\n",
      "Epoch 8/40  Iteration 1965/11040 Training loss: 1.5068 0.4804 sec/batch\n",
      "Epoch 8/40  Iteration 1966/11040 Training loss: 1.5064 0.4777 sec/batch\n",
      "Epoch 8/40  Iteration 1967/11040 Training loss: 1.5064 0.4845 sec/batch\n",
      "Epoch 8/40  Iteration 1968/11040 Training loss: 1.5067 0.4839 sec/batch\n",
      "Epoch 8/40  Iteration 1969/11040 Training loss: 1.5066 0.4852 sec/batch\n",
      "Epoch 8/40  Iteration 1970/11040 Training loss: 1.5060 0.4837 sec/batch\n",
      "Epoch 8/40  Iteration 1971/11040 Training loss: 1.5054 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 1972/11040 Training loss: 1.5058 0.4795 sec/batch\n",
      "Epoch 8/40  Iteration 1973/11040 Training loss: 1.5052 0.4835 sec/batch\n",
      "Epoch 8/40  Iteration 1974/11040 Training loss: 1.5044 0.4806 sec/batch\n",
      "Epoch 8/40  Iteration 1975/11040 Training loss: 1.5049 0.4827 sec/batch\n",
      "Epoch 8/40  Iteration 1976/11040 Training loss: 1.5049 0.4853 sec/batch\n",
      "Epoch 8/40  Iteration 1977/11040 Training loss: 1.5047 0.4942 sec/batch\n",
      "Epoch 8/40  Iteration 1978/11040 Training loss: 1.5043 0.4740 sec/batch\n",
      "Epoch 8/40  Iteration 1979/11040 Training loss: 1.5038 0.4687 sec/batch\n",
      "Epoch 8/40  Iteration 1980/11040 Training loss: 1.5032 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 1981/11040 Training loss: 1.5024 0.4853 sec/batch\n",
      "Epoch 8/40  Iteration 1982/11040 Training loss: 1.5027 0.4885 sec/batch\n",
      "Epoch 8/40  Iteration 1983/11040 Training loss: 1.5026 0.4844 sec/batch\n",
      "Epoch 8/40  Iteration 1984/11040 Training loss: 1.5023 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 1985/11040 Training loss: 1.5025 0.4846 sec/batch\n",
      "Epoch 8/40  Iteration 1986/11040 Training loss: 1.5023 0.4896 sec/batch\n",
      "Epoch 8/40  Iteration 1987/11040 Training loss: 1.5023 0.4839 sec/batch\n",
      "Epoch 8/40  Iteration 1988/11040 Training loss: 1.5024 0.4850 sec/batch\n",
      "Epoch 8/40  Iteration 1989/11040 Training loss: 1.5021 0.4945 sec/batch\n",
      "Epoch 8/40  Iteration 1990/11040 Training loss: 1.5020 0.4788 sec/batch\n",
      "Epoch 8/40  Iteration 1991/11040 Training loss: 1.5024 0.4688 sec/batch\n",
      "Epoch 8/40  Iteration 1992/11040 Training loss: 1.5020 0.4797 sec/batch\n",
      "Epoch 8/40  Iteration 1993/11040 Training loss: 1.5019 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 1994/11040 Training loss: 1.5013 0.4841 sec/batch\n",
      "Epoch 8/40  Iteration 1995/11040 Training loss: 1.5009 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 1996/11040 Training loss: 1.5007 0.4701 sec/batch\n",
      "Epoch 8/40  Iteration 1997/11040 Training loss: 1.5006 0.4778 sec/batch\n",
      "Epoch 8/40  Iteration 1998/11040 Training loss: 1.5000 0.4896 sec/batch\n",
      "Epoch 8/40  Iteration 1999/11040 Training loss: 1.4996 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2000/11040 Training loss: 1.4996 0.4896 sec/batch\n",
      "Validation loss: 1.42828 Saving checkpoint!\n",
      "Epoch 8/40  Iteration 2001/11040 Training loss: 1.5007 0.4689 sec/batch\n",
      "Epoch 8/40  Iteration 2002/11040 Training loss: 1.5010 0.4756 sec/batch\n",
      "Epoch 8/40  Iteration 2003/11040 Training loss: 1.5006 0.4837 sec/batch\n",
      "Epoch 8/40  Iteration 2004/11040 Training loss: 1.5001 0.4744 sec/batch\n",
      "Epoch 8/40  Iteration 2005/11040 Training loss: 1.4994 0.4686 sec/batch\n",
      "Epoch 8/40  Iteration 2006/11040 Training loss: 1.4992 0.4859 sec/batch\n",
      "Epoch 8/40  Iteration 2007/11040 Training loss: 1.4990 0.4835 sec/batch\n",
      "Epoch 8/40  Iteration 2008/11040 Training loss: 1.4991 0.4829 sec/batch\n",
      "Epoch 8/40  Iteration 2009/11040 Training loss: 1.4988 0.4802 sec/batch\n",
      "Epoch 8/40  Iteration 2010/11040 Training loss: 1.4986 0.4827 sec/batch\n",
      "Epoch 8/40  Iteration 2011/11040 Training loss: 1.4981 0.4690 sec/batch\n",
      "Epoch 8/40  Iteration 2012/11040 Training loss: 1.4980 0.4723 sec/batch\n",
      "Epoch 8/40  Iteration 2013/11040 Training loss: 1.4975 0.4701 sec/batch\n",
      "Epoch 8/40  Iteration 2014/11040 Training loss: 1.4973 0.4781 sec/batch\n",
      "Epoch 8/40  Iteration 2015/11040 Training loss: 1.4970 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2016/11040 Training loss: 1.4967 0.4735 sec/batch\n",
      "Epoch 8/40  Iteration 2017/11040 Training loss: 1.4965 0.4797 sec/batch\n",
      "Epoch 8/40  Iteration 2018/11040 Training loss: 1.4963 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2019/11040 Training loss: 1.4958 0.4801 sec/batch\n",
      "Epoch 8/40  Iteration 2020/11040 Training loss: 1.4949 0.4780 sec/batch\n",
      "Epoch 8/40  Iteration 2021/11040 Training loss: 1.4949 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 2022/11040 Training loss: 1.4949 0.4802 sec/batch\n",
      "Epoch 8/40  Iteration 2023/11040 Training loss: 1.4954 0.4799 sec/batch\n",
      "Epoch 8/40  Iteration 2024/11040 Training loss: 1.4952 0.4777 sec/batch\n",
      "Epoch 8/40  Iteration 2025/11040 Training loss: 1.4950 0.4840 sec/batch\n",
      "Epoch 8/40  Iteration 2026/11040 Training loss: 1.4953 0.4754 sec/batch\n",
      "Epoch 8/40  Iteration 2027/11040 Training loss: 1.4958 0.4827 sec/batch\n",
      "Epoch 8/40  Iteration 2028/11040 Training loss: 1.4962 0.4794 sec/batch\n",
      "Epoch 8/40  Iteration 2029/11040 Training loss: 1.4966 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2030/11040 Training loss: 1.4969 0.4630 sec/batch\n",
      "Epoch 8/40  Iteration 2031/11040 Training loss: 1.4970 0.4651 sec/batch\n",
      "Epoch 8/40  Iteration 2032/11040 Training loss: 1.4968 0.4777 sec/batch\n",
      "Epoch 8/40  Iteration 2033/11040 Training loss: 1.4968 0.4802 sec/batch\n",
      "Epoch 8/40  Iteration 2034/11040 Training loss: 1.4966 0.4780 sec/batch\n",
      "Epoch 8/40  Iteration 2035/11040 Training loss: 1.4960 0.4763 sec/batch\n",
      "Epoch 8/40  Iteration 2036/11040 Training loss: 1.4962 0.4713 sec/batch\n",
      "Epoch 8/40  Iteration 2037/11040 Training loss: 1.4959 0.4742 sec/batch\n",
      "Epoch 8/40  Iteration 2038/11040 Training loss: 1.4958 0.4742 sec/batch\n",
      "Epoch 8/40  Iteration 2039/11040 Training loss: 1.4961 0.4784 sec/batch\n",
      "Epoch 8/40  Iteration 2040/11040 Training loss: 1.4964 0.4694 sec/batch\n",
      "Epoch 8/40  Iteration 2041/11040 Training loss: 1.4966 0.4682 sec/batch\n",
      "Epoch 8/40  Iteration 2042/11040 Training loss: 1.4966 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2043/11040 Training loss: 1.4968 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2044/11040 Training loss: 1.4971 0.4643 sec/batch\n",
      "Epoch 8/40  Iteration 2045/11040 Training loss: 1.4973 0.4637 sec/batch\n",
      "Epoch 8/40  Iteration 2046/11040 Training loss: 1.4975 0.4722 sec/batch\n",
      "Epoch 8/40  Iteration 2047/11040 Training loss: 1.4979 0.4841 sec/batch\n",
      "Epoch 8/40  Iteration 2048/11040 Training loss: 1.4983 0.4808 sec/batch\n",
      "Epoch 8/40  Iteration 2049/11040 Training loss: 1.4983 0.4781 sec/batch\n",
      "Epoch 8/40  Iteration 2050/11040 Training loss: 1.4987 0.4802 sec/batch\n",
      "Epoch 8/40  Iteration 2051/11040 Training loss: 1.4988 0.4722 sec/batch\n",
      "Epoch 8/40  Iteration 2052/11040 Training loss: 1.4991 0.4687 sec/batch\n",
      "Epoch 8/40  Iteration 2053/11040 Training loss: 1.4992 0.4692 sec/batch\n",
      "Epoch 8/40  Iteration 2054/11040 Training loss: 1.4992 0.4786 sec/batch\n",
      "Epoch 8/40  Iteration 2055/11040 Training loss: 1.4995 0.4861 sec/batch\n",
      "Epoch 8/40  Iteration 2056/11040 Training loss: 1.4999 0.4829 sec/batch\n",
      "Epoch 8/40  Iteration 2057/11040 Training loss: 1.4999 0.4703 sec/batch\n",
      "Epoch 8/40  Iteration 2058/11040 Training loss: 1.5002 0.4719 sec/batch\n",
      "Epoch 8/40  Iteration 2059/11040 Training loss: 1.5001 0.4848 sec/batch\n",
      "Epoch 8/40  Iteration 2060/11040 Training loss: 1.5001 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 2061/11040 Training loss: 1.4999 0.4760 sec/batch\n",
      "Epoch 8/40  Iteration 2062/11040 Training loss: 1.5000 0.4680 sec/batch\n",
      "Epoch 8/40  Iteration 2063/11040 Training loss: 1.4999 0.4771 sec/batch\n",
      "Epoch 8/40  Iteration 2064/11040 Training loss: 1.4996 0.4800 sec/batch\n",
      "Epoch 8/40  Iteration 2065/11040 Training loss: 1.4997 0.4798 sec/batch\n",
      "Epoch 8/40  Iteration 2066/11040 Training loss: 1.4996 0.4845 sec/batch\n",
      "Epoch 8/40  Iteration 2067/11040 Training loss: 1.4998 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 2068/11040 Training loss: 1.5001 0.4729 sec/batch\n",
      "Epoch 8/40  Iteration 2069/11040 Training loss: 1.5000 0.4844 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40  Iteration 2070/11040 Training loss: 1.5002 0.4711 sec/batch\n",
      "Epoch 8/40  Iteration 2071/11040 Training loss: 1.5003 0.4711 sec/batch\n",
      "Epoch 8/40  Iteration 2072/11040 Training loss: 1.5003 0.4789 sec/batch\n",
      "Epoch 8/40  Iteration 2073/11040 Training loss: 1.5003 0.4739 sec/batch\n",
      "Epoch 8/40  Iteration 2074/11040 Training loss: 1.5004 0.4686 sec/batch\n",
      "Epoch 8/40  Iteration 2075/11040 Training loss: 1.5002 0.4643 sec/batch\n",
      "Epoch 8/40  Iteration 2076/11040 Training loss: 1.5001 0.4684 sec/batch\n",
      "Epoch 8/40  Iteration 2077/11040 Training loss: 1.5000 0.4633 sec/batch\n",
      "Epoch 8/40  Iteration 2078/11040 Training loss: 1.4999 0.4798 sec/batch\n",
      "Epoch 8/40  Iteration 2079/11040 Training loss: 1.4999 0.4782 sec/batch\n",
      "Epoch 8/40  Iteration 2080/11040 Training loss: 1.4997 0.4839 sec/batch\n",
      "Epoch 8/40  Iteration 2081/11040 Training loss: 1.4993 0.4799 sec/batch\n",
      "Epoch 8/40  Iteration 2082/11040 Training loss: 1.4993 0.4738 sec/batch\n",
      "Epoch 8/40  Iteration 2083/11040 Training loss: 1.4993 0.4686 sec/batch\n",
      "Epoch 8/40  Iteration 2084/11040 Training loss: 1.4993 0.4690 sec/batch\n",
      "Epoch 8/40  Iteration 2085/11040 Training loss: 1.4991 0.4741 sec/batch\n",
      "Epoch 8/40  Iteration 2086/11040 Training loss: 1.4991 0.4692 sec/batch\n",
      "Epoch 8/40  Iteration 2087/11040 Training loss: 1.4990 0.4792 sec/batch\n",
      "Epoch 8/40  Iteration 2088/11040 Training loss: 1.4989 0.4775 sec/batch\n",
      "Epoch 8/40  Iteration 2089/11040 Training loss: 1.4986 0.4788 sec/batch\n",
      "Epoch 8/40  Iteration 2090/11040 Training loss: 1.4987 0.4809 sec/batch\n",
      "Epoch 8/40  Iteration 2091/11040 Training loss: 1.4989 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2092/11040 Training loss: 1.4989 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2093/11040 Training loss: 1.4990 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 2094/11040 Training loss: 1.4989 0.4778 sec/batch\n",
      "Epoch 8/40  Iteration 2095/11040 Training loss: 1.4990 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2096/11040 Training loss: 1.4990 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2097/11040 Training loss: 1.4991 0.4635 sec/batch\n",
      "Epoch 8/40  Iteration 2098/11040 Training loss: 1.4993 0.4637 sec/batch\n",
      "Epoch 8/40  Iteration 2099/11040 Training loss: 1.4994 0.4789 sec/batch\n",
      "Epoch 8/40  Iteration 2100/11040 Training loss: 1.4993 0.4794 sec/batch\n",
      "Epoch 8/40  Iteration 2101/11040 Training loss: 1.4991 0.4801 sec/batch\n",
      "Epoch 8/40  Iteration 2102/11040 Training loss: 1.4991 0.4773 sec/batch\n",
      "Epoch 8/40  Iteration 2103/11040 Training loss: 1.4993 0.4794 sec/batch\n",
      "Epoch 8/40  Iteration 2104/11040 Training loss: 1.4993 0.4798 sec/batch\n",
      "Epoch 8/40  Iteration 2105/11040 Training loss: 1.4993 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2106/11040 Training loss: 1.4993 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2107/11040 Training loss: 1.4993 0.4801 sec/batch\n",
      "Epoch 8/40  Iteration 2108/11040 Training loss: 1.4992 0.4831 sec/batch\n",
      "Epoch 8/40  Iteration 2109/11040 Training loss: 1.4992 0.4736 sec/batch\n",
      "Epoch 8/40  Iteration 2110/11040 Training loss: 1.4991 0.4701 sec/batch\n",
      "Epoch 8/40  Iteration 2111/11040 Training loss: 1.4994 0.4780 sec/batch\n",
      "Epoch 8/40  Iteration 2112/11040 Training loss: 1.4995 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2113/11040 Training loss: 1.4993 0.4635 sec/batch\n",
      "Epoch 8/40  Iteration 2114/11040 Training loss: 1.4993 0.4642 sec/batch\n",
      "Epoch 8/40  Iteration 2115/11040 Training loss: 1.4991 0.4781 sec/batch\n",
      "Epoch 8/40  Iteration 2116/11040 Training loss: 1.4992 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2117/11040 Training loss: 1.4992 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2118/11040 Training loss: 1.4993 0.4857 sec/batch\n",
      "Epoch 8/40  Iteration 2119/11040 Training loss: 1.4995 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 2120/11040 Training loss: 1.4996 0.4733 sec/batch\n",
      "Epoch 8/40  Iteration 2121/11040 Training loss: 1.4995 0.4685 sec/batch\n",
      "Epoch 8/40  Iteration 2122/11040 Training loss: 1.4994 0.4792 sec/batch\n",
      "Epoch 8/40  Iteration 2123/11040 Training loss: 1.4992 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 2124/11040 Training loss: 1.4992 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2125/11040 Training loss: 1.4991 0.4744 sec/batch\n",
      "Epoch 8/40  Iteration 2126/11040 Training loss: 1.4991 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2127/11040 Training loss: 1.4994 0.4740 sec/batch\n",
      "Epoch 8/40  Iteration 2128/11040 Training loss: 1.4993 0.4689 sec/batch\n",
      "Epoch 8/40  Iteration 2129/11040 Training loss: 1.4993 0.4789 sec/batch\n",
      "Epoch 8/40  Iteration 2130/11040 Training loss: 1.4994 0.4841 sec/batch\n",
      "Epoch 8/40  Iteration 2131/11040 Training loss: 1.4995 0.4671 sec/batch\n",
      "Epoch 8/40  Iteration 2132/11040 Training loss: 1.4995 0.4705 sec/batch\n",
      "Epoch 8/40  Iteration 2133/11040 Training loss: 1.4996 0.4632 sec/batch\n",
      "Epoch 8/40  Iteration 2134/11040 Training loss: 1.4996 0.4744 sec/batch\n",
      "Epoch 8/40  Iteration 2135/11040 Training loss: 1.4998 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 2136/11040 Training loss: 1.4999 0.4847 sec/batch\n",
      "Epoch 8/40  Iteration 2137/11040 Training loss: 1.4999 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2138/11040 Training loss: 1.4998 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2139/11040 Training loss: 1.4997 0.4784 sec/batch\n",
      "Epoch 8/40  Iteration 2140/11040 Training loss: 1.4997 0.4684 sec/batch\n",
      "Epoch 8/40  Iteration 2141/11040 Training loss: 1.4995 0.4798 sec/batch\n",
      "Epoch 8/40  Iteration 2142/11040 Training loss: 1.4995 0.4782 sec/batch\n",
      "Epoch 8/40  Iteration 2143/11040 Training loss: 1.4994 0.4800 sec/batch\n",
      "Epoch 8/40  Iteration 2144/11040 Training loss: 1.4994 0.4796 sec/batch\n",
      "Epoch 8/40  Iteration 2145/11040 Training loss: 1.4994 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 2146/11040 Training loss: 1.4993 0.4626 sec/batch\n",
      "Epoch 8/40  Iteration 2147/11040 Training loss: 1.4992 0.4795 sec/batch\n",
      "Epoch 8/40  Iteration 2148/11040 Training loss: 1.4991 0.4787 sec/batch\n",
      "Epoch 8/40  Iteration 2149/11040 Training loss: 1.4990 0.4799 sec/batch\n",
      "Epoch 8/40  Iteration 2150/11040 Training loss: 1.4988 0.4686 sec/batch\n",
      "Epoch 8/40  Iteration 2151/11040 Training loss: 1.4987 0.4738 sec/batch\n",
      "Epoch 8/40  Iteration 2152/11040 Training loss: 1.4987 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2153/11040 Training loss: 1.4986 0.4836 sec/batch\n",
      "Epoch 8/40  Iteration 2154/11040 Training loss: 1.4984 0.4789 sec/batch\n",
      "Epoch 8/40  Iteration 2155/11040 Training loss: 1.4983 0.4796 sec/batch\n",
      "Epoch 8/40  Iteration 2156/11040 Training loss: 1.4984 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2157/11040 Training loss: 1.4985 0.4689 sec/batch\n",
      "Epoch 8/40  Iteration 2158/11040 Training loss: 1.4984 0.4837 sec/batch\n",
      "Epoch 8/40  Iteration 2159/11040 Training loss: 1.4982 0.4744 sec/batch\n",
      "Epoch 8/40  Iteration 2160/11040 Training loss: 1.4980 0.4682 sec/batch\n",
      "Epoch 8/40  Iteration 2161/11040 Training loss: 1.4981 0.4741 sec/batch\n",
      "Epoch 8/40  Iteration 2162/11040 Training loss: 1.4981 0.4685 sec/batch\n",
      "Epoch 8/40  Iteration 2163/11040 Training loss: 1.4981 0.4738 sec/batch\n",
      "Epoch 8/40  Iteration 2164/11040 Training loss: 1.4980 0.4848 sec/batch\n",
      "Epoch 8/40  Iteration 2165/11040 Training loss: 1.4979 0.4789 sec/batch\n",
      "Epoch 8/40  Iteration 2166/11040 Training loss: 1.4977 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2167/11040 Training loss: 1.4977 0.4802 sec/batch\n",
      "Epoch 8/40  Iteration 2168/11040 Training loss: 1.4976 0.4831 sec/batch\n",
      "Epoch 8/40  Iteration 2169/11040 Training loss: 1.4974 0.4736 sec/batch\n",
      "Epoch 8/40  Iteration 2170/11040 Training loss: 1.4973 0.4703 sec/batch\n",
      "Epoch 8/40  Iteration 2171/11040 Training loss: 1.4974 0.4894 sec/batch\n",
      "Epoch 8/40  Iteration 2172/11040 Training loss: 1.4973 0.4837 sec/batch\n",
      "Epoch 8/40  Iteration 2173/11040 Training loss: 1.4972 0.4798 sec/batch\n",
      "Epoch 8/40  Iteration 2174/11040 Training loss: 1.4971 0.4774 sec/batch\n",
      "Epoch 8/40  Iteration 2175/11040 Training loss: 1.4970 0.4688 sec/batch\n",
      "Epoch 8/40  Iteration 2176/11040 Training loss: 1.4967 0.4902 sec/batch\n",
      "Epoch 8/40  Iteration 2177/11040 Training loss: 1.4966 0.4834 sec/batch\n",
      "Epoch 8/40  Iteration 2178/11040 Training loss: 1.4965 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2179/11040 Training loss: 1.4965 0.4791 sec/batch\n",
      "Epoch 8/40  Iteration 2180/11040 Training loss: 1.4964 0.4687 sec/batch\n",
      "Epoch 8/40  Iteration 2181/11040 Training loss: 1.4963 0.4790 sec/batch\n",
      "Epoch 8/40  Iteration 2182/11040 Training loss: 1.4962 0.4633 sec/batch\n",
      "Epoch 8/40  Iteration 2183/11040 Training loss: 1.4960 0.4791 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40  Iteration 2184/11040 Training loss: 1.4959 0.4843 sec/batch\n",
      "Epoch 8/40  Iteration 2185/11040 Training loss: 1.4959 0.4849 sec/batch\n",
      "Epoch 8/40  Iteration 2186/11040 Training loss: 1.4960 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2187/11040 Training loss: 1.4959 0.4843 sec/batch\n",
      "Epoch 8/40  Iteration 2188/11040 Training loss: 1.4959 0.4793 sec/batch\n",
      "Epoch 8/40  Iteration 2189/11040 Training loss: 1.4958 0.4785 sec/batch\n",
      "Epoch 8/40  Iteration 2190/11040 Training loss: 1.4958 0.4635 sec/batch\n",
      "Epoch 8/40  Iteration 2191/11040 Training loss: 1.4957 0.4798 sec/batch\n",
      "Epoch 8/40  Iteration 2192/11040 Training loss: 1.4956 0.4836 sec/batch\n",
      "Epoch 8/40  Iteration 2193/11040 Training loss: 1.4955 0.4743 sec/batch\n",
      "Epoch 8/40  Iteration 2194/11040 Training loss: 1.4956 0.4839 sec/batch\n",
      "Epoch 8/40  Iteration 2195/11040 Training loss: 1.4955 0.4842 sec/batch\n",
      "Epoch 8/40  Iteration 2196/11040 Training loss: 1.4956 0.4843 sec/batch\n",
      "Epoch 8/40  Iteration 2197/11040 Training loss: 1.4955 0.4752 sec/batch\n",
      "Epoch 8/40  Iteration 2198/11040 Training loss: 1.4955 0.4838 sec/batch\n",
      "Epoch 8/40  Iteration 2199/11040 Training loss: 1.4956 0.4802 sec/batch\n",
      "Epoch 8/40  Iteration 2200/11040 Training loss: 1.4955 0.4731 sec/batch\n",
      "Epoch 8/40  Iteration 2201/11040 Training loss: 1.4956 0.4849 sec/batch\n",
      "Epoch 8/40  Iteration 2202/11040 Training loss: 1.4956 0.4781 sec/batch\n",
      "Epoch 8/40  Iteration 2203/11040 Training loss: 1.4959 0.4784 sec/batch\n",
      "Epoch 8/40  Iteration 2204/11040 Training loss: 1.4960 0.4800 sec/batch\n",
      "Epoch 8/40  Iteration 2205/11040 Training loss: 1.4960 0.4632 sec/batch\n",
      "Epoch 8/40  Iteration 2206/11040 Training loss: 1.4960 0.4631 sec/batch\n",
      "Epoch 8/40  Iteration 2207/11040 Training loss: 1.4959 0.4642 sec/batch\n",
      "Epoch 8/40  Iteration 2208/11040 Training loss: 1.4958 0.4688 sec/batch\n",
      "Epoch 9/40  Iteration 2209/11040 Training loss: 1.5542 0.4781 sec/batch\n",
      "Epoch 9/40  Iteration 2210/11040 Training loss: 1.5044 0.4641 sec/batch\n",
      "Epoch 9/40  Iteration 2211/11040 Training loss: 1.4979 0.4645 sec/batch\n",
      "Epoch 9/40  Iteration 2212/11040 Training loss: 1.4989 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2213/11040 Training loss: 1.4995 0.4771 sec/batch\n",
      "Epoch 9/40  Iteration 2214/11040 Training loss: 1.4952 0.4800 sec/batch\n",
      "Epoch 9/40  Iteration 2215/11040 Training loss: 1.4877 0.4836 sec/batch\n",
      "Epoch 9/40  Iteration 2216/11040 Training loss: 1.4863 0.4742 sec/batch\n",
      "Epoch 9/40  Iteration 2217/11040 Training loss: 1.4831 0.4844 sec/batch\n",
      "Epoch 9/40  Iteration 2218/11040 Training loss: 1.4840 0.4805 sec/batch\n",
      "Epoch 9/40  Iteration 2219/11040 Training loss: 1.4823 0.4878 sec/batch\n",
      "Epoch 9/40  Iteration 2220/11040 Training loss: 1.4821 0.4689 sec/batch\n",
      "Epoch 9/40  Iteration 2221/11040 Training loss: 1.4800 0.4686 sec/batch\n",
      "Epoch 9/40  Iteration 2222/11040 Training loss: 1.4769 0.4740 sec/batch\n",
      "Epoch 9/40  Iteration 2223/11040 Training loss: 1.4771 0.4685 sec/batch\n",
      "Epoch 9/40  Iteration 2224/11040 Training loss: 1.4772 0.4796 sec/batch\n",
      "Epoch 9/40  Iteration 2225/11040 Training loss: 1.4753 0.4834 sec/batch\n",
      "Epoch 9/40  Iteration 2226/11040 Training loss: 1.4740 0.4744 sec/batch\n",
      "Epoch 9/40  Iteration 2227/11040 Training loss: 1.4730 0.4838 sec/batch\n",
      "Epoch 9/40  Iteration 2228/11040 Training loss: 1.4735 0.4845 sec/batch\n",
      "Epoch 9/40  Iteration 2229/11040 Training loss: 1.4743 0.4899 sec/batch\n",
      "Epoch 9/40  Iteration 2230/11040 Training loss: 1.4755 0.4835 sec/batch\n",
      "Epoch 9/40  Iteration 2231/11040 Training loss: 1.4757 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2232/11040 Training loss: 1.4774 0.4848 sec/batch\n",
      "Epoch 9/40  Iteration 2233/11040 Training loss: 1.4765 0.4735 sec/batch\n",
      "Epoch 9/40  Iteration 2234/11040 Training loss: 1.4757 0.4857 sec/batch\n",
      "Epoch 9/40  Iteration 2235/11040 Training loss: 1.4748 0.4768 sec/batch\n",
      "Epoch 9/40  Iteration 2236/11040 Training loss: 1.4745 0.4649 sec/batch\n",
      "Epoch 9/40  Iteration 2237/11040 Training loss: 1.4745 0.4784 sec/batch\n",
      "Epoch 9/40  Iteration 2238/11040 Training loss: 1.4748 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2239/11040 Training loss: 1.4745 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2240/11040 Training loss: 1.4747 0.5000 sec/batch\n",
      "Epoch 9/40  Iteration 2241/11040 Training loss: 1.4748 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2242/11040 Training loss: 1.4743 0.4841 sec/batch\n",
      "Epoch 9/40  Iteration 2243/11040 Training loss: 1.4738 0.4797 sec/batch\n",
      "Epoch 9/40  Iteration 2244/11040 Training loss: 1.4742 0.4778 sec/batch\n",
      "Epoch 9/40  Iteration 2245/11040 Training loss: 1.4740 0.4637 sec/batch\n",
      "Epoch 9/40  Iteration 2246/11040 Training loss: 1.4733 0.4643 sec/batch\n",
      "Epoch 9/40  Iteration 2247/11040 Training loss: 1.4726 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2248/11040 Training loss: 1.4731 0.4791 sec/batch\n",
      "Epoch 9/40  Iteration 2249/11040 Training loss: 1.4727 0.4843 sec/batch\n",
      "Epoch 9/40  Iteration 2250/11040 Training loss: 1.4716 0.4749 sec/batch\n",
      "Epoch 9/40  Iteration 2251/11040 Training loss: 1.4721 0.4778 sec/batch\n",
      "Epoch 9/40  Iteration 2252/11040 Training loss: 1.4721 0.4846 sec/batch\n",
      "Epoch 9/40  Iteration 2253/11040 Training loss: 1.4720 0.4734 sec/batch\n",
      "Epoch 9/40  Iteration 2254/11040 Training loss: 1.4716 0.4860 sec/batch\n",
      "Epoch 9/40  Iteration 2255/11040 Training loss: 1.4714 0.4731 sec/batch\n",
      "Epoch 9/40  Iteration 2256/11040 Training loss: 1.4708 0.4850 sec/batch\n",
      "Epoch 9/40  Iteration 2257/11040 Training loss: 1.4698 0.4671 sec/batch\n",
      "Epoch 9/40  Iteration 2258/11040 Training loss: 1.4701 0.4809 sec/batch\n",
      "Epoch 9/40  Iteration 2259/11040 Training loss: 1.4699 0.4776 sec/batch\n",
      "Epoch 9/40  Iteration 2260/11040 Training loss: 1.4694 0.4636 sec/batch\n",
      "Epoch 9/40  Iteration 2261/11040 Training loss: 1.4697 0.4648 sec/batch\n",
      "Epoch 9/40  Iteration 2262/11040 Training loss: 1.4693 0.4783 sec/batch\n",
      "Epoch 9/40  Iteration 2263/11040 Training loss: 1.4694 0.4783 sec/batch\n",
      "Epoch 9/40  Iteration 2264/11040 Training loss: 1.4693 0.4849 sec/batch\n",
      "Epoch 9/40  Iteration 2265/11040 Training loss: 1.4688 0.4788 sec/batch\n",
      "Epoch 9/40  Iteration 2266/11040 Training loss: 1.4689 0.4751 sec/batch\n",
      "Epoch 9/40  Iteration 2267/11040 Training loss: 1.4692 0.4675 sec/batch\n",
      "Epoch 9/40  Iteration 2268/11040 Training loss: 1.4687 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2269/11040 Training loss: 1.4687 0.4639 sec/batch\n",
      "Epoch 9/40  Iteration 2270/11040 Training loss: 1.4683 0.4632 sec/batch\n",
      "Epoch 9/40  Iteration 2271/11040 Training loss: 1.4681 0.4804 sec/batch\n",
      "Epoch 9/40  Iteration 2272/11040 Training loss: 1.4678 0.4776 sec/batch\n",
      "Epoch 9/40  Iteration 2273/11040 Training loss: 1.4678 0.4696 sec/batch\n",
      "Epoch 9/40  Iteration 2274/11040 Training loss: 1.4672 0.4788 sec/batch\n",
      "Epoch 9/40  Iteration 2275/11040 Training loss: 1.4670 0.4645 sec/batch\n",
      "Epoch 9/40  Iteration 2276/11040 Training loss: 1.4671 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2277/11040 Training loss: 1.4668 0.4725 sec/batch\n",
      "Epoch 9/40  Iteration 2278/11040 Training loss: 1.4670 0.4704 sec/batch\n",
      "Epoch 9/40  Iteration 2279/11040 Training loss: 1.4664 0.4840 sec/batch\n",
      "Epoch 9/40  Iteration 2280/11040 Training loss: 1.4659 0.4778 sec/batch\n",
      "Epoch 9/40  Iteration 2281/11040 Training loss: 1.4651 0.4800 sec/batch\n",
      "Epoch 9/40  Iteration 2282/11040 Training loss: 1.4650 0.4848 sec/batch\n",
      "Epoch 9/40  Iteration 2283/11040 Training loss: 1.4647 0.4775 sec/batch\n",
      "Epoch 9/40  Iteration 2284/11040 Training loss: 1.4648 0.4898 sec/batch\n",
      "Epoch 9/40  Iteration 2285/11040 Training loss: 1.4645 0.4741 sec/batch\n",
      "Epoch 9/40  Iteration 2286/11040 Training loss: 1.4643 0.4891 sec/batch\n",
      "Epoch 9/40  Iteration 2287/11040 Training loss: 1.4639 0.4741 sec/batch\n",
      "Epoch 9/40  Iteration 2288/11040 Training loss: 1.4638 0.4736 sec/batch\n",
      "Epoch 9/40  Iteration 2289/11040 Training loss: 1.4633 0.4681 sec/batch\n",
      "Epoch 9/40  Iteration 2290/11040 Training loss: 1.4632 0.4689 sec/batch\n",
      "Epoch 9/40  Iteration 2291/11040 Training loss: 1.4629 0.4799 sec/batch\n",
      "Epoch 9/40  Iteration 2292/11040 Training loss: 1.4627 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2293/11040 Training loss: 1.4624 0.4999 sec/batch\n",
      "Epoch 9/40  Iteration 2294/11040 Training loss: 1.4623 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2295/11040 Training loss: 1.4618 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2296/11040 Training loss: 1.4610 0.4694 sec/batch\n",
      "Epoch 9/40  Iteration 2297/11040 Training loss: 1.4609 0.4731 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40  Iteration 2298/11040 Training loss: 1.4610 0.4681 sec/batch\n",
      "Epoch 9/40  Iteration 2299/11040 Training loss: 1.4615 0.4638 sec/batch\n",
      "Epoch 9/40  Iteration 2300/11040 Training loss: 1.4614 0.4794 sec/batch\n",
      "Epoch 9/40  Iteration 2301/11040 Training loss: 1.4612 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2302/11040 Training loss: 1.4615 0.4806 sec/batch\n",
      "Epoch 9/40  Iteration 2303/11040 Training loss: 1.4620 0.4694 sec/batch\n",
      "Epoch 9/40  Iteration 2304/11040 Training loss: 1.4623 0.4824 sec/batch\n",
      "Epoch 9/40  Iteration 2305/11040 Training loss: 1.4627 0.4793 sec/batch\n",
      "Epoch 9/40  Iteration 2306/11040 Training loss: 1.4630 0.4742 sec/batch\n",
      "Epoch 9/40  Iteration 2307/11040 Training loss: 1.4631 0.4782 sec/batch\n",
      "Epoch 9/40  Iteration 2308/11040 Training loss: 1.4628 0.4802 sec/batch\n",
      "Epoch 9/40  Iteration 2309/11040 Training loss: 1.4630 0.4834 sec/batch\n",
      "Epoch 9/40  Iteration 2310/11040 Training loss: 1.4628 0.4867 sec/batch\n",
      "Epoch 9/40  Iteration 2311/11040 Training loss: 1.4622 0.4703 sec/batch\n",
      "Epoch 9/40  Iteration 2312/11040 Training loss: 1.4624 0.4782 sec/batch\n",
      "Epoch 9/40  Iteration 2313/11040 Training loss: 1.4620 0.4862 sec/batch\n",
      "Epoch 9/40  Iteration 2314/11040 Training loss: 1.4619 0.4880 sec/batch\n",
      "Epoch 9/40  Iteration 2315/11040 Training loss: 1.4622 0.4837 sec/batch\n",
      "Epoch 9/40  Iteration 2316/11040 Training loss: 1.4624 0.4737 sec/batch\n",
      "Epoch 9/40  Iteration 2317/11040 Training loss: 1.4627 0.4724 sec/batch\n",
      "Epoch 9/40  Iteration 2318/11040 Training loss: 1.4628 0.4718 sec/batch\n",
      "Epoch 9/40  Iteration 2319/11040 Training loss: 1.4629 0.4834 sec/batch\n",
      "Epoch 9/40  Iteration 2320/11040 Training loss: 1.4631 0.4699 sec/batch\n",
      "Epoch 9/40  Iteration 2321/11040 Training loss: 1.4633 0.4721 sec/batch\n",
      "Epoch 9/40  Iteration 2322/11040 Training loss: 1.4636 0.4687 sec/batch\n",
      "Epoch 9/40  Iteration 2323/11040 Training loss: 1.4641 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2324/11040 Training loss: 1.4645 0.4796 sec/batch\n",
      "Epoch 9/40  Iteration 2325/11040 Training loss: 1.4645 0.4991 sec/batch\n",
      "Epoch 9/40  Iteration 2326/11040 Training loss: 1.4649 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2327/11040 Training loss: 1.4651 0.4843 sec/batch\n",
      "Epoch 9/40  Iteration 2328/11040 Training loss: 1.4654 0.4794 sec/batch\n",
      "Epoch 9/40  Iteration 2329/11040 Training loss: 1.4655 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2330/11040 Training loss: 1.4656 0.4690 sec/batch\n",
      "Epoch 9/40  Iteration 2331/11040 Training loss: 1.4660 0.4733 sec/batch\n",
      "Epoch 9/40  Iteration 2332/11040 Training loss: 1.4664 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2333/11040 Training loss: 1.4664 0.4853 sec/batch\n",
      "Epoch 9/40  Iteration 2334/11040 Training loss: 1.4667 0.4722 sec/batch\n",
      "Epoch 9/40  Iteration 2335/11040 Training loss: 1.4666 0.4650 sec/batch\n",
      "Epoch 9/40  Iteration 2336/11040 Training loss: 1.4666 0.4674 sec/batch\n",
      "Epoch 9/40  Iteration 2337/11040 Training loss: 1.4665 0.4804 sec/batch\n",
      "Epoch 9/40  Iteration 2338/11040 Training loss: 1.4666 0.4782 sec/batch\n",
      "Epoch 9/40  Iteration 2339/11040 Training loss: 1.4665 0.4798 sec/batch\n",
      "Epoch 9/40  Iteration 2340/11040 Training loss: 1.4661 0.4780 sec/batch\n",
      "Epoch 9/40  Iteration 2341/11040 Training loss: 1.4662 0.4788 sec/batch\n",
      "Epoch 9/40  Iteration 2342/11040 Training loss: 1.4662 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2343/11040 Training loss: 1.4665 0.4640 sec/batch\n",
      "Epoch 9/40  Iteration 2344/11040 Training loss: 1.4667 0.4686 sec/batch\n",
      "Epoch 9/40  Iteration 2345/11040 Training loss: 1.4666 0.4634 sec/batch\n",
      "Epoch 9/40  Iteration 2346/11040 Training loss: 1.4669 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2347/11040 Training loss: 1.4670 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2348/11040 Training loss: 1.4671 0.4784 sec/batch\n",
      "Epoch 9/40  Iteration 2349/11040 Training loss: 1.4671 0.4860 sec/batch\n",
      "Epoch 9/40  Iteration 2350/11040 Training loss: 1.4672 0.4879 sec/batch\n",
      "Epoch 9/40  Iteration 2351/11040 Training loss: 1.4671 0.4850 sec/batch\n",
      "Epoch 9/40  Iteration 2352/11040 Training loss: 1.4668 0.4800 sec/batch\n",
      "Epoch 9/40  Iteration 2353/11040 Training loss: 1.4668 0.4781 sec/batch\n",
      "Epoch 9/40  Iteration 2354/11040 Training loss: 1.4667 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2355/11040 Training loss: 1.4667 0.4678 sec/batch\n",
      "Epoch 9/40  Iteration 2356/11040 Training loss: 1.4665 0.4791 sec/batch\n",
      "Epoch 9/40  Iteration 2357/11040 Training loss: 1.4662 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2358/11040 Training loss: 1.4662 0.4794 sec/batch\n",
      "Epoch 9/40  Iteration 2359/11040 Training loss: 1.4662 0.4684 sec/batch\n",
      "Epoch 9/40  Iteration 2360/11040 Training loss: 1.4663 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2361/11040 Training loss: 1.4662 0.4782 sec/batch\n",
      "Epoch 9/40  Iteration 2362/11040 Training loss: 1.4662 0.4693 sec/batch\n",
      "Epoch 9/40  Iteration 2363/11040 Training loss: 1.4661 0.4632 sec/batch\n",
      "Epoch 9/40  Iteration 2364/11040 Training loss: 1.4661 0.4799 sec/batch\n",
      "Epoch 9/40  Iteration 2365/11040 Training loss: 1.4659 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2366/11040 Training loss: 1.4659 0.4782 sec/batch\n",
      "Epoch 9/40  Iteration 2367/11040 Training loss: 1.4660 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2368/11040 Training loss: 1.4661 0.4637 sec/batch\n",
      "Epoch 9/40  Iteration 2369/11040 Training loss: 1.4662 0.4788 sec/batch\n",
      "Epoch 9/40  Iteration 2370/11040 Training loss: 1.4662 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2371/11040 Training loss: 1.4662 0.4794 sec/batch\n",
      "Epoch 9/40  Iteration 2372/11040 Training loss: 1.4663 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2373/11040 Training loss: 1.4664 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2374/11040 Training loss: 1.4666 0.4739 sec/batch\n",
      "Epoch 9/40  Iteration 2375/11040 Training loss: 1.4667 0.4846 sec/batch\n",
      "Epoch 9/40  Iteration 2376/11040 Training loss: 1.4665 0.4797 sec/batch\n",
      "Epoch 9/40  Iteration 2377/11040 Training loss: 1.4663 0.4783 sec/batch\n",
      "Epoch 9/40  Iteration 2378/11040 Training loss: 1.4663 0.4638 sec/batch\n",
      "Epoch 9/40  Iteration 2379/11040 Training loss: 1.4665 0.4791 sec/batch\n",
      "Epoch 9/40  Iteration 2380/11040 Training loss: 1.4665 0.4797 sec/batch\n",
      "Epoch 9/40  Iteration 2381/11040 Training loss: 1.4665 0.4778 sec/batch\n",
      "Epoch 9/40  Iteration 2382/11040 Training loss: 1.4664 0.4798 sec/batch\n",
      "Epoch 9/40  Iteration 2383/11040 Training loss: 1.4664 0.4793 sec/batch\n",
      "Epoch 9/40  Iteration 2384/11040 Training loss: 1.4663 0.4629 sec/batch\n",
      "Epoch 9/40  Iteration 2385/11040 Training loss: 1.4662 0.4744 sec/batch\n",
      "Epoch 9/40  Iteration 2386/11040 Training loss: 1.4662 0.4680 sec/batch\n",
      "Epoch 9/40  Iteration 2387/11040 Training loss: 1.4663 0.4642 sec/batch\n",
      "Epoch 9/40  Iteration 2388/11040 Training loss: 1.4663 0.4735 sec/batch\n",
      "Epoch 9/40  Iteration 2389/11040 Training loss: 1.4663 0.4839 sec/batch\n",
      "Epoch 9/40  Iteration 2390/11040 Training loss: 1.4663 0.4749 sec/batch\n",
      "Epoch 9/40  Iteration 2391/11040 Training loss: 1.4661 0.4786 sec/batch\n",
      "Epoch 9/40  Iteration 2392/11040 Training loss: 1.4662 0.4844 sec/batch\n",
      "Epoch 9/40  Iteration 2393/11040 Training loss: 1.4661 0.4738 sec/batch\n",
      "Epoch 9/40  Iteration 2394/11040 Training loss: 1.4662 0.4845 sec/batch\n",
      "Epoch 9/40  Iteration 2395/11040 Training loss: 1.4664 0.4735 sec/batch\n",
      "Epoch 9/40  Iteration 2396/11040 Training loss: 1.4665 0.4621 sec/batch\n",
      "Epoch 9/40  Iteration 2397/11040 Training loss: 1.4664 0.4650 sec/batch\n",
      "Epoch 9/40  Iteration 2398/11040 Training loss: 1.4663 0.4789 sec/batch\n",
      "Epoch 9/40  Iteration 2399/11040 Training loss: 1.4661 0.4731 sec/batch\n",
      "Epoch 9/40  Iteration 2400/11040 Training loss: 1.4660 0.4699 sec/batch\n",
      "Epoch 9/40  Iteration 2401/11040 Training loss: 1.4659 0.4789 sec/batch\n",
      "Epoch 9/40  Iteration 2402/11040 Training loss: 1.4659 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2403/11040 Training loss: 1.4661 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2404/11040 Training loss: 1.4660 0.4794 sec/batch\n",
      "Epoch 9/40  Iteration 2405/11040 Training loss: 1.4661 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2406/11040 Training loss: 1.4661 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2407/11040 Training loss: 1.4662 0.4743 sec/batch\n",
      "Epoch 9/40  Iteration 2408/11040 Training loss: 1.4662 0.4837 sec/batch\n",
      "Epoch 9/40  Iteration 2409/11040 Training loss: 1.4663 0.4704 sec/batch\n",
      "Epoch 9/40  Iteration 2410/11040 Training loss: 1.4664 0.4620 sec/batch\n",
      "Epoch 9/40  Iteration 2411/11040 Training loss: 1.4665 0.4789 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40  Iteration 2412/11040 Training loss: 1.4666 0.4807 sec/batch\n",
      "Epoch 9/40  Iteration 2413/11040 Training loss: 1.4665 0.4791 sec/batch\n",
      "Epoch 9/40  Iteration 2414/11040 Training loss: 1.4664 0.4776 sec/batch\n",
      "Epoch 9/40  Iteration 2415/11040 Training loss: 1.4664 0.4739 sec/batch\n",
      "Epoch 9/40  Iteration 2416/11040 Training loss: 1.4663 0.4846 sec/batch\n",
      "Epoch 9/40  Iteration 2417/11040 Training loss: 1.4663 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2418/11040 Training loss: 1.4662 0.4798 sec/batch\n",
      "Epoch 9/40  Iteration 2419/11040 Training loss: 1.4662 0.4729 sec/batch\n",
      "Epoch 9/40  Iteration 2420/11040 Training loss: 1.4662 0.4853 sec/batch\n",
      "Epoch 9/40  Iteration 2421/11040 Training loss: 1.4662 0.4783 sec/batch\n",
      "Epoch 9/40  Iteration 2422/11040 Training loss: 1.4661 0.4804 sec/batch\n",
      "Epoch 9/40  Iteration 2423/11040 Training loss: 1.4659 0.4779 sec/batch\n",
      "Epoch 9/40  Iteration 2424/11040 Training loss: 1.4659 0.4789 sec/batch\n",
      "Epoch 9/40  Iteration 2425/11040 Training loss: 1.4657 0.4795 sec/batch\n",
      "Epoch 9/40  Iteration 2426/11040 Training loss: 1.4655 0.4792 sec/batch\n",
      "Epoch 9/40  Iteration 2427/11040 Training loss: 1.4654 0.4733 sec/batch\n",
      "Epoch 9/40  Iteration 2428/11040 Training loss: 1.4654 0.4846 sec/batch\n",
      "Epoch 9/40  Iteration 2429/11040 Training loss: 1.4653 0.4685 sec/batch\n",
      "Epoch 9/40  Iteration 2430/11040 Training loss: 1.4651 0.4634 sec/batch\n",
      "Epoch 9/40  Iteration 2431/11040 Training loss: 1.4650 0.4643 sec/batch\n",
      "Epoch 9/40  Iteration 2432/11040 Training loss: 1.4651 0.4786 sec/batch\n",
      "Epoch 9/40  Iteration 2433/11040 Training loss: 1.4651 0.4801 sec/batch\n",
      "Epoch 9/40  Iteration 2434/11040 Training loss: 1.4651 0.4821 sec/batch\n",
      "Epoch 9/40  Iteration 2435/11040 Training loss: 1.4649 0.4729 sec/batch\n",
      "Epoch 9/40  Iteration 2436/11040 Training loss: 1.4648 0.4643 sec/batch\n",
      "Epoch 9/40  Iteration 2437/11040 Training loss: 1.4648 0.4743 sec/batch\n",
      "Epoch 9/40  Iteration 2438/11040 Training loss: 1.4649 0.4780 sec/batch\n",
      "Epoch 9/40  Iteration 2439/11040 Training loss: 1.4648 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2440/11040 Training loss: 1.4648 0.4788 sec/batch\n",
      "Epoch 9/40  Iteration 2441/11040 Training loss: 1.4646 0.4793 sec/batch\n",
      "Epoch 9/40  Iteration 2442/11040 Training loss: 1.4644 0.4798 sec/batch\n",
      "Epoch 9/40  Iteration 2443/11040 Training loss: 1.4645 0.4789 sec/batch\n",
      "Epoch 9/40  Iteration 2444/11040 Training loss: 1.4644 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2445/11040 Training loss: 1.4642 0.4878 sec/batch\n",
      "Epoch 9/40  Iteration 2446/11040 Training loss: 1.4642 0.4655 sec/batch\n",
      "Epoch 9/40  Iteration 2447/11040 Training loss: 1.4642 0.4807 sec/batch\n",
      "Epoch 9/40  Iteration 2448/11040 Training loss: 1.4641 0.4790 sec/batch\n",
      "Epoch 9/40  Iteration 2449/11040 Training loss: 1.4641 0.4777 sec/batch\n",
      "Epoch 9/40  Iteration 2450/11040 Training loss: 1.4641 0.4787 sec/batch\n",
      "Epoch 9/40  Iteration 2451/11040 Training loss: 1.4639 0.4800 sec/batch\n",
      "Epoch 9/40  Iteration 2452/11040 Training loss: 1.4637 0.4800 sec/batch\n",
      "Epoch 9/40  Iteration 2453/11040 Training loss: 1.4636 0.4831 sec/batch\n",
      "Epoch 9/40  Iteration 2454/11040 Training loss: 1.4635 0.4742 sec/batch\n",
      "Epoch 9/40  Iteration 2455/11040 Training loss: 1.4635 0.4685 sec/batch\n",
      "Epoch 9/40  Iteration 2456/11040 Training loss: 1.4634 0.4741 sec/batch\n",
      "Epoch 9/40  Iteration 2457/11040 Training loss: 1.4633 0.4683 sec/batch\n",
      "Epoch 9/40  Iteration 2458/11040 Training loss: 1.4632 0.4638 sec/batch\n",
      "Epoch 9/40  Iteration 2459/11040 Training loss: 1.4631 0.4782 sec/batch\n",
      "Epoch 9/40  Iteration 2460/11040 Training loss: 1.4630 0.4794 sec/batch\n",
      "Epoch 9/40  Iteration 2461/11040 Training loss: 1.4629 0.4806 sec/batch\n",
      "Epoch 9/40  Iteration 2462/11040 Training loss: 1.4630 0.4680 sec/batch\n",
      "Epoch 9/40  Iteration 2463/11040 Training loss: 1.4629 0.4780 sec/batch\n",
      "Epoch 9/40  Iteration 2464/11040 Training loss: 1.4629 0.4845 sec/batch\n",
      "Epoch 9/40  Iteration 2465/11040 Training loss: 1.4629 0.4897 sec/batch\n",
      "Epoch 9/40  Iteration 2466/11040 Training loss: 1.4629 0.4843 sec/batch\n",
      "Epoch 9/40  Iteration 2467/11040 Training loss: 1.4628 0.4806 sec/batch\n",
      "Epoch 9/40  Iteration 2468/11040 Training loss: 1.4627 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2469/11040 Training loss: 1.4627 0.4834 sec/batch\n",
      "Epoch 9/40  Iteration 2470/11040 Training loss: 1.4628 0.4799 sec/batch\n",
      "Epoch 9/40  Iteration 2471/11040 Training loss: 1.4628 0.4789 sec/batch\n",
      "Epoch 9/40  Iteration 2472/11040 Training loss: 1.4628 0.4837 sec/batch\n",
      "Epoch 9/40  Iteration 2473/11040 Training loss: 1.4627 0.4796 sec/batch\n",
      "Epoch 9/40  Iteration 2474/11040 Training loss: 1.4627 0.4843 sec/batch\n",
      "Epoch 9/40  Iteration 2475/11040 Training loss: 1.4628 0.4844 sec/batch\n",
      "Epoch 9/40  Iteration 2476/11040 Training loss: 1.4628 0.4844 sec/batch\n",
      "Epoch 9/40  Iteration 2477/11040 Training loss: 1.4628 0.4835 sec/batch\n",
      "Epoch 9/40  Iteration 2478/11040 Training loss: 1.4629 0.4803 sec/batch\n",
      "Epoch 9/40  Iteration 2479/11040 Training loss: 1.4631 0.4772 sec/batch\n",
      "Epoch 9/40  Iteration 2480/11040 Training loss: 1.4633 0.4801 sec/batch\n",
      "Epoch 9/40  Iteration 2481/11040 Training loss: 1.4633 0.4687 sec/batch\n",
      "Epoch 9/40  Iteration 2482/11040 Training loss: 1.4633 0.4785 sec/batch\n",
      "Epoch 9/40  Iteration 2483/11040 Training loss: 1.4632 0.4806 sec/batch\n",
      "Epoch 9/40  Iteration 2484/11040 Training loss: 1.4632 0.4785 sec/batch\n",
      "Epoch 10/40  Iteration 2485/11040 Training loss: 1.5166 0.4682 sec/batch\n",
      "Epoch 10/40  Iteration 2486/11040 Training loss: 1.4708 0.4787 sec/batch\n",
      "Epoch 10/40  Iteration 2487/11040 Training loss: 1.4652 0.4644 sec/batch\n",
      "Epoch 10/40  Iteration 2488/11040 Training loss: 1.4661 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2489/11040 Training loss: 1.4697 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2490/11040 Training loss: 1.4661 0.4862 sec/batch\n",
      "Epoch 10/40  Iteration 2491/11040 Training loss: 1.4583 0.4878 sec/batch\n",
      "Epoch 10/40  Iteration 2492/11040 Training loss: 1.4569 0.4690 sec/batch\n",
      "Epoch 10/40  Iteration 2493/11040 Training loss: 1.4551 0.4631 sec/batch\n",
      "Epoch 10/40  Iteration 2494/11040 Training loss: 1.4552 0.4793 sec/batch\n",
      "Epoch 10/40  Iteration 2495/11040 Training loss: 1.4536 0.4801 sec/batch\n",
      "Epoch 10/40  Iteration 2496/11040 Training loss: 1.4537 0.4833 sec/batch\n",
      "Epoch 10/40  Iteration 2497/11040 Training loss: 1.4521 0.4744 sec/batch\n",
      "Epoch 10/40  Iteration 2498/11040 Training loss: 1.4500 0.4629 sec/batch\n",
      "Epoch 10/40  Iteration 2499/11040 Training loss: 1.4497 0.4650 sec/batch\n",
      "Epoch 10/40  Iteration 2500/11040 Training loss: 1.4500 0.4676 sec/batch\n",
      "Validation loss: 1.37812 Saving checkpoint!\n",
      "Epoch 10/40  Iteration 2501/11040 Training loss: 1.4535 0.4844 sec/batch\n",
      "Epoch 10/40  Iteration 2502/11040 Training loss: 1.4519 0.4770 sec/batch\n",
      "Epoch 10/40  Iteration 2503/11040 Training loss: 1.4509 0.4767 sec/batch\n",
      "Epoch 10/40  Iteration 2504/11040 Training loss: 1.4507 0.4775 sec/batch\n",
      "Epoch 10/40  Iteration 2505/11040 Training loss: 1.4519 0.4792 sec/batch\n",
      "Epoch 10/40  Iteration 2506/11040 Training loss: 1.4524 0.4791 sec/batch\n",
      "Epoch 10/40  Iteration 2507/11040 Training loss: 1.4522 0.4691 sec/batch\n",
      "Epoch 10/40  Iteration 2508/11040 Training loss: 1.4535 0.4749 sec/batch\n",
      "Epoch 10/40  Iteration 2509/11040 Training loss: 1.4520 0.4837 sec/batch\n",
      "Epoch 10/40  Iteration 2510/11040 Training loss: 1.4509 0.4728 sec/batch\n",
      "Epoch 10/40  Iteration 2511/11040 Training loss: 1.4502 0.4904 sec/batch\n",
      "Epoch 10/40  Iteration 2512/11040 Training loss: 1.4498 0.4670 sec/batch\n",
      "Epoch 10/40  Iteration 2513/11040 Training loss: 1.4496 0.4807 sec/batch\n",
      "Epoch 10/40  Iteration 2514/11040 Training loss: 1.4495 0.4776 sec/batch\n",
      "Epoch 10/40  Iteration 2515/11040 Training loss: 1.4490 0.4737 sec/batch\n",
      "Epoch 10/40  Iteration 2516/11040 Training loss: 1.4490 0.4638 sec/batch\n",
      "Epoch 10/40  Iteration 2517/11040 Training loss: 1.4488 0.4690 sec/batch\n",
      "Epoch 10/40  Iteration 2518/11040 Training loss: 1.4481 0.4792 sec/batch\n",
      "Epoch 10/40  Iteration 2519/11040 Training loss: 1.4477 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2520/11040 Training loss: 1.4480 0.4686 sec/batch\n",
      "Epoch 10/40  Iteration 2521/11040 Training loss: 1.4479 0.4635 sec/batch\n",
      "Epoch 10/40  Iteration 2522/11040 Training loss: 1.4473 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2523/11040 Training loss: 1.4466 0.4633 sec/batch\n",
      "Epoch 10/40  Iteration 2524/11040 Training loss: 1.4470 0.4635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40  Iteration 2525/11040 Training loss: 1.4463 0.4639 sec/batch\n",
      "Epoch 10/40  Iteration 2526/11040 Training loss: 1.4454 0.4790 sec/batch\n",
      "Epoch 10/40  Iteration 2527/11040 Training loss: 1.4457 0.4787 sec/batch\n",
      "Epoch 10/40  Iteration 2528/11040 Training loss: 1.4455 0.4791 sec/batch\n",
      "Epoch 10/40  Iteration 2529/11040 Training loss: 1.4453 0.4790 sec/batch\n",
      "Epoch 10/40  Iteration 2530/11040 Training loss: 1.4447 0.4798 sec/batch\n",
      "Epoch 10/40  Iteration 2531/11040 Training loss: 1.4440 0.4735 sec/batch\n",
      "Epoch 10/40  Iteration 2532/11040 Training loss: 1.4437 0.4844 sec/batch\n",
      "Epoch 10/40  Iteration 2533/11040 Training loss: 1.4428 0.4789 sec/batch\n",
      "Epoch 10/40  Iteration 2534/11040 Training loss: 1.4431 0.4805 sec/batch\n",
      "Epoch 10/40  Iteration 2535/11040 Training loss: 1.4429 0.4731 sec/batch\n",
      "Epoch 10/40  Iteration 2536/11040 Training loss: 1.4424 0.4840 sec/batch\n",
      "Epoch 10/40  Iteration 2537/11040 Training loss: 1.4428 0.4789 sec/batch\n",
      "Epoch 10/40  Iteration 2538/11040 Training loss: 1.4425 0.4751 sec/batch\n",
      "Epoch 10/40  Iteration 2539/11040 Training loss: 1.4425 0.4835 sec/batch\n",
      "Epoch 10/40  Iteration 2540/11040 Training loss: 1.4424 0.4790 sec/batch\n",
      "Epoch 10/40  Iteration 2541/11040 Training loss: 1.4421 0.4745 sec/batch\n",
      "Epoch 10/40  Iteration 2542/11040 Training loss: 1.4421 0.4794 sec/batch\n",
      "Epoch 10/40  Iteration 2543/11040 Training loss: 1.4425 0.4775 sec/batch\n",
      "Epoch 10/40  Iteration 2544/11040 Training loss: 1.4421 0.4810 sec/batch\n",
      "Epoch 10/40  Iteration 2545/11040 Training loss: 1.4420 0.4780 sec/batch\n",
      "Epoch 10/40  Iteration 2546/11040 Training loss: 1.4414 0.4687 sec/batch\n",
      "Epoch 10/40  Iteration 2547/11040 Training loss: 1.4412 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2548/11040 Training loss: 1.4409 0.4794 sec/batch\n",
      "Epoch 10/40  Iteration 2549/11040 Training loss: 1.4408 0.4793 sec/batch\n",
      "Epoch 10/40  Iteration 2550/11040 Training loss: 1.4404 0.4643 sec/batch\n",
      "Epoch 10/40  Iteration 2551/11040 Training loss: 1.4401 0.4779 sec/batch\n",
      "Epoch 10/40  Iteration 2552/11040 Training loss: 1.4401 0.4632 sec/batch\n",
      "Epoch 10/40  Iteration 2553/11040 Training loss: 1.4398 0.4633 sec/batch\n",
      "Epoch 10/40  Iteration 2554/11040 Training loss: 1.4399 0.4802 sec/batch\n",
      "Epoch 10/40  Iteration 2555/11040 Training loss: 1.4395 0.4784 sec/batch\n",
      "Epoch 10/40  Iteration 2556/11040 Training loss: 1.4390 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2557/11040 Training loss: 1.4382 0.4793 sec/batch\n",
      "Epoch 10/40  Iteration 2558/11040 Training loss: 1.4379 0.4857 sec/batch\n",
      "Epoch 10/40  Iteration 2559/11040 Training loss: 1.4377 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2560/11040 Training loss: 1.4377 0.4779 sec/batch\n",
      "Epoch 10/40  Iteration 2561/11040 Training loss: 1.4373 0.4838 sec/batch\n",
      "Epoch 10/40  Iteration 2562/11040 Training loss: 1.4373 0.4748 sec/batch\n",
      "Epoch 10/40  Iteration 2563/11040 Training loss: 1.4368 0.4642 sec/batch\n",
      "Epoch 10/40  Iteration 2564/11040 Training loss: 1.4366 0.4671 sec/batch\n",
      "Epoch 10/40  Iteration 2565/11040 Training loss: 1.4362 0.4634 sec/batch\n",
      "Epoch 10/40  Iteration 2566/11040 Training loss: 1.4361 0.4799 sec/batch\n",
      "Epoch 10/40  Iteration 2567/11040 Training loss: 1.4359 0.4788 sec/batch\n",
      "Epoch 10/40  Iteration 2568/11040 Training loss: 1.4356 0.4789 sec/batch\n",
      "Epoch 10/40  Iteration 2569/11040 Training loss: 1.4353 0.4839 sec/batch\n",
      "Epoch 10/40  Iteration 2570/11040 Training loss: 1.4351 0.4748 sec/batch\n",
      "Epoch 10/40  Iteration 2571/11040 Training loss: 1.4346 0.4799 sec/batch\n",
      "Epoch 10/40  Iteration 2572/11040 Training loss: 1.4336 0.4777 sec/batch\n",
      "Epoch 10/40  Iteration 2573/11040 Training loss: 1.4336 0.4790 sec/batch\n",
      "Epoch 10/40  Iteration 2574/11040 Training loss: 1.4336 0.4789 sec/batch\n",
      "Epoch 10/40  Iteration 2575/11040 Training loss: 1.4341 0.4850 sec/batch\n",
      "Epoch 10/40  Iteration 2576/11040 Training loss: 1.4342 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2577/11040 Training loss: 1.4340 0.4683 sec/batch\n",
      "Epoch 10/40  Iteration 2578/11040 Training loss: 1.4343 0.4742 sec/batch\n",
      "Epoch 10/40  Iteration 2579/11040 Training loss: 1.4349 0.4837 sec/batch\n",
      "Epoch 10/40  Iteration 2580/11040 Training loss: 1.4353 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2581/11040 Training loss: 1.4358 0.4787 sec/batch\n",
      "Epoch 10/40  Iteration 2582/11040 Training loss: 1.4361 0.4635 sec/batch\n",
      "Epoch 10/40  Iteration 2583/11040 Training loss: 1.4362 0.4696 sec/batch\n",
      "Epoch 10/40  Iteration 2584/11040 Training loss: 1.4359 0.4739 sec/batch\n",
      "Epoch 10/40  Iteration 2585/11040 Training loss: 1.4360 0.4841 sec/batch\n",
      "Epoch 10/40  Iteration 2586/11040 Training loss: 1.4361 0.4787 sec/batch\n",
      "Epoch 10/40  Iteration 2587/11040 Training loss: 1.4356 0.4791 sec/batch\n",
      "Epoch 10/40  Iteration 2588/11040 Training loss: 1.4359 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2589/11040 Training loss: 1.4356 0.4696 sec/batch\n",
      "Epoch 10/40  Iteration 2590/11040 Training loss: 1.4356 0.4785 sec/batch\n",
      "Epoch 10/40  Iteration 2591/11040 Training loss: 1.4358 0.4688 sec/batch\n",
      "Epoch 10/40  Iteration 2592/11040 Training loss: 1.4362 0.4847 sec/batch\n",
      "Epoch 10/40  Iteration 2593/11040 Training loss: 1.4364 0.4787 sec/batch\n",
      "Epoch 10/40  Iteration 2594/11040 Training loss: 1.4364 0.4793 sec/batch\n",
      "Epoch 10/40  Iteration 2595/11040 Training loss: 1.4366 0.4786 sec/batch\n",
      "Epoch 10/40  Iteration 2596/11040 Training loss: 1.4368 0.4843 sec/batch\n",
      "Epoch 10/40  Iteration 2597/11040 Training loss: 1.4371 0.4686 sec/batch\n",
      "Epoch 10/40  Iteration 2598/11040 Training loss: 1.4374 0.4773 sec/batch\n",
      "Epoch 10/40  Iteration 2599/11040 Training loss: 1.4378 0.4723 sec/batch\n",
      "Epoch 10/40  Iteration 2600/11040 Training loss: 1.4382 0.4598 sec/batch\n",
      "Epoch 10/40  Iteration 2601/11040 Training loss: 1.4382 0.4751 sec/batch\n",
      "Epoch 10/40  Iteration 2602/11040 Training loss: 1.4386 0.4744 sec/batch\n",
      "Epoch 10/40  Iteration 2603/11040 Training loss: 1.4387 0.4742 sec/batch\n",
      "Epoch 10/40  Iteration 2604/11040 Training loss: 1.4388 0.4762 sec/batch\n",
      "Epoch 10/40  Iteration 2605/11040 Training loss: 1.4389 0.4739 sec/batch\n",
      "Epoch 10/40  Iteration 2606/11040 Training loss: 1.4389 0.4605 sec/batch\n",
      "Epoch 10/40  Iteration 2607/11040 Training loss: 1.4393 0.4602 sec/batch\n",
      "Epoch 10/40  Iteration 2608/11040 Training loss: 1.4396 0.4725 sec/batch\n",
      "Epoch 10/40  Iteration 2609/11040 Training loss: 1.4397 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2610/11040 Training loss: 1.4400 0.4603 sec/batch\n",
      "Epoch 10/40  Iteration 2611/11040 Training loss: 1.4400 0.4594 sec/batch\n",
      "Epoch 10/40  Iteration 2612/11040 Training loss: 1.4399 0.4586 sec/batch\n",
      "Epoch 10/40  Iteration 2613/11040 Training loss: 1.4398 0.4760 sec/batch\n",
      "Epoch 10/40  Iteration 2614/11040 Training loss: 1.4400 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2615/11040 Training loss: 1.4398 0.4752 sec/batch\n",
      "Epoch 10/40  Iteration 2616/11040 Training loss: 1.4395 0.4750 sec/batch\n",
      "Epoch 10/40  Iteration 2617/11040 Training loss: 1.4396 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2618/11040 Training loss: 1.4395 0.4753 sec/batch\n",
      "Epoch 10/40  Iteration 2619/11040 Training loss: 1.4398 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2620/11040 Training loss: 1.4400 0.4757 sec/batch\n",
      "Epoch 10/40  Iteration 2621/11040 Training loss: 1.4400 0.4731 sec/batch\n",
      "Epoch 10/40  Iteration 2622/11040 Training loss: 1.4402 0.4763 sec/batch\n",
      "Epoch 10/40  Iteration 2623/11040 Training loss: 1.4403 0.4745 sec/batch\n",
      "Epoch 10/40  Iteration 2624/11040 Training loss: 1.4403 0.4747 sec/batch\n",
      "Epoch 10/40  Iteration 2625/11040 Training loss: 1.4403 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2626/11040 Training loss: 1.4404 0.4761 sec/batch\n",
      "Epoch 10/40  Iteration 2627/11040 Training loss: 1.4404 0.4609 sec/batch\n",
      "Epoch 10/40  Iteration 2628/11040 Training loss: 1.4402 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2629/11040 Training loss: 1.4401 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2630/11040 Training loss: 1.4401 0.4755 sec/batch\n",
      "Epoch 10/40  Iteration 2631/11040 Training loss: 1.4400 0.4755 sec/batch\n",
      "Epoch 10/40  Iteration 2632/11040 Training loss: 1.4398 0.4734 sec/batch\n",
      "Epoch 10/40  Iteration 2633/11040 Training loss: 1.4395 0.4762 sec/batch\n",
      "Epoch 10/40  Iteration 2634/11040 Training loss: 1.4395 0.4776 sec/batch\n",
      "Epoch 10/40  Iteration 2635/11040 Training loss: 1.4394 0.4728 sec/batch\n",
      "Epoch 10/40  Iteration 2636/11040 Training loss: 1.4395 0.4759 sec/batch\n",
      "Epoch 10/40  Iteration 2637/11040 Training loss: 1.4394 0.4701 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40  Iteration 2638/11040 Training loss: 1.4394 0.4769 sec/batch\n",
      "Epoch 10/40  Iteration 2639/11040 Training loss: 1.4393 0.4728 sec/batch\n",
      "Epoch 10/40  Iteration 2640/11040 Training loss: 1.4393 0.4734 sec/batch\n",
      "Epoch 10/40  Iteration 2641/11040 Training loss: 1.4391 0.4763 sec/batch\n",
      "Epoch 10/40  Iteration 2642/11040 Training loss: 1.4391 0.4742 sec/batch\n",
      "Epoch 10/40  Iteration 2643/11040 Training loss: 1.4392 0.4570 sec/batch\n",
      "Epoch 10/40  Iteration 2644/11040 Training loss: 1.4393 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2645/11040 Training loss: 1.4394 0.4754 sec/batch\n",
      "Epoch 10/40  Iteration 2646/11040 Training loss: 1.4394 0.4746 sec/batch\n",
      "Epoch 10/40  Iteration 2647/11040 Training loss: 1.4394 0.4754 sec/batch\n",
      "Epoch 10/40  Iteration 2648/11040 Training loss: 1.4395 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2649/11040 Training loss: 1.4395 0.4738 sec/batch\n",
      "Epoch 10/40  Iteration 2650/11040 Training loss: 1.4397 0.4615 sec/batch\n",
      "Epoch 10/40  Iteration 2651/11040 Training loss: 1.4398 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2652/11040 Training loss: 1.4397 0.4761 sec/batch\n",
      "Epoch 10/40  Iteration 2653/11040 Training loss: 1.4396 0.4772 sec/batch\n",
      "Epoch 10/40  Iteration 2654/11040 Training loss: 1.4396 0.4731 sec/batch\n",
      "Epoch 10/40  Iteration 2655/11040 Training loss: 1.4398 0.4745 sec/batch\n",
      "Epoch 10/40  Iteration 2656/11040 Training loss: 1.4398 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2657/11040 Training loss: 1.4398 0.4606 sec/batch\n",
      "Epoch 10/40  Iteration 2658/11040 Training loss: 1.4397 0.4750 sec/batch\n",
      "Epoch 10/40  Iteration 2659/11040 Training loss: 1.4397 0.4763 sec/batch\n",
      "Epoch 10/40  Iteration 2660/11040 Training loss: 1.4396 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2661/11040 Training loss: 1.4396 0.4775 sec/batch\n",
      "Epoch 10/40  Iteration 2662/11040 Training loss: 1.4396 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2663/11040 Training loss: 1.4397 0.4746 sec/batch\n",
      "Epoch 10/40  Iteration 2664/11040 Training loss: 1.4398 0.4744 sec/batch\n",
      "Epoch 10/40  Iteration 2665/11040 Training loss: 1.4398 0.4764 sec/batch\n",
      "Epoch 10/40  Iteration 2666/11040 Training loss: 1.4398 0.4745 sec/batch\n",
      "Epoch 10/40  Iteration 2667/11040 Training loss: 1.4396 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2668/11040 Training loss: 1.4396 0.4756 sec/batch\n",
      "Epoch 10/40  Iteration 2669/11040 Training loss: 1.4396 0.4753 sec/batch\n",
      "Epoch 10/40  Iteration 2670/11040 Training loss: 1.4397 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2671/11040 Training loss: 1.4399 0.4772 sec/batch\n",
      "Epoch 10/40  Iteration 2672/11040 Training loss: 1.4400 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2673/11040 Training loss: 1.4399 0.4764 sec/batch\n",
      "Epoch 10/40  Iteration 2674/11040 Training loss: 1.4397 0.4761 sec/batch\n",
      "Epoch 10/40  Iteration 2675/11040 Training loss: 1.4396 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2676/11040 Training loss: 1.4395 0.4751 sec/batch\n",
      "Epoch 10/40  Iteration 2677/11040 Training loss: 1.4395 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2678/11040 Training loss: 1.4395 0.4775 sec/batch\n",
      "Epoch 10/40  Iteration 2679/11040 Training loss: 1.4397 0.4736 sec/batch\n",
      "Epoch 10/40  Iteration 2680/11040 Training loss: 1.4396 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2681/11040 Training loss: 1.4397 0.4761 sec/batch\n",
      "Epoch 10/40  Iteration 2682/11040 Training loss: 1.4397 0.4745 sec/batch\n",
      "Epoch 10/40  Iteration 2683/11040 Training loss: 1.4398 0.4755 sec/batch\n",
      "Epoch 10/40  Iteration 2684/11040 Training loss: 1.4399 0.4753 sec/batch\n",
      "Epoch 10/40  Iteration 2685/11040 Training loss: 1.4400 0.4762 sec/batch\n",
      "Epoch 10/40  Iteration 2686/11040 Training loss: 1.4400 0.4744 sec/batch\n",
      "Epoch 10/40  Iteration 2687/11040 Training loss: 1.4402 0.4752 sec/batch\n",
      "Epoch 10/40  Iteration 2688/11040 Training loss: 1.4403 0.4764 sec/batch\n",
      "Epoch 10/40  Iteration 2689/11040 Training loss: 1.4402 0.4731 sec/batch\n",
      "Epoch 10/40  Iteration 2690/11040 Training loss: 1.4402 0.4762 sec/batch\n",
      "Epoch 10/40  Iteration 2691/11040 Training loss: 1.4401 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2692/11040 Training loss: 1.4402 0.4610 sec/batch\n",
      "Epoch 10/40  Iteration 2693/11040 Training loss: 1.4401 0.4593 sec/batch\n",
      "Epoch 10/40  Iteration 2694/11040 Training loss: 1.4401 0.4747 sec/batch\n",
      "Epoch 10/40  Iteration 2695/11040 Training loss: 1.4400 0.4756 sec/batch\n",
      "Epoch 10/40  Iteration 2696/11040 Training loss: 1.4400 0.4746 sec/batch\n",
      "Epoch 10/40  Iteration 2697/11040 Training loss: 1.4400 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2698/11040 Training loss: 1.4399 0.4767 sec/batch\n",
      "Epoch 10/40  Iteration 2699/11040 Training loss: 1.4398 0.4765 sec/batch\n",
      "Epoch 10/40  Iteration 2700/11040 Training loss: 1.4398 0.4747 sec/batch\n",
      "Epoch 10/40  Iteration 2701/11040 Training loss: 1.4396 0.4730 sec/batch\n",
      "Epoch 10/40  Iteration 2702/11040 Training loss: 1.4394 0.4762 sec/batch\n",
      "Epoch 10/40  Iteration 2703/11040 Training loss: 1.4393 0.4774 sec/batch\n",
      "Epoch 10/40  Iteration 2704/11040 Training loss: 1.4393 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2705/11040 Training loss: 1.4392 0.4746 sec/batch\n",
      "Epoch 10/40  Iteration 2706/11040 Training loss: 1.4390 0.4755 sec/batch\n",
      "Epoch 10/40  Iteration 2707/11040 Training loss: 1.4389 0.4619 sec/batch\n",
      "Epoch 10/40  Iteration 2708/11040 Training loss: 1.4390 0.4736 sec/batch\n",
      "Epoch 10/40  Iteration 2709/11040 Training loss: 1.4390 0.4810 sec/batch\n",
      "Epoch 10/40  Iteration 2710/11040 Training loss: 1.4390 0.4784 sec/batch\n",
      "Epoch 10/40  Iteration 2711/11040 Training loss: 1.4388 0.4812 sec/batch\n",
      "Epoch 10/40  Iteration 2712/11040 Training loss: 1.4387 0.4746 sec/batch\n",
      "Epoch 10/40  Iteration 2713/11040 Training loss: 1.4387 0.4752 sec/batch\n",
      "Epoch 10/40  Iteration 2714/11040 Training loss: 1.4388 0.4761 sec/batch\n",
      "Epoch 10/40  Iteration 2715/11040 Training loss: 1.4387 0.4732 sec/batch\n",
      "Epoch 10/40  Iteration 2716/11040 Training loss: 1.4387 0.4773 sec/batch\n",
      "Epoch 10/40  Iteration 2717/11040 Training loss: 1.4385 0.4750 sec/batch\n",
      "Epoch 10/40  Iteration 2718/11040 Training loss: 1.4383 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2719/11040 Training loss: 1.4384 0.4744 sec/batch\n",
      "Epoch 10/40  Iteration 2720/11040 Training loss: 1.4383 0.4760 sec/batch\n",
      "Epoch 10/40  Iteration 2721/11040 Training loss: 1.4382 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2722/11040 Training loss: 1.4382 0.4607 sec/batch\n",
      "Epoch 10/40  Iteration 2723/11040 Training loss: 1.4382 0.4626 sec/batch\n",
      "Epoch 10/40  Iteration 2724/11040 Training loss: 1.4382 0.4715 sec/batch\n",
      "Epoch 10/40  Iteration 2725/11040 Training loss: 1.4382 0.4765 sec/batch\n",
      "Epoch 10/40  Iteration 2726/11040 Training loss: 1.4381 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2727/11040 Training loss: 1.4380 0.4917 sec/batch\n",
      "Epoch 10/40  Iteration 2728/11040 Training loss: 1.4378 0.4771 sec/batch\n",
      "Epoch 10/40  Iteration 2729/11040 Training loss: 1.4377 0.4735 sec/batch\n",
      "Epoch 10/40  Iteration 2730/11040 Training loss: 1.4377 0.4744 sec/batch\n",
      "Epoch 10/40  Iteration 2731/11040 Training loss: 1.4376 0.4761 sec/batch\n",
      "Epoch 10/40  Iteration 2732/11040 Training loss: 1.4375 0.4780 sec/batch\n",
      "Epoch 10/40  Iteration 2733/11040 Training loss: 1.4375 0.4727 sec/batch\n",
      "Epoch 10/40  Iteration 2734/11040 Training loss: 1.4375 0.4747 sec/batch\n",
      "Epoch 10/40  Iteration 2735/11040 Training loss: 1.4373 0.4734 sec/batch\n",
      "Epoch 10/40  Iteration 2736/11040 Training loss: 1.4372 0.4752 sec/batch\n",
      "Epoch 10/40  Iteration 2737/11040 Training loss: 1.4372 0.4709 sec/batch\n",
      "Epoch 10/40  Iteration 2738/11040 Training loss: 1.4372 0.4789 sec/batch\n",
      "Epoch 10/40  Iteration 2739/11040 Training loss: 1.4371 0.4733 sec/batch\n",
      "Epoch 10/40  Iteration 2740/11040 Training loss: 1.4370 0.4790 sec/batch\n",
      "Epoch 10/40  Iteration 2741/11040 Training loss: 1.4370 0.4901 sec/batch\n",
      "Epoch 10/40  Iteration 2742/11040 Training loss: 1.4371 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2743/11040 Training loss: 1.4369 0.4778 sec/batch\n",
      "Epoch 10/40  Iteration 2744/11040 Training loss: 1.4369 0.4723 sec/batch\n",
      "Epoch 10/40  Iteration 2745/11040 Training loss: 1.4368 0.4762 sec/batch\n",
      "Epoch 10/40  Iteration 2746/11040 Training loss: 1.4369 0.4740 sec/batch\n",
      "Epoch 10/40  Iteration 2747/11040 Training loss: 1.4369 0.4743 sec/batch\n",
      "Epoch 10/40  Iteration 2748/11040 Training loss: 1.4369 0.4617 sec/batch\n",
      "Epoch 10/40  Iteration 2749/11040 Training loss: 1.4368 0.4735 sec/batch\n",
      "Epoch 10/40  Iteration 2750/11040 Training loss: 1.4369 0.4746 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40  Iteration 2751/11040 Training loss: 1.4369 0.4741 sec/batch\n",
      "Epoch 10/40  Iteration 2752/11040 Training loss: 1.4369 0.4767 sec/batch\n",
      "Epoch 10/40  Iteration 2753/11040 Training loss: 1.4370 0.4737 sec/batch\n",
      "Epoch 10/40  Iteration 2754/11040 Training loss: 1.4370 0.4770 sec/batch\n",
      "Epoch 10/40  Iteration 2755/11040 Training loss: 1.4373 0.4748 sec/batch\n",
      "Epoch 10/40  Iteration 2756/11040 Training loss: 1.4374 0.4763 sec/batch\n",
      "Epoch 10/40  Iteration 2757/11040 Training loss: 1.4375 0.4749 sec/batch\n",
      "Epoch 10/40  Iteration 2758/11040 Training loss: 1.4374 0.4918 sec/batch\n",
      "Epoch 10/40  Iteration 2759/11040 Training loss: 1.4373 0.4737 sec/batch\n",
      "Epoch 10/40  Iteration 2760/11040 Training loss: 1.4373 0.4770 sec/batch\n",
      "Epoch 11/40  Iteration 2761/11040 Training loss: 1.4978 0.4856 sec/batch\n",
      "Epoch 11/40  Iteration 2762/11040 Training loss: 1.4544 0.4823 sec/batch\n",
      "Epoch 11/40  Iteration 2763/11040 Training loss: 1.4494 0.4759 sec/batch\n",
      "Epoch 11/40  Iteration 2764/11040 Training loss: 1.4490 0.4753 sec/batch\n",
      "Epoch 11/40  Iteration 2765/11040 Training loss: 1.4521 0.4902 sec/batch\n",
      "Epoch 11/40  Iteration 2766/11040 Training loss: 1.4485 0.4747 sec/batch\n",
      "Epoch 11/40  Iteration 2767/11040 Training loss: 1.4408 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2768/11040 Training loss: 1.4387 0.4929 sec/batch\n",
      "Epoch 11/40  Iteration 2769/11040 Training loss: 1.4343 0.4811 sec/batch\n",
      "Epoch 11/40  Iteration 2770/11040 Training loss: 1.4344 0.4851 sec/batch\n",
      "Epoch 11/40  Iteration 2771/11040 Training loss: 1.4322 0.4744 sec/batch\n",
      "Epoch 11/40  Iteration 2772/11040 Training loss: 1.4324 0.4919 sec/batch\n",
      "Epoch 11/40  Iteration 2773/11040 Training loss: 1.4304 0.4899 sec/batch\n",
      "Epoch 11/40  Iteration 2774/11040 Training loss: 1.4274 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2775/11040 Training loss: 1.4270 0.4768 sec/batch\n",
      "Epoch 11/40  Iteration 2776/11040 Training loss: 1.4278 0.4765 sec/batch\n",
      "Epoch 11/40  Iteration 2777/11040 Training loss: 1.4252 0.4915 sec/batch\n",
      "Epoch 11/40  Iteration 2778/11040 Training loss: 1.4236 0.4912 sec/batch\n",
      "Epoch 11/40  Iteration 2779/11040 Training loss: 1.4230 0.4780 sec/batch\n",
      "Epoch 11/40  Iteration 2780/11040 Training loss: 1.4233 0.4893 sec/batch\n",
      "Epoch 11/40  Iteration 2781/11040 Training loss: 1.4250 0.4896 sec/batch\n",
      "Epoch 11/40  Iteration 2782/11040 Training loss: 1.4260 0.4913 sec/batch\n",
      "Epoch 11/40  Iteration 2783/11040 Training loss: 1.4261 0.4750 sec/batch\n",
      "Epoch 11/40  Iteration 2784/11040 Training loss: 1.4273 0.4725 sec/batch\n",
      "Epoch 11/40  Iteration 2785/11040 Training loss: 1.4265 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2786/11040 Training loss: 1.4255 0.4766 sec/batch\n",
      "Epoch 11/40  Iteration 2787/11040 Training loss: 1.4247 0.4907 sec/batch\n",
      "Epoch 11/40  Iteration 2788/11040 Training loss: 1.4245 0.4763 sec/batch\n",
      "Epoch 11/40  Iteration 2789/11040 Training loss: 1.4244 0.4755 sec/batch\n",
      "Epoch 11/40  Iteration 2790/11040 Training loss: 1.4246 0.4706 sec/batch\n",
      "Epoch 11/40  Iteration 2791/11040 Training loss: 1.4243 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2792/11040 Training loss: 1.4242 0.4843 sec/batch\n",
      "Epoch 11/40  Iteration 2793/11040 Training loss: 1.4239 0.4927 sec/batch\n",
      "Epoch 11/40  Iteration 2794/11040 Training loss: 1.4235 0.4815 sec/batch\n",
      "Epoch 11/40  Iteration 2795/11040 Training loss: 1.4232 0.4816 sec/batch\n",
      "Epoch 11/40  Iteration 2796/11040 Training loss: 1.4235 0.4692 sec/batch\n",
      "Epoch 11/40  Iteration 2797/11040 Training loss: 1.4231 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 2798/11040 Training loss: 1.4223 0.4749 sec/batch\n",
      "Epoch 11/40  Iteration 2799/11040 Training loss: 1.4217 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2800/11040 Training loss: 1.4219 0.4749 sec/batch\n",
      "Epoch 11/40  Iteration 2801/11040 Training loss: 1.4212 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2802/11040 Training loss: 1.4203 0.4906 sec/batch\n",
      "Epoch 11/40  Iteration 2803/11040 Training loss: 1.4206 0.4784 sec/batch\n",
      "Epoch 11/40  Iteration 2804/11040 Training loss: 1.4204 0.4765 sec/batch\n",
      "Epoch 11/40  Iteration 2805/11040 Training loss: 1.4204 0.4893 sec/batch\n",
      "Epoch 11/40  Iteration 2806/11040 Training loss: 1.4199 0.4936 sec/batch\n",
      "Epoch 11/40  Iteration 2807/11040 Training loss: 1.4192 0.4909 sec/batch\n",
      "Epoch 11/40  Iteration 2808/11040 Training loss: 1.4188 0.4768 sec/batch\n",
      "Epoch 11/40  Iteration 2809/11040 Training loss: 1.4182 0.4763 sec/batch\n",
      "Epoch 11/40  Iteration 2810/11040 Training loss: 1.4186 0.4764 sec/batch\n",
      "Epoch 11/40  Iteration 2811/11040 Training loss: 1.4183 0.4910 sec/batch\n",
      "Epoch 11/40  Iteration 2812/11040 Training loss: 1.4180 0.4799 sec/batch\n",
      "Epoch 11/40  Iteration 2813/11040 Training loss: 1.4183 0.4871 sec/batch\n",
      "Epoch 11/40  Iteration 2814/11040 Training loss: 1.4179 0.4755 sec/batch\n",
      "Epoch 11/40  Iteration 2815/11040 Training loss: 1.4180 0.4621 sec/batch\n",
      "Epoch 11/40  Iteration 2816/11040 Training loss: 1.4180 0.4732 sec/batch\n",
      "Epoch 11/40  Iteration 2817/11040 Training loss: 1.4176 0.4738 sec/batch\n",
      "Epoch 11/40  Iteration 2818/11040 Training loss: 1.4178 0.4748 sec/batch\n",
      "Epoch 11/40  Iteration 2819/11040 Training loss: 1.4181 0.4767 sec/batch\n",
      "Epoch 11/40  Iteration 2820/11040 Training loss: 1.4178 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 2821/11040 Training loss: 1.4179 0.4611 sec/batch\n",
      "Epoch 11/40  Iteration 2822/11040 Training loss: 1.4173 0.4729 sec/batch\n",
      "Epoch 11/40  Iteration 2823/11040 Training loss: 1.4171 0.4746 sec/batch\n",
      "Epoch 11/40  Iteration 2824/11040 Training loss: 1.4167 0.4772 sec/batch\n",
      "Epoch 11/40  Iteration 2825/11040 Training loss: 1.4168 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2826/11040 Training loss: 1.4163 0.4764 sec/batch\n",
      "Epoch 11/40  Iteration 2827/11040 Training loss: 1.4161 0.4744 sec/batch\n",
      "Epoch 11/40  Iteration 2828/11040 Training loss: 1.4162 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2829/11040 Training loss: 1.4160 0.4731 sec/batch\n",
      "Epoch 11/40  Iteration 2830/11040 Training loss: 1.4161 0.4759 sec/batch\n",
      "Epoch 11/40  Iteration 2831/11040 Training loss: 1.4156 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 2832/11040 Training loss: 1.4150 0.4754 sec/batch\n",
      "Epoch 11/40  Iteration 2833/11040 Training loss: 1.4144 0.4813 sec/batch\n",
      "Epoch 11/40  Iteration 2834/11040 Training loss: 1.4142 0.4684 sec/batch\n",
      "Epoch 11/40  Iteration 2835/11040 Training loss: 1.4139 0.4763 sec/batch\n",
      "Epoch 11/40  Iteration 2836/11040 Training loss: 1.4140 0.4750 sec/batch\n",
      "Epoch 11/40  Iteration 2837/11040 Training loss: 1.4136 0.4737 sec/batch\n",
      "Epoch 11/40  Iteration 2838/11040 Training loss: 1.4134 0.4750 sec/batch\n",
      "Epoch 11/40  Iteration 2839/11040 Training loss: 1.4130 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2840/11040 Training loss: 1.4129 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2841/11040 Training loss: 1.4125 0.4748 sec/batch\n",
      "Epoch 11/40  Iteration 2842/11040 Training loss: 1.4124 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2843/11040 Training loss: 1.4121 0.4915 sec/batch\n",
      "Epoch 11/40  Iteration 2844/11040 Training loss: 1.4120 0.4754 sec/batch\n",
      "Epoch 11/40  Iteration 2845/11040 Training loss: 1.4119 0.4747 sec/batch\n",
      "Epoch 11/40  Iteration 2846/11040 Training loss: 1.4117 0.4601 sec/batch\n",
      "Epoch 11/40  Iteration 2847/11040 Training loss: 1.4112 0.4749 sec/batch\n",
      "Epoch 11/40  Iteration 2848/11040 Training loss: 1.4104 0.4773 sec/batch\n",
      "Epoch 11/40  Iteration 2849/11040 Training loss: 1.4104 0.4585 sec/batch\n",
      "Epoch 11/40  Iteration 2850/11040 Training loss: 1.4105 0.4739 sec/batch\n",
      "Epoch 11/40  Iteration 2851/11040 Training loss: 1.4109 0.4769 sec/batch\n",
      "Epoch 11/40  Iteration 2852/11040 Training loss: 1.4110 0.4728 sec/batch\n",
      "Epoch 11/40  Iteration 2853/11040 Training loss: 1.4108 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 2854/11040 Training loss: 1.4112 0.4603 sec/batch\n",
      "Epoch 11/40  Iteration 2855/11040 Training loss: 1.4117 0.4591 sec/batch\n",
      "Epoch 11/40  Iteration 2856/11040 Training loss: 1.4120 0.4598 sec/batch\n",
      "Epoch 11/40  Iteration 2857/11040 Training loss: 1.4125 0.4743 sec/batch\n",
      "Epoch 11/40  Iteration 2858/11040 Training loss: 1.4129 0.4749 sec/batch\n",
      "Epoch 11/40  Iteration 2859/11040 Training loss: 1.4130 0.4589 sec/batch\n",
      "Epoch 11/40  Iteration 2860/11040 Training loss: 1.4128 0.4751 sec/batch\n",
      "Epoch 11/40  Iteration 2861/11040 Training loss: 1.4129 0.4750 sec/batch\n",
      "Epoch 11/40  Iteration 2862/11040 Training loss: 1.4129 0.4733 sec/batch\n",
      "Epoch 11/40  Iteration 2863/11040 Training loss: 1.4124 0.4761 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40  Iteration 2864/11040 Training loss: 1.4126 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 2865/11040 Training loss: 1.4123 0.4732 sec/batch\n",
      "Epoch 11/40  Iteration 2866/11040 Training loss: 1.4123 0.4616 sec/batch\n",
      "Epoch 11/40  Iteration 2867/11040 Training loss: 1.4126 0.4574 sec/batch\n",
      "Epoch 11/40  Iteration 2868/11040 Training loss: 1.4128 0.4598 sec/batch\n",
      "Epoch 11/40  Iteration 2869/11040 Training loss: 1.4131 0.4586 sec/batch\n",
      "Epoch 11/40  Iteration 2870/11040 Training loss: 1.4132 0.4749 sec/batch\n",
      "Epoch 11/40  Iteration 2871/11040 Training loss: 1.4133 0.4737 sec/batch\n",
      "Epoch 11/40  Iteration 2872/11040 Training loss: 1.4135 0.4769 sec/batch\n",
      "Epoch 11/40  Iteration 2873/11040 Training loss: 1.4138 0.4742 sec/batch\n",
      "Epoch 11/40  Iteration 2874/11040 Training loss: 1.4140 0.4782 sec/batch\n",
      "Epoch 11/40  Iteration 2875/11040 Training loss: 1.4145 0.4694 sec/batch\n",
      "Epoch 11/40  Iteration 2876/11040 Training loss: 1.4149 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 2877/11040 Training loss: 1.4149 0.4741 sec/batch\n",
      "Epoch 11/40  Iteration 2878/11040 Training loss: 1.4153 0.4597 sec/batch\n",
      "Epoch 11/40  Iteration 2879/11040 Training loss: 1.4153 0.4582 sec/batch\n",
      "Epoch 11/40  Iteration 2880/11040 Training loss: 1.4155 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 2881/11040 Training loss: 1.4156 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 2882/11040 Training loss: 1.4157 0.4771 sec/batch\n",
      "Epoch 11/40  Iteration 2883/11040 Training loss: 1.4160 0.4728 sec/batch\n",
      "Epoch 11/40  Iteration 2884/11040 Training loss: 1.4164 0.4593 sec/batch\n",
      "Epoch 11/40  Iteration 2885/11040 Training loss: 1.4165 0.4682 sec/batch\n",
      "Epoch 11/40  Iteration 2886/11040 Training loss: 1.4167 0.4827 sec/batch\n",
      "Epoch 11/40  Iteration 2887/11040 Training loss: 1.4167 0.4727 sec/batch\n",
      "Epoch 11/40  Iteration 2888/11040 Training loss: 1.4167 0.4756 sec/batch\n",
      "Epoch 11/40  Iteration 2889/11040 Training loss: 1.4165 0.4598 sec/batch\n",
      "Epoch 11/40  Iteration 2890/11040 Training loss: 1.4166 0.4741 sec/batch\n",
      "Epoch 11/40  Iteration 2891/11040 Training loss: 1.4164 0.4747 sec/batch\n",
      "Epoch 11/40  Iteration 2892/11040 Training loss: 1.4162 0.4778 sec/batch\n",
      "Epoch 11/40  Iteration 2893/11040 Training loss: 1.4163 0.4709 sec/batch\n",
      "Epoch 11/40  Iteration 2894/11040 Training loss: 1.4162 0.4767 sec/batch\n",
      "Epoch 11/40  Iteration 2895/11040 Training loss: 1.4165 0.4743 sec/batch\n",
      "Epoch 11/40  Iteration 2896/11040 Training loss: 1.4167 0.4585 sec/batch\n",
      "Epoch 11/40  Iteration 2897/11040 Training loss: 1.4166 0.4749 sec/batch\n",
      "Epoch 11/40  Iteration 2898/11040 Training loss: 1.4168 0.4741 sec/batch\n",
      "Epoch 11/40  Iteration 2899/11040 Training loss: 1.4170 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 2900/11040 Training loss: 1.4170 0.4736 sec/batch\n",
      "Epoch 11/40  Iteration 2901/11040 Training loss: 1.4170 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2902/11040 Training loss: 1.4172 0.4766 sec/batch\n",
      "Epoch 11/40  Iteration 2903/11040 Training loss: 1.4170 0.4726 sec/batch\n",
      "Epoch 11/40  Iteration 2904/11040 Training loss: 1.4168 0.4751 sec/batch\n",
      "Epoch 11/40  Iteration 2905/11040 Training loss: 1.4168 0.4728 sec/batch\n",
      "Epoch 11/40  Iteration 2906/11040 Training loss: 1.4167 0.4617 sec/batch\n",
      "Epoch 11/40  Iteration 2907/11040 Training loss: 1.4167 0.4744 sec/batch\n",
      "Epoch 11/40  Iteration 2908/11040 Training loss: 1.4166 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 2909/11040 Training loss: 1.4163 0.4594 sec/batch\n",
      "Epoch 11/40  Iteration 2910/11040 Training loss: 1.4164 0.4741 sec/batch\n",
      "Epoch 11/40  Iteration 2911/11040 Training loss: 1.4163 0.4750 sec/batch\n",
      "Epoch 11/40  Iteration 2912/11040 Training loss: 1.4163 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 2913/11040 Training loss: 1.4162 0.4698 sec/batch\n",
      "Epoch 11/40  Iteration 2914/11040 Training loss: 1.4162 0.4614 sec/batch\n",
      "Epoch 11/40  Iteration 2915/11040 Training loss: 1.4163 0.4595 sec/batch\n",
      "Epoch 11/40  Iteration 2916/11040 Training loss: 1.4162 0.4592 sec/batch\n",
      "Epoch 11/40  Iteration 2917/11040 Training loss: 1.4160 0.4586 sec/batch\n",
      "Epoch 11/40  Iteration 2918/11040 Training loss: 1.4160 0.4746 sec/batch\n",
      "Epoch 11/40  Iteration 2919/11040 Training loss: 1.4162 0.4757 sec/batch\n",
      "Epoch 11/40  Iteration 2920/11040 Training loss: 1.4162 0.4602 sec/batch\n",
      "Epoch 11/40  Iteration 2921/11040 Training loss: 1.4164 0.4589 sec/batch\n",
      "Epoch 11/40  Iteration 2922/11040 Training loss: 1.4163 0.4734 sec/batch\n",
      "Epoch 11/40  Iteration 2923/11040 Training loss: 1.4164 0.4742 sec/batch\n",
      "Epoch 11/40  Iteration 2924/11040 Training loss: 1.4164 0.4763 sec/batch\n",
      "Epoch 11/40  Iteration 2925/11040 Training loss: 1.4165 0.4742 sec/batch\n",
      "Epoch 11/40  Iteration 2926/11040 Training loss: 1.4167 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2927/11040 Training loss: 1.4168 0.4763 sec/batch\n",
      "Epoch 11/40  Iteration 2928/11040 Training loss: 1.4166 0.4585 sec/batch\n",
      "Epoch 11/40  Iteration 2929/11040 Training loss: 1.4165 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2930/11040 Training loss: 1.4164 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 2931/11040 Training loss: 1.4166 0.4746 sec/batch\n",
      "Epoch 11/40  Iteration 2932/11040 Training loss: 1.4166 0.4770 sec/batch\n",
      "Epoch 11/40  Iteration 2933/11040 Training loss: 1.4166 0.4876 sec/batch\n",
      "Epoch 11/40  Iteration 2934/11040 Training loss: 1.4165 0.4733 sec/batch\n",
      "Epoch 11/40  Iteration 2935/11040 Training loss: 1.4164 0.4929 sec/batch\n",
      "Epoch 11/40  Iteration 2936/11040 Training loss: 1.4164 0.4744 sec/batch\n",
      "Epoch 11/40  Iteration 2937/11040 Training loss: 1.4165 0.4647 sec/batch\n",
      "Epoch 11/40  Iteration 2938/11040 Training loss: 1.4164 0.4702 sec/batch\n",
      "Epoch 11/40  Iteration 2939/11040 Training loss: 1.4166 0.4587 sec/batch\n",
      "Epoch 11/40  Iteration 2940/11040 Training loss: 1.4167 0.4601 sec/batch\n",
      "Epoch 11/40  Iteration 2941/11040 Training loss: 1.4167 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2942/11040 Training loss: 1.4168 0.4757 sec/batch\n",
      "Epoch 11/40  Iteration 2943/11040 Training loss: 1.4165 0.4593 sec/batch\n",
      "Epoch 11/40  Iteration 2944/11040 Training loss: 1.4166 0.4743 sec/batch\n",
      "Epoch 11/40  Iteration 2945/11040 Training loss: 1.4165 0.4759 sec/batch\n",
      "Epoch 11/40  Iteration 2946/11040 Training loss: 1.4166 0.4744 sec/batch\n",
      "Epoch 11/40  Iteration 2947/11040 Training loss: 1.4169 0.4747 sec/batch\n",
      "Epoch 11/40  Iteration 2948/11040 Training loss: 1.4169 0.4746 sec/batch\n",
      "Epoch 11/40  Iteration 2949/11040 Training loss: 1.4169 0.4756 sec/batch\n",
      "Epoch 11/40  Iteration 2950/11040 Training loss: 1.4168 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 2951/11040 Training loss: 1.4166 0.4739 sec/batch\n",
      "Epoch 11/40  Iteration 2952/11040 Training loss: 1.4166 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2953/11040 Training loss: 1.4165 0.4729 sec/batch\n",
      "Epoch 11/40  Iteration 2954/11040 Training loss: 1.4165 0.4773 sec/batch\n",
      "Epoch 11/40  Iteration 2955/11040 Training loss: 1.4167 0.4716 sec/batch\n",
      "Epoch 11/40  Iteration 2956/11040 Training loss: 1.4167 0.4601 sec/batch\n",
      "Epoch 11/40  Iteration 2957/11040 Training loss: 1.4168 0.4643 sec/batch\n",
      "Epoch 11/40  Iteration 2958/11040 Training loss: 1.4168 0.4729 sec/batch\n",
      "Epoch 11/40  Iteration 2959/11040 Training loss: 1.4169 0.4715 sec/batch\n",
      "Epoch 11/40  Iteration 2960/11040 Training loss: 1.4170 0.4747 sec/batch\n",
      "Epoch 11/40  Iteration 2961/11040 Training loss: 1.4171 0.4755 sec/batch\n",
      "Epoch 11/40  Iteration 2962/11040 Training loss: 1.4172 0.4731 sec/batch\n",
      "Epoch 11/40  Iteration 2963/11040 Training loss: 1.4174 0.4694 sec/batch\n",
      "Epoch 11/40  Iteration 2964/11040 Training loss: 1.4175 0.4657 sec/batch\n",
      "Epoch 11/40  Iteration 2965/11040 Training loss: 1.4174 0.4764 sec/batch\n",
      "Epoch 11/40  Iteration 2966/11040 Training loss: 1.4174 0.4729 sec/batch\n",
      "Epoch 11/40  Iteration 2967/11040 Training loss: 1.4174 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 2968/11040 Training loss: 1.4174 0.4729 sec/batch\n",
      "Epoch 11/40  Iteration 2969/11040 Training loss: 1.4173 0.4615 sec/batch\n",
      "Epoch 11/40  Iteration 2970/11040 Training loss: 1.4172 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 2971/11040 Training loss: 1.4171 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2972/11040 Training loss: 1.4171 0.4761 sec/batch\n",
      "Epoch 11/40  Iteration 2973/11040 Training loss: 1.4172 0.4758 sec/batch\n",
      "Epoch 11/40  Iteration 2974/11040 Training loss: 1.4171 0.4734 sec/batch\n",
      "Epoch 11/40  Iteration 2975/11040 Training loss: 1.4170 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2976/11040 Training loss: 1.4170 0.4636 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40  Iteration 2977/11040 Training loss: 1.4168 0.4863 sec/batch\n",
      "Epoch 11/40  Iteration 2978/11040 Training loss: 1.4166 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 2979/11040 Training loss: 1.4165 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2980/11040 Training loss: 1.4165 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2981/11040 Training loss: 1.4164 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 2982/11040 Training loss: 1.4163 0.4611 sec/batch\n",
      "Epoch 11/40  Iteration 2983/11040 Training loss: 1.4161 0.4583 sec/batch\n",
      "Epoch 11/40  Iteration 2984/11040 Training loss: 1.4163 0.4697 sec/batch\n",
      "Epoch 11/40  Iteration 2985/11040 Training loss: 1.4163 0.4806 sec/batch\n",
      "Epoch 11/40  Iteration 2986/11040 Training loss: 1.4163 0.4756 sec/batch\n",
      "Epoch 11/40  Iteration 2987/11040 Training loss: 1.4161 0.4720 sec/batch\n",
      "Epoch 11/40  Iteration 2988/11040 Training loss: 1.4160 0.4747 sec/batch\n",
      "Epoch 11/40  Iteration 2989/11040 Training loss: 1.4160 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 2990/11040 Training loss: 1.4160 0.4748 sec/batch\n",
      "Epoch 11/40  Iteration 2991/11040 Training loss: 1.4160 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 2992/11040 Training loss: 1.4160 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 2993/11040 Training loss: 1.4158 0.4720 sec/batch\n",
      "Epoch 11/40  Iteration 2994/11040 Training loss: 1.4157 0.4589 sec/batch\n",
      "Epoch 11/40  Iteration 2995/11040 Training loss: 1.4157 0.4603 sec/batch\n",
      "Epoch 11/40  Iteration 2996/11040 Training loss: 1.4156 0.4602 sec/batch\n",
      "Epoch 11/40  Iteration 2997/11040 Training loss: 1.4156 0.4730 sec/batch\n",
      "Epoch 11/40  Iteration 2998/11040 Training loss: 1.4156 0.4761 sec/batch\n",
      "Epoch 11/40  Iteration 2999/11040 Training loss: 1.4156 0.4731 sec/batch\n",
      "Epoch 11/40  Iteration 3000/11040 Training loss: 1.4156 0.4629 sec/batch\n",
      "Validation loss: 1.35256 Saving checkpoint!\n",
      "Epoch 11/40  Iteration 3001/11040 Training loss: 1.4159 0.4688 sec/batch\n",
      "Epoch 11/40  Iteration 3002/11040 Training loss: 1.4159 0.4701 sec/batch\n",
      "Epoch 11/40  Iteration 3003/11040 Training loss: 1.4158 0.4725 sec/batch\n",
      "Epoch 11/40  Iteration 3004/11040 Training loss: 1.4156 0.4611 sec/batch\n",
      "Epoch 11/40  Iteration 3005/11040 Training loss: 1.4154 0.4737 sec/batch\n",
      "Epoch 11/40  Iteration 3006/11040 Training loss: 1.4154 0.4752 sec/batch\n",
      "Epoch 11/40  Iteration 3007/11040 Training loss: 1.4154 0.4754 sec/batch\n",
      "Epoch 11/40  Iteration 3008/11040 Training loss: 1.4153 0.4602 sec/batch\n",
      "Epoch 11/40  Iteration 3009/11040 Training loss: 1.4152 0.4732 sec/batch\n",
      "Epoch 11/40  Iteration 3010/11040 Training loss: 1.4152 0.4738 sec/batch\n",
      "Epoch 11/40  Iteration 3011/11040 Training loss: 1.4151 0.4762 sec/batch\n",
      "Epoch 11/40  Iteration 3012/11040 Training loss: 1.4150 0.4725 sec/batch\n",
      "Epoch 11/40  Iteration 3013/11040 Training loss: 1.4150 0.4761 sec/batch\n",
      "Epoch 11/40  Iteration 3014/11040 Training loss: 1.4150 0.4738 sec/batch\n",
      "Epoch 11/40  Iteration 3015/11040 Training loss: 1.4149 0.4702 sec/batch\n",
      "Epoch 11/40  Iteration 3016/11040 Training loss: 1.4149 0.4642 sec/batch\n",
      "Epoch 11/40  Iteration 3017/11040 Training loss: 1.4149 0.4735 sec/batch\n",
      "Epoch 11/40  Iteration 3018/11040 Training loss: 1.4150 0.4605 sec/batch\n",
      "Epoch 11/40  Iteration 3019/11040 Training loss: 1.4149 0.4767 sec/batch\n",
      "Epoch 11/40  Iteration 3020/11040 Training loss: 1.4148 0.4759 sec/batch\n",
      "Epoch 11/40  Iteration 3021/11040 Training loss: 1.4148 0.4877 sec/batch\n",
      "Epoch 11/40  Iteration 3022/11040 Training loss: 1.4149 0.4720 sec/batch\n",
      "Epoch 11/40  Iteration 3023/11040 Training loss: 1.4149 0.4760 sec/batch\n",
      "Epoch 11/40  Iteration 3024/11040 Training loss: 1.4150 0.4757 sec/batch\n",
      "Epoch 11/40  Iteration 3025/11040 Training loss: 1.4149 0.4626 sec/batch\n",
      "Epoch 11/40  Iteration 3026/11040 Training loss: 1.4150 0.4718 sec/batch\n",
      "Epoch 11/40  Iteration 3027/11040 Training loss: 1.4151 0.4740 sec/batch\n",
      "Epoch 11/40  Iteration 3028/11040 Training loss: 1.4151 0.4753 sec/batch\n",
      "Epoch 11/40  Iteration 3029/11040 Training loss: 1.4151 0.4733 sec/batch\n",
      "Epoch 11/40  Iteration 3030/11040 Training loss: 1.4152 0.4680 sec/batch\n",
      "Epoch 11/40  Iteration 3031/11040 Training loss: 1.4154 0.4668 sec/batch\n",
      "Epoch 11/40  Iteration 3032/11040 Training loss: 1.4156 0.4745 sec/batch\n",
      "Epoch 11/40  Iteration 3033/11040 Training loss: 1.4156 0.4754 sec/batch\n",
      "Epoch 11/40  Iteration 3034/11040 Training loss: 1.4156 0.4764 sec/batch\n",
      "Epoch 11/40  Iteration 3035/11040 Training loss: 1.4156 0.4878 sec/batch\n",
      "Epoch 11/40  Iteration 3036/11040 Training loss: 1.4156 0.4642 sec/batch\n",
      "Epoch 12/40  Iteration 3037/11040 Training loss: 1.4710 0.4564 sec/batch\n",
      "Epoch 12/40  Iteration 3038/11040 Training loss: 1.4310 0.4734 sec/batch\n",
      "Epoch 12/40  Iteration 3039/11040 Training loss: 1.4281 0.4773 sec/batch\n",
      "Epoch 12/40  Iteration 3040/11040 Training loss: 1.4276 0.4730 sec/batch\n",
      "Epoch 12/40  Iteration 3041/11040 Training loss: 1.4280 0.4725 sec/batch\n",
      "Epoch 12/40  Iteration 3042/11040 Training loss: 1.4256 0.4770 sec/batch\n",
      "Epoch 12/40  Iteration 3043/11040 Training loss: 1.4165 0.4746 sec/batch\n",
      "Epoch 12/40  Iteration 3044/11040 Training loss: 1.4153 0.4730 sec/batch\n",
      "Epoch 12/40  Iteration 3045/11040 Training loss: 1.4117 0.4759 sec/batch\n",
      "Epoch 12/40  Iteration 3046/11040 Training loss: 1.4120 0.4587 sec/batch\n",
      "Epoch 12/40  Iteration 3047/11040 Training loss: 1.4100 0.4597 sec/batch\n",
      "Epoch 12/40  Iteration 3048/11040 Training loss: 1.4091 0.4585 sec/batch\n",
      "Epoch 12/40  Iteration 3049/11040 Training loss: 1.4073 0.4746 sec/batch\n",
      "Epoch 12/40  Iteration 3050/11040 Training loss: 1.4052 0.4768 sec/batch\n",
      "Epoch 12/40  Iteration 3051/11040 Training loss: 1.4047 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3052/11040 Training loss: 1.4049 0.4755 sec/batch\n",
      "Epoch 12/40  Iteration 3053/11040 Training loss: 1.4024 0.4588 sec/batch\n",
      "Epoch 12/40  Iteration 3054/11040 Training loss: 1.4005 0.4731 sec/batch\n",
      "Epoch 12/40  Iteration 3055/11040 Training loss: 1.3994 0.4760 sec/batch\n",
      "Epoch 12/40  Iteration 3056/11040 Training loss: 1.4000 0.4733 sec/batch\n",
      "Epoch 12/40  Iteration 3057/11040 Training loss: 1.4013 0.4603 sec/batch\n",
      "Epoch 12/40  Iteration 3058/11040 Training loss: 1.4024 0.4760 sec/batch\n",
      "Epoch 12/40  Iteration 3059/11040 Training loss: 1.4028 0.4732 sec/batch\n",
      "Epoch 12/40  Iteration 3060/11040 Training loss: 1.4037 0.4763 sec/batch\n",
      "Epoch 12/40  Iteration 3061/11040 Training loss: 1.4029 0.4755 sec/batch\n",
      "Epoch 12/40  Iteration 3062/11040 Training loss: 1.4021 0.4734 sec/batch\n",
      "Epoch 12/40  Iteration 3063/11040 Training loss: 1.4015 0.4607 sec/batch\n",
      "Epoch 12/40  Iteration 3064/11040 Training loss: 1.4008 0.4736 sec/batch\n",
      "Epoch 12/40  Iteration 3065/11040 Training loss: 1.4010 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3066/11040 Training loss: 1.4013 0.4769 sec/batch\n",
      "Epoch 12/40  Iteration 3067/11040 Training loss: 1.4011 0.4733 sec/batch\n",
      "Epoch 12/40  Iteration 3068/11040 Training loss: 1.4011 0.4730 sec/batch\n",
      "Epoch 12/40  Iteration 3069/11040 Training loss: 1.4010 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3070/11040 Training loss: 1.4006 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3071/11040 Training loss: 1.4004 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3072/11040 Training loss: 1.4009 0.4583 sec/batch\n",
      "Epoch 12/40  Iteration 3073/11040 Training loss: 1.4006 0.4762 sec/batch\n",
      "Epoch 12/40  Iteration 3074/11040 Training loss: 1.3999 0.4727 sec/batch\n",
      "Epoch 12/40  Iteration 3075/11040 Training loss: 1.3991 0.4761 sec/batch\n",
      "Epoch 12/40  Iteration 3076/11040 Training loss: 1.3995 0.4757 sec/batch\n",
      "Epoch 12/40  Iteration 3077/11040 Training loss: 1.3989 0.4749 sec/batch\n",
      "Epoch 12/40  Iteration 3078/11040 Training loss: 1.3980 0.4737 sec/batch\n",
      "Epoch 12/40  Iteration 3079/11040 Training loss: 1.3984 0.4755 sec/batch\n",
      "Epoch 12/40  Iteration 3080/11040 Training loss: 1.3983 0.4594 sec/batch\n",
      "Epoch 12/40  Iteration 3081/11040 Training loss: 1.3982 0.4740 sec/batch\n",
      "Epoch 12/40  Iteration 3082/11040 Training loss: 1.3977 0.4761 sec/batch\n",
      "Epoch 12/40  Iteration 3083/11040 Training loss: 1.3971 0.4741 sec/batch\n",
      "Epoch 12/40  Iteration 3084/11040 Training loss: 1.3968 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3085/11040 Training loss: 1.3960 0.4799 sec/batch\n",
      "Epoch 12/40  Iteration 3086/11040 Training loss: 1.3964 0.4696 sec/batch\n",
      "Epoch 12/40  Iteration 3087/11040 Training loss: 1.3964 0.4755 sec/batch\n",
      "Epoch 12/40  Iteration 3088/11040 Training loss: 1.3961 0.4575 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40  Iteration 3089/11040 Training loss: 1.3965 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3090/11040 Training loss: 1.3963 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3091/11040 Training loss: 1.3964 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3092/11040 Training loss: 1.3964 0.4741 sec/batch\n",
      "Epoch 12/40  Iteration 3093/11040 Training loss: 1.3961 0.4612 sec/batch\n",
      "Epoch 12/40  Iteration 3094/11040 Training loss: 1.3961 0.4584 sec/batch\n",
      "Epoch 12/40  Iteration 3095/11040 Training loss: 1.3964 0.4593 sec/batch\n",
      "Epoch 12/40  Iteration 3096/11040 Training loss: 1.3960 0.4585 sec/batch\n",
      "Epoch 12/40  Iteration 3097/11040 Training loss: 1.3961 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3098/11040 Training loss: 1.3955 0.4727 sec/batch\n",
      "Epoch 12/40  Iteration 3099/11040 Training loss: 1.3954 0.4624 sec/batch\n",
      "Epoch 12/40  Iteration 3100/11040 Training loss: 1.3952 0.4732 sec/batch\n",
      "Epoch 12/40  Iteration 3101/11040 Training loss: 1.3951 0.4611 sec/batch\n",
      "Epoch 12/40  Iteration 3102/11040 Training loss: 1.3947 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3103/11040 Training loss: 1.3944 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3104/11040 Training loss: 1.3945 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3105/11040 Training loss: 1.3943 0.4749 sec/batch\n",
      "Epoch 12/40  Iteration 3106/11040 Training loss: 1.3945 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3107/11040 Training loss: 1.3940 0.4559 sec/batch\n",
      "Epoch 12/40  Iteration 3108/11040 Training loss: 1.3935 0.4632 sec/batch\n",
      "Epoch 12/40  Iteration 3109/11040 Training loss: 1.3930 0.4740 sec/batch\n",
      "Epoch 12/40  Iteration 3110/11040 Training loss: 1.3928 0.4749 sec/batch\n",
      "Epoch 12/40  Iteration 3111/11040 Training loss: 1.3925 0.4742 sec/batch\n",
      "Epoch 12/40  Iteration 3112/11040 Training loss: 1.3927 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3113/11040 Training loss: 1.3923 0.4741 sec/batch\n",
      "Epoch 12/40  Iteration 3114/11040 Training loss: 1.3920 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3115/11040 Training loss: 1.3916 0.4753 sec/batch\n",
      "Epoch 12/40  Iteration 3116/11040 Training loss: 1.3916 0.4744 sec/batch\n",
      "Epoch 12/40  Iteration 3117/11040 Training loss: 1.3913 0.4758 sec/batch\n",
      "Epoch 12/40  Iteration 3118/11040 Training loss: 1.3914 0.4824 sec/batch\n",
      "Epoch 12/40  Iteration 3119/11040 Training loss: 1.3913 0.4832 sec/batch\n",
      "Epoch 12/40  Iteration 3120/11040 Training loss: 1.3911 0.4746 sec/batch\n",
      "Epoch 12/40  Iteration 3121/11040 Training loss: 1.3910 0.4790 sec/batch\n",
      "Epoch 12/40  Iteration 3122/11040 Training loss: 1.3910 0.4720 sec/batch\n",
      "Epoch 12/40  Iteration 3123/11040 Training loss: 1.3905 0.4573 sec/batch\n",
      "Epoch 12/40  Iteration 3124/11040 Training loss: 1.3897 0.4747 sec/batch\n",
      "Epoch 12/40  Iteration 3125/11040 Training loss: 1.3898 0.4773 sec/batch\n",
      "Epoch 12/40  Iteration 3126/11040 Training loss: 1.3897 0.4728 sec/batch\n",
      "Epoch 12/40  Iteration 3127/11040 Training loss: 1.3901 0.4597 sec/batch\n",
      "Epoch 12/40  Iteration 3128/11040 Training loss: 1.3901 0.4594 sec/batch\n",
      "Epoch 12/40  Iteration 3129/11040 Training loss: 1.3899 0.4732 sec/batch\n",
      "Epoch 12/40  Iteration 3130/11040 Training loss: 1.3902 0.4751 sec/batch\n",
      "Epoch 12/40  Iteration 3131/11040 Training loss: 1.3907 0.4756 sec/batch\n",
      "Epoch 12/40  Iteration 3132/11040 Training loss: 1.3909 0.4733 sec/batch\n",
      "Epoch 12/40  Iteration 3133/11040 Training loss: 1.3913 0.4744 sec/batch\n",
      "Epoch 12/40  Iteration 3134/11040 Training loss: 1.3916 0.4760 sec/batch\n",
      "Epoch 12/40  Iteration 3135/11040 Training loss: 1.3917 0.4743 sec/batch\n",
      "Epoch 12/40  Iteration 3136/11040 Training loss: 1.3914 0.4765 sec/batch\n",
      "Epoch 12/40  Iteration 3137/11040 Training loss: 1.3914 0.4726 sec/batch\n",
      "Epoch 12/40  Iteration 3138/11040 Training loss: 1.3915 0.4591 sec/batch\n",
      "Epoch 12/40  Iteration 3139/11040 Training loss: 1.3909 0.4603 sec/batch\n",
      "Epoch 12/40  Iteration 3140/11040 Training loss: 1.3911 0.4732 sec/batch\n",
      "Epoch 12/40  Iteration 3141/11040 Training loss: 1.3908 0.4587 sec/batch\n",
      "Epoch 12/40  Iteration 3142/11040 Training loss: 1.3908 0.4601 sec/batch\n",
      "Epoch 12/40  Iteration 3143/11040 Training loss: 1.3911 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3144/11040 Training loss: 1.3913 0.4748 sec/batch\n",
      "Epoch 12/40  Iteration 3145/11040 Training loss: 1.3916 0.4746 sec/batch\n",
      "Epoch 12/40  Iteration 3146/11040 Training loss: 1.3916 0.4748 sec/batch\n",
      "Epoch 12/40  Iteration 3147/11040 Training loss: 1.3917 0.4597 sec/batch\n",
      "Epoch 12/40  Iteration 3148/11040 Training loss: 1.3919 0.4588 sec/batch\n",
      "Epoch 12/40  Iteration 3149/11040 Training loss: 1.3921 0.4578 sec/batch\n",
      "Epoch 12/40  Iteration 3150/11040 Training loss: 1.3923 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3151/11040 Training loss: 1.3927 0.4599 sec/batch\n",
      "Epoch 12/40  Iteration 3152/11040 Training loss: 1.3931 0.4743 sec/batch\n",
      "Epoch 12/40  Iteration 3153/11040 Training loss: 1.3930 0.4757 sec/batch\n",
      "Epoch 12/40  Iteration 3154/11040 Training loss: 1.3935 0.4582 sec/batch\n",
      "Epoch 12/40  Iteration 3155/11040 Training loss: 1.3936 0.4593 sec/batch\n",
      "Epoch 12/40  Iteration 3156/11040 Training loss: 1.3938 0.4606 sec/batch\n",
      "Epoch 12/40  Iteration 3157/11040 Training loss: 1.3938 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3158/11040 Training loss: 1.3938 0.4740 sec/batch\n",
      "Epoch 12/40  Iteration 3159/11040 Training loss: 1.3941 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3160/11040 Training loss: 1.3946 0.4591 sec/batch\n",
      "Epoch 12/40  Iteration 3161/11040 Training loss: 1.3945 0.4596 sec/batch\n",
      "Epoch 12/40  Iteration 3162/11040 Training loss: 1.3948 0.4582 sec/batch\n",
      "Epoch 12/40  Iteration 3163/11040 Training loss: 1.3948 0.4779 sec/batch\n",
      "Epoch 12/40  Iteration 3164/11040 Training loss: 1.3947 0.6616 sec/batch\n",
      "Epoch 12/40  Iteration 3165/11040 Training loss: 1.3946 0.6785 sec/batch\n",
      "Epoch 12/40  Iteration 3166/11040 Training loss: 1.3948 0.6648 sec/batch\n",
      "Epoch 12/40  Iteration 3167/11040 Training loss: 1.3947 0.6758 sec/batch\n",
      "Epoch 12/40  Iteration 3168/11040 Training loss: 1.3944 0.6963 sec/batch\n",
      "Epoch 12/40  Iteration 3169/11040 Training loss: 1.3945 0.6795 sec/batch\n",
      "Epoch 12/40  Iteration 3170/11040 Training loss: 1.3944 0.6818 sec/batch\n",
      "Epoch 12/40  Iteration 3171/11040 Training loss: 1.3946 0.6606 sec/batch\n",
      "Epoch 12/40  Iteration 3172/11040 Training loss: 1.3948 0.6779 sec/batch\n",
      "Epoch 12/40  Iteration 3173/11040 Training loss: 1.3948 0.6637 sec/batch\n",
      "Epoch 12/40  Iteration 3174/11040 Training loss: 1.3950 0.6944 sec/batch\n",
      "Epoch 12/40  Iteration 3175/11040 Training loss: 1.3951 0.6784 sec/batch\n",
      "Epoch 12/40  Iteration 3176/11040 Training loss: 1.3951 0.7278 sec/batch\n",
      "Epoch 12/40  Iteration 3177/11040 Training loss: 1.3952 0.6672 sec/batch\n",
      "Epoch 12/40  Iteration 3178/11040 Training loss: 1.3952 0.8984 sec/batch\n",
      "Epoch 12/40  Iteration 3179/11040 Training loss: 1.3950 0.7913 sec/batch\n",
      "Epoch 12/40  Iteration 3180/11040 Training loss: 1.3950 0.7971 sec/batch\n",
      "Epoch 12/40  Iteration 3181/11040 Training loss: 1.3950 0.7563 sec/batch\n",
      "Epoch 12/40  Iteration 3182/11040 Training loss: 1.3949 0.7571 sec/batch\n",
      "Epoch 12/40  Iteration 3183/11040 Training loss: 1.3949 0.7955 sec/batch\n",
      "Epoch 12/40  Iteration 3184/11040 Training loss: 1.3948 0.8008 sec/batch\n",
      "Epoch 12/40  Iteration 3185/11040 Training loss: 1.3945 0.8138 sec/batch\n",
      "Epoch 12/40  Iteration 3186/11040 Training loss: 1.3945 0.8606 sec/batch\n",
      "Epoch 12/40  Iteration 3187/11040 Training loss: 1.3944 0.8027 sec/batch\n",
      "Epoch 12/40  Iteration 3188/11040 Training loss: 1.3945 0.8285 sec/batch\n",
      "Epoch 12/40  Iteration 3189/11040 Training loss: 1.3944 0.7846 sec/batch\n",
      "Epoch 12/40  Iteration 3190/11040 Training loss: 1.3945 0.8289 sec/batch\n",
      "Epoch 12/40  Iteration 3191/11040 Training loss: 1.3945 0.8139 sec/batch\n",
      "Epoch 12/40  Iteration 3192/11040 Training loss: 1.3945 0.8141 sec/batch\n",
      "Epoch 12/40  Iteration 3193/11040 Training loss: 1.3943 0.7200 sec/batch\n",
      "Epoch 12/40  Iteration 3194/11040 Training loss: 1.3945 0.7928 sec/batch\n",
      "Epoch 12/40  Iteration 3195/11040 Training loss: 1.3946 0.4818 sec/batch\n",
      "Epoch 12/40  Iteration 3196/11040 Training loss: 1.3946 0.4758 sec/batch\n",
      "Epoch 12/40  Iteration 3197/11040 Training loss: 1.3947 0.4756 sec/batch\n",
      "Epoch 12/40  Iteration 3198/11040 Training loss: 1.3947 0.4725 sec/batch\n",
      "Epoch 12/40  Iteration 3199/11040 Training loss: 1.3947 0.4738 sec/batch\n",
      "Epoch 12/40  Iteration 3200/11040 Training loss: 1.3948 0.4774 sec/batch\n",
      "Epoch 12/40  Iteration 3201/11040 Training loss: 1.3948 0.4631 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40  Iteration 3202/11040 Training loss: 1.3951 0.4705 sec/batch\n",
      "Epoch 12/40  Iteration 3203/11040 Training loss: 1.3951 0.4729 sec/batch\n",
      "Epoch 12/40  Iteration 3204/11040 Training loss: 1.3950 0.4747 sec/batch\n",
      "Epoch 12/40  Iteration 3205/11040 Training loss: 1.3949 0.9804 sec/batch\n",
      "Epoch 12/40  Iteration 3206/11040 Training loss: 1.3948 0.8787 sec/batch\n",
      "Epoch 12/40  Iteration 3207/11040 Training loss: 1.3950 0.7119 sec/batch\n",
      "Epoch 12/40  Iteration 3208/11040 Training loss: 1.3951 0.7237 sec/batch\n",
      "Epoch 12/40  Iteration 3209/11040 Training loss: 1.3951 0.6660 sec/batch\n",
      "Epoch 12/40  Iteration 3210/11040 Training loss: 1.3951 0.6315 sec/batch\n",
      "Epoch 12/40  Iteration 3211/11040 Training loss: 1.3951 0.4718 sec/batch\n",
      "Epoch 12/40  Iteration 3212/11040 Training loss: 1.3951 0.4737 sec/batch\n",
      "Epoch 12/40  Iteration 3213/11040 Training loss: 1.3951 0.4737 sec/batch\n",
      "Epoch 12/40  Iteration 3214/11040 Training loss: 1.3951 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3215/11040 Training loss: 1.3953 0.4615 sec/batch\n",
      "Epoch 12/40  Iteration 3216/11040 Training loss: 1.3954 0.4719 sec/batch\n",
      "Epoch 12/40  Iteration 3217/11040 Training loss: 1.3955 0.4604 sec/batch\n",
      "Epoch 12/40  Iteration 3218/11040 Training loss: 1.3955 0.4607 sec/batch\n",
      "Epoch 12/40  Iteration 3219/11040 Training loss: 1.3953 0.4737 sec/batch\n",
      "Epoch 12/40  Iteration 3220/11040 Training loss: 1.3955 0.4575 sec/batch\n",
      "Epoch 12/40  Iteration 3221/11040 Training loss: 1.3954 0.4613 sec/batch\n",
      "Epoch 12/40  Iteration 3222/11040 Training loss: 1.3955 0.4709 sec/batch\n",
      "Epoch 12/40  Iteration 3223/11040 Training loss: 1.3958 0.4730 sec/batch\n",
      "Epoch 12/40  Iteration 3224/11040 Training loss: 1.3958 0.4751 sec/batch\n",
      "Epoch 12/40  Iteration 3225/11040 Training loss: 1.3958 0.4753 sec/batch\n",
      "Epoch 12/40  Iteration 3226/11040 Training loss: 1.3957 0.4595 sec/batch\n",
      "Epoch 12/40  Iteration 3227/11040 Training loss: 1.3956 0.4690 sec/batch\n",
      "Epoch 12/40  Iteration 3228/11040 Training loss: 1.3956 0.4811 sec/batch\n",
      "Epoch 12/40  Iteration 3229/11040 Training loss: 1.3956 0.4719 sec/batch\n",
      "Epoch 12/40  Iteration 3230/11040 Training loss: 1.3956 0.4777 sec/batch\n",
      "Epoch 12/40  Iteration 3231/11040 Training loss: 1.3958 0.4737 sec/batch\n",
      "Epoch 12/40  Iteration 3232/11040 Training loss: 1.3958 0.4591 sec/batch\n",
      "Epoch 12/40  Iteration 3233/11040 Training loss: 1.3959 0.4588 sec/batch\n",
      "Epoch 12/40  Iteration 3234/11040 Training loss: 1.3959 0.4635 sec/batch\n",
      "Epoch 12/40  Iteration 3235/11040 Training loss: 1.3960 0.4859 sec/batch\n",
      "Epoch 12/40  Iteration 3236/11040 Training loss: 1.3961 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3237/11040 Training loss: 1.3962 0.4769 sec/batch\n",
      "Epoch 12/40  Iteration 3238/11040 Training loss: 1.3964 0.4721 sec/batch\n",
      "Epoch 12/40  Iteration 3239/11040 Training loss: 1.3965 0.4742 sec/batch\n",
      "Epoch 12/40  Iteration 3240/11040 Training loss: 1.3967 0.4816 sec/batch\n",
      "Epoch 12/40  Iteration 3241/11040 Training loss: 1.3966 0.4698 sec/batch\n",
      "Epoch 12/40  Iteration 3242/11040 Training loss: 1.3966 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3243/11040 Training loss: 1.3966 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3244/11040 Training loss: 1.3966 0.4787 sec/batch\n",
      "Epoch 12/40  Iteration 3245/11040 Training loss: 1.3966 0.4692 sec/batch\n",
      "Epoch 12/40  Iteration 3246/11040 Training loss: 1.3965 0.4606 sec/batch\n",
      "Epoch 12/40  Iteration 3247/11040 Training loss: 1.3965 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3248/11040 Training loss: 1.3965 0.4783 sec/batch\n",
      "Epoch 12/40  Iteration 3249/11040 Training loss: 1.3965 0.4744 sec/batch\n",
      "Epoch 12/40  Iteration 3250/11040 Training loss: 1.3965 0.4749 sec/batch\n",
      "Epoch 12/40  Iteration 3251/11040 Training loss: 1.3963 0.4719 sec/batch\n",
      "Epoch 12/40  Iteration 3252/11040 Training loss: 1.3963 0.4768 sec/batch\n",
      "Epoch 12/40  Iteration 3253/11040 Training loss: 1.3961 0.4722 sec/batch\n",
      "Epoch 12/40  Iteration 3254/11040 Training loss: 1.3960 0.4692 sec/batch\n",
      "Epoch 12/40  Iteration 3255/11040 Training loss: 1.3959 0.4820 sec/batch\n",
      "Epoch 12/40  Iteration 3256/11040 Training loss: 1.3958 0.4742 sec/batch\n",
      "Epoch 12/40  Iteration 3257/11040 Training loss: 1.3958 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3258/11040 Training loss: 1.3956 0.4741 sec/batch\n",
      "Epoch 12/40  Iteration 3259/11040 Training loss: 1.3955 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3260/11040 Training loss: 1.3956 0.4782 sec/batch\n",
      "Epoch 12/40  Iteration 3261/11040 Training loss: 1.3956 0.4723 sec/batch\n",
      "Epoch 12/40  Iteration 3262/11040 Training loss: 1.3956 0.4595 sec/batch\n",
      "Epoch 12/40  Iteration 3263/11040 Training loss: 1.3954 0.4752 sec/batch\n",
      "Epoch 12/40  Iteration 3264/11040 Training loss: 1.3953 0.4738 sec/batch\n",
      "Epoch 12/40  Iteration 3265/11040 Training loss: 1.3953 0.4750 sec/batch\n",
      "Epoch 12/40  Iteration 3266/11040 Training loss: 1.3954 0.4741 sec/batch\n",
      "Epoch 12/40  Iteration 3267/11040 Training loss: 1.3954 0.4760 sec/batch\n",
      "Epoch 12/40  Iteration 3268/11040 Training loss: 1.3954 0.4724 sec/batch\n",
      "Epoch 12/40  Iteration 3269/11040 Training loss: 1.3952 0.4737 sec/batch\n",
      "Epoch 12/40  Iteration 3270/11040 Training loss: 1.3951 0.4774 sec/batch\n",
      "Epoch 12/40  Iteration 3271/11040 Training loss: 1.3951 0.4731 sec/batch\n",
      "Epoch 12/40  Iteration 3272/11040 Training loss: 1.3951 0.4764 sec/batch\n",
      "Epoch 12/40  Iteration 3273/11040 Training loss: 1.3950 0.4731 sec/batch\n",
      "Epoch 12/40  Iteration 3274/11040 Training loss: 1.3950 0.4753 sec/batch\n",
      "Epoch 12/40  Iteration 3275/11040 Training loss: 1.3950 0.4764 sec/batch\n",
      "Epoch 12/40  Iteration 3276/11040 Training loss: 1.3950 0.4733 sec/batch\n",
      "Epoch 12/40  Iteration 3277/11040 Training loss: 1.3950 0.4590 sec/batch\n",
      "Epoch 12/40  Iteration 3278/11040 Training loss: 1.3950 0.4729 sec/batch\n",
      "Epoch 12/40  Iteration 3279/11040 Training loss: 1.3949 0.4761 sec/batch\n",
      "Epoch 12/40  Iteration 3280/11040 Training loss: 1.3947 0.4763 sec/batch\n",
      "Epoch 12/40  Iteration 3281/11040 Training loss: 1.3946 0.4740 sec/batch\n",
      "Epoch 12/40  Iteration 3282/11040 Training loss: 1.3946 0.4809 sec/batch\n",
      "Epoch 12/40  Iteration 3283/11040 Training loss: 1.3946 0.4684 sec/batch\n",
      "Epoch 12/40  Iteration 3284/11040 Training loss: 1.3945 0.4724 sec/batch\n",
      "Epoch 12/40  Iteration 3285/11040 Training loss: 1.3944 0.4771 sec/batch\n",
      "Epoch 12/40  Iteration 3286/11040 Training loss: 1.3944 0.4732 sec/batch\n",
      "Epoch 12/40  Iteration 3287/11040 Training loss: 1.3943 0.4596 sec/batch\n",
      "Epoch 12/40  Iteration 3288/11040 Training loss: 1.3943 0.4762 sec/batch\n",
      "Epoch 12/40  Iteration 3289/11040 Training loss: 1.3942 0.4745 sec/batch\n",
      "Epoch 12/40  Iteration 3290/11040 Training loss: 1.3943 0.4746 sec/batch\n",
      "Epoch 12/40  Iteration 3291/11040 Training loss: 1.3942 0.4731 sec/batch\n",
      "Epoch 12/40  Iteration 3292/11040 Training loss: 1.3942 0.4775 sec/batch\n",
      "Epoch 12/40  Iteration 3293/11040 Training loss: 1.3942 0.4720 sec/batch\n",
      "Epoch 12/40  Iteration 3294/11040 Training loss: 1.3942 0.4755 sec/batch\n",
      "Epoch 12/40  Iteration 3295/11040 Training loss: 1.3941 0.4619 sec/batch\n",
      "Epoch 12/40  Iteration 3296/11040 Training loss: 1.3941 0.4573 sec/batch\n",
      "Epoch 12/40  Iteration 3297/11040 Training loss: 1.3941 0.4739 sec/batch\n",
      "Epoch 12/40  Iteration 3298/11040 Training loss: 1.3941 0.4726 sec/batch\n",
      "Epoch 12/40  Iteration 3299/11040 Training loss: 1.3942 0.4761 sec/batch\n",
      "Epoch 12/40  Iteration 3300/11040 Training loss: 1.3942 0.4764 sec/batch\n",
      "Epoch 12/40  Iteration 3301/11040 Training loss: 1.3942 0.4582 sec/batch\n",
      "Epoch 12/40  Iteration 3302/11040 Training loss: 1.3942 0.4606 sec/batch\n",
      "Epoch 12/40  Iteration 3303/11040 Training loss: 1.3943 0.4721 sec/batch\n",
      "Epoch 12/40  Iteration 3304/11040 Training loss: 1.3943 0.4749 sec/batch\n",
      "Epoch 12/40  Iteration 3305/11040 Training loss: 1.3944 0.4758 sec/batch\n",
      "Epoch 12/40  Iteration 3306/11040 Training loss: 1.3944 0.4759 sec/batch\n",
      "Epoch 12/40  Iteration 3307/11040 Training loss: 1.3947 0.4586 sec/batch\n",
      "Epoch 12/40  Iteration 3308/11040 Training loss: 1.3949 0.4744 sec/batch\n",
      "Epoch 12/40  Iteration 3309/11040 Training loss: 1.3949 0.4772 sec/batch\n",
      "Epoch 12/40  Iteration 3310/11040 Training loss: 1.3949 0.4729 sec/batch\n",
      "Epoch 12/40  Iteration 3311/11040 Training loss: 1.3948 0.4607 sec/batch\n",
      "Epoch 12/40  Iteration 3312/11040 Training loss: 1.3948 0.4728 sec/batch\n",
      "Epoch 13/40  Iteration 3313/11040 Training loss: 1.4556 0.4764 sec/batch\n",
      "Epoch 13/40  Iteration 3314/11040 Training loss: 1.4160 0.4727 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40  Iteration 3315/11040 Training loss: 1.4108 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3316/11040 Training loss: 1.4098 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3317/11040 Training loss: 1.4101 0.4738 sec/batch\n",
      "Epoch 13/40  Iteration 3318/11040 Training loss: 1.4064 0.4605 sec/batch\n",
      "Epoch 13/40  Iteration 3319/11040 Training loss: 1.3984 0.4587 sec/batch\n",
      "Epoch 13/40  Iteration 3320/11040 Training loss: 1.3962 0.4594 sec/batch\n",
      "Epoch 13/40  Iteration 3321/11040 Training loss: 1.3939 0.4585 sec/batch\n",
      "Epoch 13/40  Iteration 3322/11040 Training loss: 1.3943 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3323/11040 Training loss: 1.3922 0.4759 sec/batch\n",
      "Epoch 13/40  Iteration 3324/11040 Training loss: 1.3918 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3325/11040 Training loss: 1.3898 0.4596 sec/batch\n",
      "Epoch 13/40  Iteration 3326/11040 Training loss: 1.3868 0.4582 sec/batch\n",
      "Epoch 13/40  Iteration 3327/11040 Training loss: 1.3870 0.4756 sec/batch\n",
      "Epoch 13/40  Iteration 3328/11040 Training loss: 1.3870 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3329/11040 Training loss: 1.3843 0.4596 sec/batch\n",
      "Epoch 13/40  Iteration 3330/11040 Training loss: 1.3828 0.4739 sec/batch\n",
      "Epoch 13/40  Iteration 3331/11040 Training loss: 1.3815 0.4748 sec/batch\n",
      "Epoch 13/40  Iteration 3332/11040 Training loss: 1.3817 0.4762 sec/batch\n",
      "Epoch 13/40  Iteration 3333/11040 Training loss: 1.3830 0.4587 sec/batch\n",
      "Epoch 13/40  Iteration 3334/11040 Training loss: 1.3840 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3335/11040 Training loss: 1.3838 0.4622 sec/batch\n",
      "Epoch 13/40  Iteration 3336/11040 Training loss: 1.3850 0.4716 sec/batch\n",
      "Epoch 13/40  Iteration 3337/11040 Training loss: 1.3842 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3338/11040 Training loss: 1.3834 0.4738 sec/batch\n",
      "Epoch 13/40  Iteration 3339/11040 Training loss: 1.3824 0.4761 sec/batch\n",
      "Epoch 13/40  Iteration 3340/11040 Training loss: 1.3820 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3341/11040 Training loss: 1.3819 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3342/11040 Training loss: 1.3821 0.4734 sec/batch\n",
      "Epoch 13/40  Iteration 3343/11040 Training loss: 1.3819 0.4739 sec/batch\n",
      "Epoch 13/40  Iteration 3344/11040 Training loss: 1.3816 0.4605 sec/batch\n",
      "Epoch 13/40  Iteration 3345/11040 Training loss: 1.3816 0.4730 sec/batch\n",
      "Epoch 13/40  Iteration 3346/11040 Training loss: 1.3811 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3347/11040 Training loss: 1.3811 0.4591 sec/batch\n",
      "Epoch 13/40  Iteration 3348/11040 Training loss: 1.3816 0.4759 sec/batch\n",
      "Epoch 13/40  Iteration 3349/11040 Training loss: 1.3815 0.4758 sec/batch\n",
      "Epoch 13/40  Iteration 3350/11040 Training loss: 1.3803 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3351/11040 Training loss: 1.3796 0.4761 sec/batch\n",
      "Epoch 13/40  Iteration 3352/11040 Training loss: 1.3798 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3353/11040 Training loss: 1.3793 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3354/11040 Training loss: 1.3781 0.4745 sec/batch\n",
      "Epoch 13/40  Iteration 3355/11040 Training loss: 1.3784 0.4752 sec/batch\n",
      "Epoch 13/40  Iteration 3356/11040 Training loss: 1.3780 0.4742 sec/batch\n",
      "Epoch 13/40  Iteration 3357/11040 Training loss: 1.3779 0.4587 sec/batch\n",
      "Epoch 13/40  Iteration 3358/11040 Training loss: 1.3776 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3359/11040 Training loss: 1.3770 0.4754 sec/batch\n",
      "Epoch 13/40  Iteration 3360/11040 Training loss: 1.3767 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3361/11040 Training loss: 1.3759 0.4728 sec/batch\n",
      "Epoch 13/40  Iteration 3362/11040 Training loss: 1.3764 0.4603 sec/batch\n",
      "Epoch 13/40  Iteration 3363/11040 Training loss: 1.3762 0.4601 sec/batch\n",
      "Epoch 13/40  Iteration 3364/11040 Training loss: 1.3756 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3365/11040 Training loss: 1.3760 0.4730 sec/batch\n",
      "Epoch 13/40  Iteration 3366/11040 Training loss: 1.3756 0.4596 sec/batch\n",
      "Epoch 13/40  Iteration 3367/11040 Training loss: 1.3758 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3368/11040 Training loss: 1.3759 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3369/11040 Training loss: 1.3757 0.4752 sec/batch\n",
      "Epoch 13/40  Iteration 3370/11040 Training loss: 1.3759 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3371/11040 Training loss: 1.3762 0.4605 sec/batch\n",
      "Epoch 13/40  Iteration 3372/11040 Training loss: 1.3759 0.4592 sec/batch\n",
      "Epoch 13/40  Iteration 3373/11040 Training loss: 1.3759 0.4585 sec/batch\n",
      "Epoch 13/40  Iteration 3374/11040 Training loss: 1.3755 0.4593 sec/batch\n",
      "Epoch 13/40  Iteration 3375/11040 Training loss: 1.3755 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3376/11040 Training loss: 1.3755 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3377/11040 Training loss: 1.3757 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3378/11040 Training loss: 1.3754 0.4759 sec/batch\n",
      "Epoch 13/40  Iteration 3379/11040 Training loss: 1.3752 0.4574 sec/batch\n",
      "Epoch 13/40  Iteration 3380/11040 Training loss: 1.3753 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3381/11040 Training loss: 1.3751 0.4742 sec/batch\n",
      "Epoch 13/40  Iteration 3382/11040 Training loss: 1.3753 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3383/11040 Training loss: 1.3749 0.4604 sec/batch\n",
      "Epoch 13/40  Iteration 3384/11040 Training loss: 1.3744 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3385/11040 Training loss: 1.3739 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3386/11040 Training loss: 1.3736 0.4762 sec/batch\n",
      "Epoch 13/40  Iteration 3387/11040 Training loss: 1.3735 0.4735 sec/batch\n",
      "Epoch 13/40  Iteration 3388/11040 Training loss: 1.3737 0.4603 sec/batch\n",
      "Epoch 13/40  Iteration 3389/11040 Training loss: 1.3733 0.4588 sec/batch\n",
      "Epoch 13/40  Iteration 3390/11040 Training loss: 1.3731 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3391/11040 Training loss: 1.3728 0.4613 sec/batch\n",
      "Epoch 13/40  Iteration 3392/11040 Training loss: 1.3727 0.4576 sec/batch\n",
      "Epoch 13/40  Iteration 3393/11040 Training loss: 1.3724 0.4761 sec/batch\n",
      "Epoch 13/40  Iteration 3394/11040 Training loss: 1.3723 0.4599 sec/batch\n",
      "Epoch 13/40  Iteration 3395/11040 Training loss: 1.3721 0.4732 sec/batch\n",
      "Epoch 13/40  Iteration 3396/11040 Training loss: 1.3718 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3397/11040 Training loss: 1.3717 0.4757 sec/batch\n",
      "Epoch 13/40  Iteration 3398/11040 Training loss: 1.3716 0.4593 sec/batch\n",
      "Epoch 13/40  Iteration 3399/11040 Training loss: 1.3710 0.4605 sec/batch\n",
      "Epoch 13/40  Iteration 3400/11040 Training loss: 1.3702 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3401/11040 Training loss: 1.3703 0.4574 sec/batch\n",
      "Epoch 13/40  Iteration 3402/11040 Training loss: 1.3704 0.4614 sec/batch\n",
      "Epoch 13/40  Iteration 3403/11040 Training loss: 1.3708 0.4572 sec/batch\n",
      "Epoch 13/40  Iteration 3404/11040 Training loss: 1.3708 0.4585 sec/batch\n",
      "Epoch 13/40  Iteration 3405/11040 Training loss: 1.3706 0.4606 sec/batch\n",
      "Epoch 13/40  Iteration 3406/11040 Training loss: 1.3709 0.4733 sec/batch\n",
      "Epoch 13/40  Iteration 3407/11040 Training loss: 1.3713 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3408/11040 Training loss: 1.3717 0.4739 sec/batch\n",
      "Epoch 13/40  Iteration 3409/11040 Training loss: 1.3721 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3410/11040 Training loss: 1.3725 0.4748 sec/batch\n",
      "Epoch 13/40  Iteration 3411/11040 Training loss: 1.3725 0.4595 sec/batch\n",
      "Epoch 13/40  Iteration 3412/11040 Training loss: 1.3724 0.4585 sec/batch\n",
      "Epoch 13/40  Iteration 3413/11040 Training loss: 1.3726 0.4596 sec/batch\n",
      "Epoch 13/40  Iteration 3414/11040 Training loss: 1.3727 0.4582 sec/batch\n",
      "Epoch 13/40  Iteration 3415/11040 Training loss: 1.3721 0.4607 sec/batch\n",
      "Epoch 13/40  Iteration 3416/11040 Training loss: 1.3723 0.4716 sec/batch\n",
      "Epoch 13/40  Iteration 3417/11040 Training loss: 1.3720 0.4762 sec/batch\n",
      "Epoch 13/40  Iteration 3418/11040 Training loss: 1.3720 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3419/11040 Training loss: 1.3722 0.4822 sec/batch\n",
      "Epoch 13/40  Iteration 3420/11040 Training loss: 1.3725 0.4820 sec/batch\n",
      "Epoch 13/40  Iteration 3421/11040 Training loss: 1.3728 0.4752 sec/batch\n",
      "Epoch 13/40  Iteration 3422/11040 Training loss: 1.3729 0.4770 sec/batch\n",
      "Epoch 13/40  Iteration 3423/11040 Training loss: 1.3729 0.4746 sec/batch\n",
      "Epoch 13/40  Iteration 3424/11040 Training loss: 1.3732 0.4729 sec/batch\n",
      "Epoch 13/40  Iteration 3425/11040 Training loss: 1.3734 0.4604 sec/batch\n",
      "Epoch 13/40  Iteration 3426/11040 Training loss: 1.3736 0.4576 sec/batch\n",
      "Epoch 13/40  Iteration 3427/11040 Training loss: 1.3742 0.4606 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40  Iteration 3428/11040 Training loss: 1.3747 0.4730 sec/batch\n",
      "Epoch 13/40  Iteration 3429/11040 Training loss: 1.3746 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3430/11040 Training loss: 1.3750 0.4762 sec/batch\n",
      "Epoch 13/40  Iteration 3431/11040 Training loss: 1.3752 0.4730 sec/batch\n",
      "Epoch 13/40  Iteration 3432/11040 Training loss: 1.3754 0.4742 sec/batch\n",
      "Epoch 13/40  Iteration 3433/11040 Training loss: 1.3754 0.4782 sec/batch\n",
      "Epoch 13/40  Iteration 3434/11040 Training loss: 1.3754 0.4736 sec/batch\n",
      "Epoch 13/40  Iteration 3435/11040 Training loss: 1.3758 0.4731 sec/batch\n",
      "Epoch 13/40  Iteration 3436/11040 Training loss: 1.3762 0.4738 sec/batch\n",
      "Epoch 13/40  Iteration 3437/11040 Training loss: 1.3762 0.4755 sec/batch\n",
      "Epoch 13/40  Iteration 3438/11040 Training loss: 1.3765 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3439/11040 Training loss: 1.3765 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3440/11040 Training loss: 1.3765 0.4734 sec/batch\n",
      "Epoch 13/40  Iteration 3441/11040 Training loss: 1.3764 0.4635 sec/batch\n",
      "Epoch 13/40  Iteration 3442/11040 Training loss: 1.3766 0.4701 sec/batch\n",
      "Epoch 13/40  Iteration 3443/11040 Training loss: 1.3765 0.4604 sec/batch\n",
      "Epoch 13/40  Iteration 3444/11040 Training loss: 1.3762 0.4594 sec/batch\n",
      "Epoch 13/40  Iteration 3445/11040 Training loss: 1.3763 0.4591 sec/batch\n",
      "Epoch 13/40  Iteration 3446/11040 Training loss: 1.3762 0.4595 sec/batch\n",
      "Epoch 13/40  Iteration 3447/11040 Training loss: 1.3764 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3448/11040 Training loss: 1.3766 0.4571 sec/batch\n",
      "Epoch 13/40  Iteration 3449/11040 Training loss: 1.3765 0.4715 sec/batch\n",
      "Epoch 13/40  Iteration 3450/11040 Training loss: 1.3768 0.4770 sec/batch\n",
      "Epoch 13/40  Iteration 3451/11040 Training loss: 1.3769 0.4757 sec/batch\n",
      "Epoch 13/40  Iteration 3452/11040 Training loss: 1.3770 0.4584 sec/batch\n",
      "Epoch 13/40  Iteration 3453/11040 Training loss: 1.3770 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3454/11040 Training loss: 1.3770 0.4754 sec/batch\n",
      "Epoch 13/40  Iteration 3455/11040 Training loss: 1.3769 0.4731 sec/batch\n",
      "Epoch 13/40  Iteration 3456/11040 Training loss: 1.3768 0.4762 sec/batch\n",
      "Epoch 13/40  Iteration 3457/11040 Training loss: 1.3768 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3458/11040 Training loss: 1.3768 0.4760 sec/batch\n",
      "Epoch 13/40  Iteration 3459/11040 Training loss: 1.3767 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3460/11040 Training loss: 1.3765 0.4729 sec/batch\n",
      "Epoch 13/40  Iteration 3461/11040 Training loss: 1.3763 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3462/11040 Training loss: 1.3763 0.4790 sec/batch\n",
      "Epoch 13/40  Iteration 3463/11040 Training loss: 1.3762 0.4693 sec/batch\n",
      "Epoch 13/40  Iteration 3464/11040 Training loss: 1.3763 0.4603 sec/batch\n",
      "Epoch 13/40  Iteration 3465/11040 Training loss: 1.3762 0.4675 sec/batch\n",
      "Epoch 13/40  Iteration 3466/11040 Training loss: 1.3763 0.4672 sec/batch\n",
      "Epoch 13/40  Iteration 3467/11040 Training loss: 1.3763 0.4745 sec/batch\n",
      "Epoch 13/40  Iteration 3468/11040 Training loss: 1.3762 0.4731 sec/batch\n",
      "Epoch 13/40  Iteration 3469/11040 Training loss: 1.3760 0.4605 sec/batch\n",
      "Epoch 13/40  Iteration 3470/11040 Training loss: 1.3761 0.4748 sec/batch\n",
      "Epoch 13/40  Iteration 3471/11040 Training loss: 1.3763 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3472/11040 Training loss: 1.3763 0.4768 sec/batch\n",
      "Epoch 13/40  Iteration 3473/11040 Training loss: 1.3765 0.4789 sec/batch\n",
      "Epoch 13/40  Iteration 3474/11040 Training loss: 1.3764 0.4695 sec/batch\n",
      "Epoch 13/40  Iteration 3475/11040 Training loss: 1.3765 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3476/11040 Training loss: 1.3766 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3477/11040 Training loss: 1.3766 0.4595 sec/batch\n",
      "Epoch 13/40  Iteration 3478/11040 Training loss: 1.3769 0.4602 sec/batch\n",
      "Epoch 13/40  Iteration 3479/11040 Training loss: 1.3769 0.4730 sec/batch\n",
      "Epoch 13/40  Iteration 3480/11040 Training loss: 1.3768 0.4606 sec/batch\n",
      "Epoch 13/40  Iteration 3481/11040 Training loss: 1.3768 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3482/11040 Training loss: 1.3767 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3483/11040 Training loss: 1.3770 0.4607 sec/batch\n",
      "Epoch 13/40  Iteration 3484/11040 Training loss: 1.3770 0.4572 sec/batch\n",
      "Epoch 13/40  Iteration 3485/11040 Training loss: 1.3770 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3486/11040 Training loss: 1.3769 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3487/11040 Training loss: 1.3770 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3488/11040 Training loss: 1.3770 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3489/11040 Training loss: 1.3771 0.4595 sec/batch\n",
      "Epoch 13/40  Iteration 3490/11040 Training loss: 1.3770 0.4758 sec/batch\n",
      "Epoch 13/40  Iteration 3491/11040 Training loss: 1.3772 0.4746 sec/batch\n",
      "Epoch 13/40  Iteration 3492/11040 Training loss: 1.3773 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3493/11040 Training loss: 1.3773 0.4721 sec/batch\n",
      "Epoch 13/40  Iteration 3494/11040 Training loss: 1.3773 0.4641 sec/batch\n",
      "Epoch 13/40  Iteration 3495/11040 Training loss: 1.3771 0.4714 sec/batch\n",
      "Epoch 13/40  Iteration 3496/11040 Training loss: 1.3772 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3497/11040 Training loss: 1.3772 0.4574 sec/batch\n",
      "Epoch 13/40  Iteration 3498/11040 Training loss: 1.3772 0.4746 sec/batch\n",
      "Epoch 13/40  Iteration 3499/11040 Training loss: 1.3775 0.4602 sec/batch\n",
      "Epoch 13/40  Iteration 3500/11040 Training loss: 1.3776 0.4731 sec/batch\n",
      "Validation loss: 1.33324 Saving checkpoint!\n",
      "Epoch 13/40  Iteration 3501/11040 Training loss: 1.3781 0.4687 sec/batch\n",
      "Epoch 13/40  Iteration 3502/11040 Training loss: 1.3781 0.4823 sec/batch\n",
      "Epoch 13/40  Iteration 3503/11040 Training loss: 1.3780 0.4720 sec/batch\n",
      "Epoch 13/40  Iteration 3504/11040 Training loss: 1.3780 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3505/11040 Training loss: 1.3779 0.4723 sec/batch\n",
      "Epoch 13/40  Iteration 3506/11040 Training loss: 1.3780 0.4626 sec/batch\n",
      "Epoch 13/40  Iteration 3507/11040 Training loss: 1.3783 0.4606 sec/batch\n",
      "Epoch 13/40  Iteration 3508/11040 Training loss: 1.3782 0.4733 sec/batch\n",
      "Epoch 13/40  Iteration 3509/11040 Training loss: 1.3783 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3510/11040 Training loss: 1.3784 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3511/11040 Training loss: 1.3785 0.4742 sec/batch\n",
      "Epoch 13/40  Iteration 3512/11040 Training loss: 1.3786 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3513/11040 Training loss: 1.3788 0.4752 sec/batch\n",
      "Epoch 13/40  Iteration 3514/11040 Training loss: 1.3789 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3515/11040 Training loss: 1.3790 0.4755 sec/batch\n",
      "Epoch 13/40  Iteration 3516/11040 Training loss: 1.3792 0.4729 sec/batch\n",
      "Epoch 13/40  Iteration 3517/11040 Training loss: 1.3792 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3518/11040 Training loss: 1.3791 0.4755 sec/batch\n",
      "Epoch 13/40  Iteration 3519/11040 Training loss: 1.3791 0.4590 sec/batch\n",
      "Epoch 13/40  Iteration 3520/11040 Training loss: 1.3791 0.4585 sec/batch\n",
      "Epoch 13/40  Iteration 3521/11040 Training loss: 1.3790 0.4624 sec/batch\n",
      "Epoch 13/40  Iteration 3522/11040 Training loss: 1.3790 0.4706 sec/batch\n",
      "Epoch 13/40  Iteration 3523/11040 Training loss: 1.3789 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3524/11040 Training loss: 1.3789 0.4745 sec/batch\n",
      "Epoch 13/40  Iteration 3525/11040 Training loss: 1.3789 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3526/11040 Training loss: 1.3789 0.4746 sec/batch\n",
      "Epoch 13/40  Iteration 3527/11040 Training loss: 1.3788 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3528/11040 Training loss: 1.3787 0.4769 sec/batch\n",
      "Epoch 13/40  Iteration 3529/11040 Training loss: 1.3786 0.4738 sec/batch\n",
      "Epoch 13/40  Iteration 3530/11040 Training loss: 1.3784 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3531/11040 Training loss: 1.3783 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3532/11040 Training loss: 1.3783 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3533/11040 Training loss: 1.3782 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3534/11040 Training loss: 1.3780 0.4595 sec/batch\n",
      "Epoch 13/40  Iteration 3535/11040 Training loss: 1.3779 0.4598 sec/batch\n",
      "Epoch 13/40  Iteration 3536/11040 Training loss: 1.3780 0.4746 sec/batch\n",
      "Epoch 13/40  Iteration 3537/11040 Training loss: 1.3779 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3538/11040 Training loss: 1.3779 0.4732 sec/batch\n",
      "Epoch 13/40  Iteration 3539/11040 Training loss: 1.3778 0.4606 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40  Iteration 3540/11040 Training loss: 1.3776 0.4584 sec/batch\n",
      "Epoch 13/40  Iteration 3541/11040 Training loss: 1.3777 0.4750 sec/batch\n",
      "Epoch 13/40  Iteration 3542/11040 Training loss: 1.3778 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3543/11040 Training loss: 1.3778 0.4756 sec/batch\n",
      "Epoch 13/40  Iteration 3544/11040 Training loss: 1.3778 0.4744 sec/batch\n",
      "Epoch 13/40  Iteration 3545/11040 Training loss: 1.3777 0.4732 sec/batch\n",
      "Epoch 13/40  Iteration 3546/11040 Training loss: 1.3775 0.4771 sec/batch\n",
      "Epoch 13/40  Iteration 3547/11040 Training loss: 1.3776 0.4748 sec/batch\n",
      "Epoch 13/40  Iteration 3548/11040 Training loss: 1.3776 0.4734 sec/batch\n",
      "Epoch 13/40  Iteration 3549/11040 Training loss: 1.3775 0.4740 sec/batch\n",
      "Epoch 13/40  Iteration 3550/11040 Training loss: 1.3775 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3551/11040 Training loss: 1.3775 0.4596 sec/batch\n",
      "Epoch 13/40  Iteration 3552/11040 Training loss: 1.3776 0.4741 sec/batch\n",
      "Epoch 13/40  Iteration 3553/11040 Training loss: 1.3776 0.4759 sec/batch\n",
      "Epoch 13/40  Iteration 3554/11040 Training loss: 1.3775 0.4732 sec/batch\n",
      "Epoch 13/40  Iteration 3555/11040 Training loss: 1.3774 0.4758 sec/batch\n",
      "Epoch 13/40  Iteration 3556/11040 Training loss: 1.3773 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3557/11040 Training loss: 1.3771 0.4745 sec/batch\n",
      "Epoch 13/40  Iteration 3558/11040 Training loss: 1.3771 0.4746 sec/batch\n",
      "Epoch 13/40  Iteration 3559/11040 Training loss: 1.3770 0.4739 sec/batch\n",
      "Epoch 13/40  Iteration 3560/11040 Training loss: 1.3770 0.4753 sec/batch\n",
      "Epoch 13/40  Iteration 3561/11040 Training loss: 1.3769 0.4731 sec/batch\n",
      "Epoch 13/40  Iteration 3562/11040 Training loss: 1.3769 0.4604 sec/batch\n",
      "Epoch 13/40  Iteration 3563/11040 Training loss: 1.3768 0.4593 sec/batch\n",
      "Epoch 13/40  Iteration 3564/11040 Training loss: 1.3767 0.4742 sec/batch\n",
      "Epoch 13/40  Iteration 3565/11040 Training loss: 1.3766 0.4752 sec/batch\n",
      "Epoch 13/40  Iteration 3566/11040 Training loss: 1.3767 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3567/11040 Training loss: 1.3766 0.4732 sec/batch\n",
      "Epoch 13/40  Iteration 3568/11040 Training loss: 1.3766 0.4743 sec/batch\n",
      "Epoch 13/40  Iteration 3569/11040 Training loss: 1.3766 0.4772 sec/batch\n",
      "Epoch 13/40  Iteration 3570/11040 Training loss: 1.3766 0.4760 sec/batch\n",
      "Epoch 13/40  Iteration 3571/11040 Training loss: 1.3765 0.4732 sec/batch\n",
      "Epoch 13/40  Iteration 3572/11040 Training loss: 1.3764 0.4747 sec/batch\n",
      "Epoch 13/40  Iteration 3573/11040 Training loss: 1.3764 0.4593 sec/batch\n",
      "Epoch 13/40  Iteration 3574/11040 Training loss: 1.3765 0.4600 sec/batch\n",
      "Epoch 13/40  Iteration 3575/11040 Training loss: 1.3765 0.4603 sec/batch\n",
      "Epoch 13/40  Iteration 3576/11040 Training loss: 1.3766 0.4635 sec/batch\n",
      "Epoch 13/40  Iteration 3577/11040 Training loss: 1.3765 0.4830 sec/batch\n",
      "Epoch 13/40  Iteration 3578/11040 Training loss: 1.3766 0.4606 sec/batch\n",
      "Epoch 13/40  Iteration 3579/11040 Training loss: 1.3767 0.4600 sec/batch\n",
      "Epoch 13/40  Iteration 3580/11040 Training loss: 1.3768 0.4733 sec/batch\n",
      "Epoch 13/40  Iteration 3581/11040 Training loss: 1.3768 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3582/11040 Training loss: 1.3769 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3583/11040 Training loss: 1.3772 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3584/11040 Training loss: 1.3774 0.4605 sec/batch\n",
      "Epoch 13/40  Iteration 3585/11040 Training loss: 1.3774 0.4731 sec/batch\n",
      "Epoch 13/40  Iteration 3586/11040 Training loss: 1.3775 0.4749 sec/batch\n",
      "Epoch 13/40  Iteration 3587/11040 Training loss: 1.3774 0.4751 sec/batch\n",
      "Epoch 13/40  Iteration 3588/11040 Training loss: 1.3774 0.4734 sec/batch\n",
      "Epoch 14/40  Iteration 3589/11040 Training loss: 1.4416 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3590/11040 Training loss: 1.4081 0.4759 sec/batch\n",
      "Epoch 14/40  Iteration 3591/11040 Training loss: 1.4009 0.4745 sec/batch\n",
      "Epoch 14/40  Iteration 3592/11040 Training loss: 1.3993 0.4723 sec/batch\n",
      "Epoch 14/40  Iteration 3593/11040 Training loss: 1.3974 0.4609 sec/batch\n",
      "Epoch 14/40  Iteration 3594/11040 Training loss: 1.3934 0.4590 sec/batch\n",
      "Epoch 14/40  Iteration 3595/11040 Training loss: 1.3848 0.4596 sec/batch\n",
      "Epoch 14/40  Iteration 3596/11040 Training loss: 1.3838 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3597/11040 Training loss: 1.3800 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3598/11040 Training loss: 1.3802 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3599/11040 Training loss: 1.3779 0.4758 sec/batch\n",
      "Epoch 14/40  Iteration 3600/11040 Training loss: 1.3776 0.4755 sec/batch\n",
      "Epoch 14/40  Iteration 3601/11040 Training loss: 1.3752 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3602/11040 Training loss: 1.3724 0.4596 sec/batch\n",
      "Epoch 14/40  Iteration 3603/11040 Training loss: 1.3719 0.4585 sec/batch\n",
      "Epoch 14/40  Iteration 3604/11040 Training loss: 1.3723 0.4756 sec/batch\n",
      "Epoch 14/40  Iteration 3605/11040 Training loss: 1.3698 0.4731 sec/batch\n",
      "Epoch 14/40  Iteration 3606/11040 Training loss: 1.3680 0.4761 sec/batch\n",
      "Epoch 14/40  Iteration 3607/11040 Training loss: 1.3668 0.4744 sec/batch\n",
      "Epoch 14/40  Iteration 3608/11040 Training loss: 1.3671 0.4604 sec/batch\n",
      "Epoch 14/40  Iteration 3609/11040 Training loss: 1.3691 0.4739 sec/batch\n",
      "Epoch 14/40  Iteration 3610/11040 Training loss: 1.3697 0.4597 sec/batch\n",
      "Epoch 14/40  Iteration 3611/11040 Training loss: 1.3698 0.4732 sec/batch\n",
      "Epoch 14/40  Iteration 3612/11040 Training loss: 1.3703 0.4617 sec/batch\n",
      "Epoch 14/40  Iteration 3613/11040 Training loss: 1.3691 0.4735 sec/batch\n",
      "Epoch 14/40  Iteration 3614/11040 Training loss: 1.3683 0.4716 sec/batch\n",
      "Epoch 14/40  Iteration 3615/11040 Training loss: 1.3678 0.4772 sec/batch\n",
      "Epoch 14/40  Iteration 3616/11040 Training loss: 1.3676 0.4736 sec/batch\n",
      "Epoch 14/40  Iteration 3617/11040 Training loss: 1.3680 0.4723 sec/batch\n",
      "Epoch 14/40  Iteration 3618/11040 Training loss: 1.3681 0.4815 sec/batch\n",
      "Epoch 14/40  Iteration 3619/11040 Training loss: 1.3681 0.4725 sec/batch\n",
      "Epoch 14/40  Iteration 3620/11040 Training loss: 1.3677 0.4592 sec/batch\n",
      "Epoch 14/40  Iteration 3621/11040 Training loss: 1.3674 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3622/11040 Training loss: 1.3670 0.4620 sec/batch\n",
      "Epoch 14/40  Iteration 3623/11040 Training loss: 1.3669 0.4734 sec/batch\n",
      "Epoch 14/40  Iteration 3624/11040 Training loss: 1.3673 0.4744 sec/batch\n",
      "Epoch 14/40  Iteration 3625/11040 Training loss: 1.3673 0.4597 sec/batch\n",
      "Epoch 14/40  Iteration 3626/11040 Training loss: 1.3664 0.4585 sec/batch\n",
      "Epoch 14/40  Iteration 3627/11040 Training loss: 1.3657 0.4745 sec/batch\n",
      "Epoch 14/40  Iteration 3628/11040 Training loss: 1.3658 0.4760 sec/batch\n",
      "Epoch 14/40  Iteration 3629/11040 Training loss: 1.3653 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3630/11040 Training loss: 1.3642 0.4762 sec/batch\n",
      "Epoch 14/40  Iteration 3631/11040 Training loss: 1.3646 0.4751 sec/batch\n",
      "Epoch 14/40  Iteration 3632/11040 Training loss: 1.3643 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3633/11040 Training loss: 1.3640 0.4778 sec/batch\n",
      "Epoch 14/40  Iteration 3634/11040 Training loss: 1.3635 0.4714 sec/batch\n",
      "Epoch 14/40  Iteration 3635/11040 Training loss: 1.3628 0.4612 sec/batch\n",
      "Epoch 14/40  Iteration 3636/11040 Training loss: 1.3624 0.4734 sec/batch\n",
      "Epoch 14/40  Iteration 3637/11040 Training loss: 1.3617 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3638/11040 Training loss: 1.3621 0.4720 sec/batch\n",
      "Epoch 14/40  Iteration 3639/11040 Training loss: 1.3620 0.4765 sec/batch\n",
      "Epoch 14/40  Iteration 3640/11040 Training loss: 1.3615 0.4870 sec/batch\n",
      "Epoch 14/40  Iteration 3641/11040 Training loss: 1.3617 0.4623 sec/batch\n",
      "Epoch 14/40  Iteration 3642/11040 Training loss: 1.3613 0.4735 sec/batch\n",
      "Epoch 14/40  Iteration 3643/11040 Training loss: 1.3612 0.4731 sec/batch\n",
      "Epoch 14/40  Iteration 3644/11040 Training loss: 1.3611 0.4778 sec/batch\n",
      "Epoch 14/40  Iteration 3645/11040 Training loss: 1.3609 0.4594 sec/batch\n",
      "Epoch 14/40  Iteration 3646/11040 Training loss: 1.3607 0.4586 sec/batch\n",
      "Epoch 14/40  Iteration 3647/11040 Training loss: 1.3611 0.4582 sec/batch\n",
      "Epoch 14/40  Iteration 3648/11040 Training loss: 1.3608 0.4748 sec/batch\n",
      "Epoch 14/40  Iteration 3649/11040 Training loss: 1.3607 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3650/11040 Training loss: 1.3603 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3651/11040 Training loss: 1.3602 0.4622 sec/batch\n",
      "Epoch 14/40  Iteration 3652/11040 Training loss: 1.3600 0.4725 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40  Iteration 3653/11040 Training loss: 1.3600 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3654/11040 Training loss: 1.3596 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3655/11040 Training loss: 1.3594 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3656/11040 Training loss: 1.3594 0.4772 sec/batch\n",
      "Epoch 14/40  Iteration 3657/11040 Training loss: 1.3591 0.4720 sec/batch\n",
      "Epoch 14/40  Iteration 3658/11040 Training loss: 1.3591 0.4615 sec/batch\n",
      "Epoch 14/40  Iteration 3659/11040 Training loss: 1.3587 0.4593 sec/batch\n",
      "Epoch 14/40  Iteration 3660/11040 Training loss: 1.3582 0.4586 sec/batch\n",
      "Epoch 14/40  Iteration 3661/11040 Training loss: 1.3576 0.4576 sec/batch\n",
      "Epoch 14/40  Iteration 3662/11040 Training loss: 1.3573 0.4743 sec/batch\n",
      "Epoch 14/40  Iteration 3663/11040 Training loss: 1.3572 0.4595 sec/batch\n",
      "Epoch 14/40  Iteration 3664/11040 Training loss: 1.3574 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3665/11040 Training loss: 1.3571 0.4760 sec/batch\n",
      "Epoch 14/40  Iteration 3666/11040 Training loss: 1.3569 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3667/11040 Training loss: 1.3566 0.4691 sec/batch\n",
      "Epoch 14/40  Iteration 3668/11040 Training loss: 1.3564 0.4798 sec/batch\n",
      "Epoch 14/40  Iteration 3669/11040 Training loss: 1.3561 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3670/11040 Training loss: 1.3561 0.4739 sec/batch\n",
      "Epoch 14/40  Iteration 3671/11040 Training loss: 1.3559 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3672/11040 Training loss: 1.3557 0.4739 sec/batch\n",
      "Epoch 14/40  Iteration 3673/11040 Training loss: 1.3555 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3674/11040 Training loss: 1.3554 0.4764 sec/batch\n",
      "Epoch 14/40  Iteration 3675/11040 Training loss: 1.3550 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3676/11040 Training loss: 1.3541 0.4732 sec/batch\n",
      "Epoch 14/40  Iteration 3677/11040 Training loss: 1.3542 0.4615 sec/batch\n",
      "Epoch 14/40  Iteration 3678/11040 Training loss: 1.3542 0.4725 sec/batch\n",
      "Epoch 14/40  Iteration 3679/11040 Training loss: 1.3546 0.4770 sec/batch\n",
      "Epoch 14/40  Iteration 3680/11040 Training loss: 1.3546 0.4653 sec/batch\n",
      "Epoch 14/40  Iteration 3681/11040 Training loss: 1.3545 0.4666 sec/batch\n",
      "Epoch 14/40  Iteration 3682/11040 Training loss: 1.3549 0.4745 sec/batch\n",
      "Epoch 14/40  Iteration 3683/11040 Training loss: 1.3555 0.4765 sec/batch\n",
      "Epoch 14/40  Iteration 3684/11040 Training loss: 1.3557 0.4733 sec/batch\n",
      "Epoch 14/40  Iteration 3685/11040 Training loss: 1.3561 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3686/11040 Training loss: 1.3565 0.4613 sec/batch\n",
      "Epoch 14/40  Iteration 3687/11040 Training loss: 1.3566 0.4583 sec/batch\n",
      "Epoch 14/40  Iteration 3688/11040 Training loss: 1.3564 0.4602 sec/batch\n",
      "Epoch 14/40  Iteration 3689/11040 Training loss: 1.3565 0.4598 sec/batch\n",
      "Epoch 14/40  Iteration 3690/11040 Training loss: 1.3565 0.4569 sec/batch\n",
      "Epoch 14/40  Iteration 3691/11040 Training loss: 1.3560 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3692/11040 Training loss: 1.3562 0.4767 sec/batch\n",
      "Epoch 14/40  Iteration 3693/11040 Training loss: 1.3560 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3694/11040 Training loss: 1.3559 0.4744 sec/batch\n",
      "Epoch 14/40  Iteration 3695/11040 Training loss: 1.3562 0.4763 sec/batch\n",
      "Epoch 14/40  Iteration 3696/11040 Training loss: 1.3564 0.4720 sec/batch\n",
      "Epoch 14/40  Iteration 3697/11040 Training loss: 1.3567 0.4759 sec/batch\n",
      "Epoch 14/40  Iteration 3698/11040 Training loss: 1.3568 0.4731 sec/batch\n",
      "Epoch 14/40  Iteration 3699/11040 Training loss: 1.3568 0.4599 sec/batch\n",
      "Epoch 14/40  Iteration 3700/11040 Training loss: 1.3571 0.4591 sec/batch\n",
      "Epoch 14/40  Iteration 3701/11040 Training loss: 1.3572 0.4606 sec/batch\n",
      "Epoch 14/40  Iteration 3702/11040 Training loss: 1.3573 0.4587 sec/batch\n",
      "Epoch 14/40  Iteration 3703/11040 Training loss: 1.3577 0.4729 sec/batch\n",
      "Epoch 14/40  Iteration 3704/11040 Training loss: 1.3581 0.4772 sec/batch\n",
      "Epoch 14/40  Iteration 3705/11040 Training loss: 1.3581 0.4758 sec/batch\n",
      "Epoch 14/40  Iteration 3706/11040 Training loss: 1.3585 0.4683 sec/batch\n",
      "Epoch 14/40  Iteration 3707/11040 Training loss: 1.3587 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3708/11040 Training loss: 1.3588 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3709/11040 Training loss: 1.3589 0.4763 sec/batch\n",
      "Epoch 14/40  Iteration 3710/11040 Training loss: 1.3589 0.4730 sec/batch\n",
      "Epoch 14/40  Iteration 3711/11040 Training loss: 1.3592 0.4608 sec/batch\n",
      "Epoch 14/40  Iteration 3712/11040 Training loss: 1.3597 0.4592 sec/batch\n",
      "Epoch 14/40  Iteration 3713/11040 Training loss: 1.3597 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3714/11040 Training loss: 1.3600 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3715/11040 Training loss: 1.3600 0.4620 sec/batch\n",
      "Epoch 14/40  Iteration 3716/11040 Training loss: 1.3600 0.4574 sec/batch\n",
      "Epoch 14/40  Iteration 3717/11040 Training loss: 1.3597 0.4759 sec/batch\n",
      "Epoch 14/40  Iteration 3718/11040 Training loss: 1.3599 0.4739 sec/batch\n",
      "Epoch 14/40  Iteration 3719/11040 Training loss: 1.3597 0.4760 sec/batch\n",
      "Epoch 14/40  Iteration 3720/11040 Training loss: 1.3594 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3721/11040 Training loss: 1.3595 0.4587 sec/batch\n",
      "Epoch 14/40  Iteration 3722/11040 Training loss: 1.3594 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3723/11040 Training loss: 1.3596 0.4597 sec/batch\n",
      "Epoch 14/40  Iteration 3724/11040 Training loss: 1.3597 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3725/11040 Training loss: 1.3597 0.4606 sec/batch\n",
      "Epoch 14/40  Iteration 3726/11040 Training loss: 1.3600 0.4732 sec/batch\n",
      "Epoch 14/40  Iteration 3727/11040 Training loss: 1.3601 0.4735 sec/batch\n",
      "Epoch 14/40  Iteration 3728/11040 Training loss: 1.3601 0.4767 sec/batch\n",
      "Epoch 14/40  Iteration 3729/11040 Training loss: 1.3601 0.4737 sec/batch\n",
      "Epoch 14/40  Iteration 3730/11040 Training loss: 1.3602 0.4596 sec/batch\n",
      "Epoch 14/40  Iteration 3731/11040 Training loss: 1.3601 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3732/11040 Training loss: 1.3600 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3733/11040 Training loss: 1.3600 0.4844 sec/batch\n",
      "Epoch 14/40  Iteration 3734/11040 Training loss: 1.3599 0.4647 sec/batch\n",
      "Epoch 14/40  Iteration 3735/11040 Training loss: 1.3598 0.4754 sec/batch\n",
      "Epoch 14/40  Iteration 3736/11040 Training loss: 1.3598 0.4745 sec/batch\n",
      "Epoch 14/40  Iteration 3737/11040 Training loss: 1.3595 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3738/11040 Training loss: 1.3595 0.4739 sec/batch\n",
      "Epoch 14/40  Iteration 3739/11040 Training loss: 1.3595 0.4773 sec/batch\n",
      "Epoch 14/40  Iteration 3740/11040 Training loss: 1.3597 0.4744 sec/batch\n",
      "Epoch 14/40  Iteration 3741/11040 Training loss: 1.3596 0.4754 sec/batch\n",
      "Epoch 14/40  Iteration 3742/11040 Training loss: 1.3596 0.4734 sec/batch\n",
      "Epoch 14/40  Iteration 3743/11040 Training loss: 1.3596 0.4589 sec/batch\n",
      "Epoch 14/40  Iteration 3744/11040 Training loss: 1.3595 0.4589 sec/batch\n",
      "Epoch 14/40  Iteration 3745/11040 Training loss: 1.3594 0.4596 sec/batch\n",
      "Epoch 14/40  Iteration 3746/11040 Training loss: 1.3595 0.4738 sec/batch\n",
      "Epoch 14/40  Iteration 3747/11040 Training loss: 1.3596 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3748/11040 Training loss: 1.3596 0.4743 sec/batch\n",
      "Epoch 14/40  Iteration 3749/11040 Training loss: 1.3597 0.4614 sec/batch\n",
      "Epoch 14/40  Iteration 3750/11040 Training loss: 1.3597 0.4723 sec/batch\n",
      "Epoch 14/40  Iteration 3751/11040 Training loss: 1.3598 0.4757 sec/batch\n",
      "Epoch 14/40  Iteration 3752/11040 Training loss: 1.3598 0.4736 sec/batch\n",
      "Epoch 14/40  Iteration 3753/11040 Training loss: 1.3599 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3754/11040 Training loss: 1.3601 0.4764 sec/batch\n",
      "Epoch 14/40  Iteration 3755/11040 Training loss: 1.3601 0.4721 sec/batch\n",
      "Epoch 14/40  Iteration 3756/11040 Training loss: 1.3601 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3757/11040 Training loss: 1.3600 0.4771 sec/batch\n",
      "Epoch 14/40  Iteration 3758/11040 Training loss: 1.3600 0.4745 sec/batch\n",
      "Epoch 14/40  Iteration 3759/11040 Training loss: 1.3601 0.4735 sec/batch\n",
      "Epoch 14/40  Iteration 3760/11040 Training loss: 1.3602 0.4743 sec/batch\n",
      "Epoch 14/40  Iteration 3761/11040 Training loss: 1.3602 0.4747 sec/batch\n",
      "Epoch 14/40  Iteration 3762/11040 Training loss: 1.3601 0.4602 sec/batch\n",
      "Epoch 14/40  Iteration 3763/11040 Training loss: 1.3601 0.4581 sec/batch\n",
      "Epoch 14/40  Iteration 3764/11040 Training loss: 1.3601 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3765/11040 Training loss: 1.3602 0.4763 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40  Iteration 3766/11040 Training loss: 1.3602 0.4747 sec/batch\n",
      "Epoch 14/40  Iteration 3767/11040 Training loss: 1.3604 0.4724 sec/batch\n",
      "Epoch 14/40  Iteration 3768/11040 Training loss: 1.3604 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3769/11040 Training loss: 1.3604 0.4762 sec/batch\n",
      "Epoch 14/40  Iteration 3770/11040 Training loss: 1.3604 0.4745 sec/batch\n",
      "Epoch 14/40  Iteration 3771/11040 Training loss: 1.3603 0.4603 sec/batch\n",
      "Epoch 14/40  Iteration 3772/11040 Training loss: 1.3604 0.4730 sec/batch\n",
      "Epoch 14/40  Iteration 3773/11040 Training loss: 1.3603 0.4786 sec/batch\n",
      "Epoch 14/40  Iteration 3774/11040 Training loss: 1.3604 0.4700 sec/batch\n",
      "Epoch 14/40  Iteration 3775/11040 Training loss: 1.3606 0.4615 sec/batch\n",
      "Epoch 14/40  Iteration 3776/11040 Training loss: 1.3607 0.4576 sec/batch\n",
      "Epoch 14/40  Iteration 3777/11040 Training loss: 1.3607 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3778/11040 Training loss: 1.3607 0.4761 sec/batch\n",
      "Epoch 14/40  Iteration 3779/11040 Training loss: 1.3606 0.4759 sec/batch\n",
      "Epoch 14/40  Iteration 3780/11040 Training loss: 1.3606 0.4730 sec/batch\n",
      "Epoch 14/40  Iteration 3781/11040 Training loss: 1.3605 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3782/11040 Training loss: 1.3606 0.4746 sec/batch\n",
      "Epoch 14/40  Iteration 3783/11040 Training loss: 1.3608 0.4746 sec/batch\n",
      "Epoch 14/40  Iteration 3784/11040 Training loss: 1.3608 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3785/11040 Training loss: 1.3608 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3786/11040 Training loss: 1.3609 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3787/11040 Training loss: 1.3610 0.4727 sec/batch\n",
      "Epoch 14/40  Iteration 3788/11040 Training loss: 1.3611 0.4607 sec/batch\n",
      "Epoch 14/40  Iteration 3789/11040 Training loss: 1.3613 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3790/11040 Training loss: 1.3614 0.4593 sec/batch\n",
      "Epoch 14/40  Iteration 3791/11040 Training loss: 1.3616 0.4587 sec/batch\n",
      "Epoch 14/40  Iteration 3792/11040 Training loss: 1.3617 0.4751 sec/batch\n",
      "Epoch 14/40  Iteration 3793/11040 Training loss: 1.3617 0.4765 sec/batch\n",
      "Epoch 14/40  Iteration 3794/11040 Training loss: 1.3617 0.4732 sec/batch\n",
      "Epoch 14/40  Iteration 3795/11040 Training loss: 1.3617 0.4596 sec/batch\n",
      "Epoch 14/40  Iteration 3796/11040 Training loss: 1.3618 0.4756 sec/batch\n",
      "Epoch 14/40  Iteration 3797/11040 Training loss: 1.3616 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3798/11040 Training loss: 1.3616 0.4760 sec/batch\n",
      "Epoch 14/40  Iteration 3799/11040 Training loss: 1.3615 0.4721 sec/batch\n",
      "Epoch 14/40  Iteration 3800/11040 Training loss: 1.3615 0.4601 sec/batch\n",
      "Epoch 14/40  Iteration 3801/11040 Training loss: 1.3615 0.4607 sec/batch\n",
      "Epoch 14/40  Iteration 3802/11040 Training loss: 1.3615 0.4734 sec/batch\n",
      "Epoch 14/40  Iteration 3803/11040 Training loss: 1.3614 0.4758 sec/batch\n",
      "Epoch 14/40  Iteration 3804/11040 Training loss: 1.3613 0.4743 sec/batch\n",
      "Epoch 14/40  Iteration 3805/11040 Training loss: 1.3612 0.4727 sec/batch\n",
      "Epoch 14/40  Iteration 3806/11040 Training loss: 1.3610 0.4613 sec/batch\n",
      "Epoch 14/40  Iteration 3807/11040 Training loss: 1.3609 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3808/11040 Training loss: 1.3609 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3809/11040 Training loss: 1.3608 0.4605 sec/batch\n",
      "Epoch 14/40  Iteration 3810/11040 Training loss: 1.3606 0.4587 sec/batch\n",
      "Epoch 14/40  Iteration 3811/11040 Training loss: 1.3606 0.4595 sec/batch\n",
      "Epoch 14/40  Iteration 3812/11040 Training loss: 1.3607 0.4743 sec/batch\n",
      "Epoch 14/40  Iteration 3813/11040 Training loss: 1.3607 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3814/11040 Training loss: 1.3606 0.4582 sec/batch\n",
      "Epoch 14/40  Iteration 3815/11040 Training loss: 1.3605 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3816/11040 Training loss: 1.3603 0.4740 sec/batch\n",
      "Epoch 14/40  Iteration 3817/11040 Training loss: 1.3604 0.4621 sec/batch\n",
      "Epoch 14/40  Iteration 3818/11040 Training loss: 1.3605 0.4732 sec/batch\n",
      "Epoch 14/40  Iteration 3819/11040 Training loss: 1.3604 0.4592 sec/batch\n",
      "Epoch 14/40  Iteration 3820/11040 Training loss: 1.3605 0.4599 sec/batch\n",
      "Epoch 14/40  Iteration 3821/11040 Training loss: 1.3603 0.4574 sec/batch\n",
      "Epoch 14/40  Iteration 3822/11040 Training loss: 1.3602 0.4596 sec/batch\n",
      "Epoch 14/40  Iteration 3823/11040 Training loss: 1.3602 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3824/11040 Training loss: 1.3602 0.4752 sec/batch\n",
      "Epoch 14/40  Iteration 3825/11040 Training loss: 1.3602 0.4742 sec/batch\n",
      "Epoch 14/40  Iteration 3826/11040 Training loss: 1.3602 0.4741 sec/batch\n",
      "Epoch 14/40  Iteration 3827/11040 Training loss: 1.3603 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3828/11040 Training loss: 1.3603 0.4758 sec/batch\n",
      "Epoch 14/40  Iteration 3829/11040 Training loss: 1.3603 0.4589 sec/batch\n",
      "Epoch 14/40  Iteration 3830/11040 Training loss: 1.3603 0.4594 sec/batch\n",
      "Epoch 14/40  Iteration 3831/11040 Training loss: 1.3602 0.4600 sec/batch\n",
      "Epoch 14/40  Iteration 3832/11040 Training loss: 1.3601 0.4630 sec/batch\n",
      "Epoch 14/40  Iteration 3833/11040 Training loss: 1.3600 0.4729 sec/batch\n",
      "Epoch 14/40  Iteration 3834/11040 Training loss: 1.3600 0.4733 sec/batch\n",
      "Epoch 14/40  Iteration 3835/11040 Training loss: 1.3599 0.4602 sec/batch\n",
      "Epoch 14/40  Iteration 3836/11040 Training loss: 1.3598 0.4598 sec/batch\n",
      "Epoch 14/40  Iteration 3837/11040 Training loss: 1.3597 0.4722 sec/batch\n",
      "Epoch 14/40  Iteration 3838/11040 Training loss: 1.3598 0.4608 sec/batch\n",
      "Epoch 14/40  Iteration 3839/11040 Training loss: 1.3596 0.4585 sec/batch\n",
      "Epoch 14/40  Iteration 3840/11040 Training loss: 1.3596 0.4739 sec/batch\n",
      "Epoch 14/40  Iteration 3841/11040 Training loss: 1.3595 0.4771 sec/batch\n",
      "Epoch 14/40  Iteration 3842/11040 Training loss: 1.3596 0.4730 sec/batch\n",
      "Epoch 14/40  Iteration 3843/11040 Training loss: 1.3596 0.4759 sec/batch\n",
      "Epoch 14/40  Iteration 3844/11040 Training loss: 1.3595 0.4749 sec/batch\n",
      "Epoch 14/40  Iteration 3845/11040 Training loss: 1.3596 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3846/11040 Training loss: 1.3596 0.4744 sec/batch\n",
      "Epoch 14/40  Iteration 3847/11040 Training loss: 1.3595 0.4594 sec/batch\n",
      "Epoch 14/40  Iteration 3848/11040 Training loss: 1.3595 0.4762 sec/batch\n",
      "Epoch 14/40  Iteration 3849/11040 Training loss: 1.3595 0.4730 sec/batch\n",
      "Epoch 14/40  Iteration 3850/11040 Training loss: 1.3596 0.4769 sec/batch\n",
      "Epoch 14/40  Iteration 3851/11040 Training loss: 1.3596 0.4733 sec/batch\n",
      "Epoch 14/40  Iteration 3852/11040 Training loss: 1.3597 0.4594 sec/batch\n",
      "Epoch 14/40  Iteration 3853/11040 Training loss: 1.3597 0.4587 sec/batch\n",
      "Epoch 14/40  Iteration 3854/11040 Training loss: 1.3597 0.4584 sec/batch\n",
      "Epoch 14/40  Iteration 3855/11040 Training loss: 1.3598 0.4754 sec/batch\n",
      "Epoch 14/40  Iteration 3856/11040 Training loss: 1.3599 0.4766 sec/batch\n",
      "Epoch 14/40  Iteration 3857/11040 Training loss: 1.3600 0.4727 sec/batch\n",
      "Epoch 14/40  Iteration 3858/11040 Training loss: 1.3600 0.4753 sec/batch\n",
      "Epoch 14/40  Iteration 3859/11040 Training loss: 1.3603 0.4729 sec/batch\n",
      "Epoch 14/40  Iteration 3860/11040 Training loss: 1.3605 0.4763 sec/batch\n",
      "Epoch 14/40  Iteration 3861/11040 Training loss: 1.3605 0.4733 sec/batch\n",
      "Epoch 14/40  Iteration 3862/11040 Training loss: 1.3606 0.4599 sec/batch\n",
      "Epoch 14/40  Iteration 3863/11040 Training loss: 1.3605 0.4750 sec/batch\n",
      "Epoch 14/40  Iteration 3864/11040 Training loss: 1.3605 0.4594 sec/batch\n",
      "Epoch 15/40  Iteration 3865/11040 Training loss: 1.4210 0.4584 sec/batch\n",
      "Epoch 15/40  Iteration 3866/11040 Training loss: 1.3833 0.4599 sec/batch\n",
      "Epoch 15/40  Iteration 3867/11040 Training loss: 1.3781 0.4584 sec/batch\n",
      "Epoch 15/40  Iteration 3868/11040 Training loss: 1.3768 0.4584 sec/batch\n",
      "Epoch 15/40  Iteration 3869/11040 Training loss: 1.3795 0.4705 sec/batch\n",
      "Epoch 15/40  Iteration 3870/11040 Training loss: 1.3785 0.4644 sec/batch\n",
      "Epoch 15/40  Iteration 3871/11040 Training loss: 1.3698 0.4752 sec/batch\n",
      "Epoch 15/40  Iteration 3872/11040 Training loss: 1.3686 0.4739 sec/batch\n",
      "Epoch 15/40  Iteration 3873/11040 Training loss: 1.3651 0.4585 sec/batch\n",
      "Epoch 15/40  Iteration 3874/11040 Training loss: 1.3658 0.4765 sec/batch\n",
      "Epoch 15/40  Iteration 3875/11040 Training loss: 1.3629 0.4728 sec/batch\n",
      "Epoch 15/40  Iteration 3876/11040 Training loss: 1.3619 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 3877/11040 Training loss: 1.3590 0.4781 sec/batch\n",
      "Epoch 15/40  Iteration 3878/11040 Training loss: 1.3560 0.4717 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40  Iteration 3879/11040 Training loss: 1.3550 0.4607 sec/batch\n",
      "Epoch 15/40  Iteration 3880/11040 Training loss: 1.3550 0.4574 sec/batch\n",
      "Epoch 15/40  Iteration 3881/11040 Training loss: 1.3528 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 3882/11040 Training loss: 1.3514 0.4589 sec/batch\n",
      "Epoch 15/40  Iteration 3883/11040 Training loss: 1.3505 0.4602 sec/batch\n",
      "Epoch 15/40  Iteration 3884/11040 Training loss: 1.3506 0.4731 sec/batch\n",
      "Epoch 15/40  Iteration 3885/11040 Training loss: 1.3516 0.4603 sec/batch\n",
      "Epoch 15/40  Iteration 3886/11040 Training loss: 1.3521 0.4749 sec/batch\n",
      "Epoch 15/40  Iteration 3887/11040 Training loss: 1.3524 0.4594 sec/batch\n",
      "Epoch 15/40  Iteration 3888/11040 Training loss: 1.3529 0.4585 sec/batch\n",
      "Epoch 15/40  Iteration 3889/11040 Training loss: 1.3521 0.4586 sec/batch\n",
      "Epoch 15/40  Iteration 3890/11040 Training loss: 1.3511 0.4593 sec/batch\n",
      "Epoch 15/40  Iteration 3891/11040 Training loss: 1.3507 0.4598 sec/batch\n",
      "Epoch 15/40  Iteration 3892/11040 Training loss: 1.3505 0.4731 sec/batch\n",
      "Epoch 15/40  Iteration 3893/11040 Training loss: 1.3501 0.4752 sec/batch\n",
      "Epoch 15/40  Iteration 3894/11040 Training loss: 1.3499 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 3895/11040 Training loss: 1.3496 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3896/11040 Training loss: 1.3494 0.4760 sec/batch\n",
      "Epoch 15/40  Iteration 3897/11040 Training loss: 1.3489 0.4763 sec/batch\n",
      "Epoch 15/40  Iteration 3898/11040 Training loss: 1.3484 0.4729 sec/batch\n",
      "Epoch 15/40  Iteration 3899/11040 Training loss: 1.3482 0.4586 sec/batch\n",
      "Epoch 15/40  Iteration 3900/11040 Training loss: 1.3488 0.4594 sec/batch\n",
      "Epoch 15/40  Iteration 3901/11040 Training loss: 1.3489 0.4594 sec/batch\n",
      "Epoch 15/40  Iteration 3902/11040 Training loss: 1.3479 0.4610 sec/batch\n",
      "Epoch 15/40  Iteration 3903/11040 Training loss: 1.3473 0.4716 sec/batch\n",
      "Epoch 15/40  Iteration 3904/11040 Training loss: 1.3473 0.4610 sec/batch\n",
      "Epoch 15/40  Iteration 3905/11040 Training loss: 1.3469 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 3906/11040 Training loss: 1.3460 0.4745 sec/batch\n",
      "Epoch 15/40  Iteration 3907/11040 Training loss: 1.3464 0.4752 sec/batch\n",
      "Epoch 15/40  Iteration 3908/11040 Training loss: 1.3459 0.4734 sec/batch\n",
      "Epoch 15/40  Iteration 3909/11040 Training loss: 1.3457 0.4739 sec/batch\n",
      "Epoch 15/40  Iteration 3910/11040 Training loss: 1.3454 0.4758 sec/batch\n",
      "Epoch 15/40  Iteration 3911/11040 Training loss: 1.3446 0.4774 sec/batch\n",
      "Epoch 15/40  Iteration 3912/11040 Training loss: 1.3444 0.4724 sec/batch\n",
      "Epoch 15/40  Iteration 3913/11040 Training loss: 1.3437 0.4757 sec/batch\n",
      "Epoch 15/40  Iteration 3914/11040 Training loss: 1.3440 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 3915/11040 Training loss: 1.3439 0.4598 sec/batch\n",
      "Epoch 15/40  Iteration 3916/11040 Training loss: 1.3435 0.4574 sec/batch\n",
      "Epoch 15/40  Iteration 3917/11040 Training loss: 1.3439 0.4595 sec/batch\n",
      "Epoch 15/40  Iteration 3918/11040 Training loss: 1.3435 0.4595 sec/batch\n",
      "Epoch 15/40  Iteration 3919/11040 Training loss: 1.3435 0.4754 sec/batch\n",
      "Epoch 15/40  Iteration 3920/11040 Training loss: 1.3435 0.4602 sec/batch\n",
      "Epoch 15/40  Iteration 3921/11040 Training loss: 1.3434 0.4737 sec/batch\n",
      "Epoch 15/40  Iteration 3922/11040 Training loss: 1.3434 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3923/11040 Training loss: 1.3439 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3924/11040 Training loss: 1.3436 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3925/11040 Training loss: 1.3436 0.4746 sec/batch\n",
      "Epoch 15/40  Iteration 3926/11040 Training loss: 1.3435 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 3927/11040 Training loss: 1.3435 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 3928/11040 Training loss: 1.3434 0.4598 sec/batch\n",
      "Epoch 15/40  Iteration 3929/11040 Training loss: 1.3436 0.4740 sec/batch\n",
      "Epoch 15/40  Iteration 3930/11040 Training loss: 1.3433 0.4747 sec/batch\n",
      "Epoch 15/40  Iteration 3931/11040 Training loss: 1.3431 0.4595 sec/batch\n",
      "Epoch 15/40  Iteration 3932/11040 Training loss: 1.3432 0.4702 sec/batch\n",
      "Epoch 15/40  Iteration 3933/11040 Training loss: 1.3430 0.4791 sec/batch\n",
      "Epoch 15/40  Iteration 3934/11040 Training loss: 1.3430 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 3935/11040 Training loss: 1.3426 0.4747 sec/batch\n",
      "Epoch 15/40  Iteration 3936/11040 Training loss: 1.3420 0.4763 sec/batch\n",
      "Epoch 15/40  Iteration 3937/11040 Training loss: 1.3414 0.4735 sec/batch\n",
      "Epoch 15/40  Iteration 3938/11040 Training loss: 1.3412 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 3939/11040 Training loss: 1.3411 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 3940/11040 Training loss: 1.3414 0.4593 sec/batch\n",
      "Epoch 15/40  Iteration 3941/11040 Training loss: 1.3410 0.4600 sec/batch\n",
      "Epoch 15/40  Iteration 3942/11040 Training loss: 1.3409 0.4742 sec/batch\n",
      "Epoch 15/40  Iteration 3943/11040 Training loss: 1.3405 0.4768 sec/batch\n",
      "Epoch 15/40  Iteration 3944/11040 Training loss: 1.3404 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 3945/11040 Training loss: 1.3401 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 3946/11040 Training loss: 1.3401 0.4590 sec/batch\n",
      "Epoch 15/40  Iteration 3947/11040 Training loss: 1.3400 0.4600 sec/batch\n",
      "Epoch 15/40  Iteration 3948/11040 Training loss: 1.3399 0.4588 sec/batch\n",
      "Epoch 15/40  Iteration 3949/11040 Training loss: 1.3396 0.4584 sec/batch\n",
      "Epoch 15/40  Iteration 3950/11040 Training loss: 1.3396 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3951/11040 Training loss: 1.3392 0.4759 sec/batch\n",
      "Epoch 15/40  Iteration 3952/11040 Training loss: 1.3384 0.4603 sec/batch\n",
      "Epoch 15/40  Iteration 3953/11040 Training loss: 1.3385 0.4573 sec/batch\n",
      "Epoch 15/40  Iteration 3954/11040 Training loss: 1.3385 0.4742 sec/batch\n",
      "Epoch 15/40  Iteration 3955/11040 Training loss: 1.3388 0.4764 sec/batch\n",
      "Epoch 15/40  Iteration 3956/11040 Training loss: 1.3388 0.4855 sec/batch\n",
      "Epoch 15/40  Iteration 3957/11040 Training loss: 1.3386 0.4589 sec/batch\n",
      "Epoch 15/40  Iteration 3958/11040 Training loss: 1.3390 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3959/11040 Training loss: 1.3395 0.4745 sec/batch\n",
      "Epoch 15/40  Iteration 3960/11040 Training loss: 1.3398 0.4772 sec/batch\n",
      "Epoch 15/40  Iteration 3961/11040 Training loss: 1.3402 0.4732 sec/batch\n",
      "Epoch 15/40  Iteration 3962/11040 Training loss: 1.3406 0.4734 sec/batch\n",
      "Epoch 15/40  Iteration 3963/11040 Training loss: 1.3406 0.4608 sec/batch\n",
      "Epoch 15/40  Iteration 3964/11040 Training loss: 1.3404 0.4583 sec/batch\n",
      "Epoch 15/40  Iteration 3965/11040 Training loss: 1.3405 0.4589 sec/batch\n",
      "Epoch 15/40  Iteration 3966/11040 Training loss: 1.3406 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 3967/11040 Training loss: 1.3402 0.4685 sec/batch\n",
      "Epoch 15/40  Iteration 3968/11040 Training loss: 1.3403 0.4818 sec/batch\n",
      "Epoch 15/40  Iteration 3969/11040 Training loss: 1.3401 0.4742 sec/batch\n",
      "Epoch 15/40  Iteration 3970/11040 Training loss: 1.3400 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 3971/11040 Training loss: 1.3402 0.4752 sec/batch\n",
      "Epoch 15/40  Iteration 3972/11040 Training loss: 1.3404 0.4734 sec/batch\n",
      "Epoch 15/40  Iteration 3973/11040 Training loss: 1.3407 0.4595 sec/batch\n",
      "Epoch 15/40  Iteration 3974/11040 Training loss: 1.3407 0.4594 sec/batch\n",
      "Epoch 15/40  Iteration 3975/11040 Training loss: 1.3408 0.4618 sec/batch\n",
      "Epoch 15/40  Iteration 3976/11040 Training loss: 1.3410 0.4722 sec/batch\n",
      "Epoch 15/40  Iteration 3977/11040 Training loss: 1.3411 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 3978/11040 Training loss: 1.3413 0.4762 sec/batch\n",
      "Epoch 15/40  Iteration 3979/11040 Training loss: 1.3417 0.4605 sec/batch\n",
      "Epoch 15/40  Iteration 3980/11040 Training loss: 1.3422 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 3981/11040 Training loss: 1.3421 0.4606 sec/batch\n",
      "Epoch 15/40  Iteration 3982/11040 Training loss: 1.3425 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 3983/11040 Training loss: 1.3427 0.4575 sec/batch\n",
      "Epoch 15/40  Iteration 3984/11040 Training loss: 1.3429 0.4844 sec/batch\n",
      "Epoch 15/40  Iteration 3985/11040 Training loss: 1.3429 0.4647 sec/batch\n",
      "Epoch 15/40  Iteration 3986/11040 Training loss: 1.3429 0.4713 sec/batch\n",
      "Epoch 15/40  Iteration 3987/11040 Training loss: 1.3432 0.4606 sec/batch\n",
      "Epoch 15/40  Iteration 3988/11040 Training loss: 1.3437 0.4734 sec/batch\n",
      "Epoch 15/40  Iteration 3989/11040 Training loss: 1.3438 0.4755 sec/batch\n",
      "Epoch 15/40  Iteration 3990/11040 Training loss: 1.3441 0.4745 sec/batch\n",
      "Epoch 15/40  Iteration 3991/11040 Training loss: 1.3441 0.4740 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40  Iteration 3992/11040 Training loss: 1.3441 0.4737 sec/batch\n",
      "Epoch 15/40  Iteration 3993/11040 Training loss: 1.3440 0.4761 sec/batch\n",
      "Epoch 15/40  Iteration 3994/11040 Training loss: 1.3442 0.4733 sec/batch\n",
      "Epoch 15/40  Iteration 3995/11040 Training loss: 1.3440 0.4764 sec/batch\n",
      "Epoch 15/40  Iteration 3996/11040 Training loss: 1.3438 0.4739 sec/batch\n",
      "Epoch 15/40  Iteration 3997/11040 Training loss: 1.3440 0.4750 sec/batch\n",
      "Epoch 15/40  Iteration 3998/11040 Training loss: 1.3439 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 3999/11040 Training loss: 1.3441 0.4761 sec/batch\n",
      "Epoch 15/40  Iteration 4000/11040 Training loss: 1.3443 0.4732 sec/batch\n",
      "Validation loss: 1.31312 Saving checkpoint!\n",
      "Epoch 15/40  Iteration 4001/11040 Training loss: 1.3450 0.4617 sec/batch\n",
      "Epoch 15/40  Iteration 4002/11040 Training loss: 1.3453 0.4812 sec/batch\n",
      "Epoch 15/40  Iteration 4003/11040 Training loss: 1.3454 0.4734 sec/batch\n",
      "Epoch 15/40  Iteration 4004/11040 Training loss: 1.3455 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 4005/11040 Training loss: 1.3455 0.4761 sec/batch\n",
      "Epoch 15/40  Iteration 4006/11040 Training loss: 1.3456 0.4746 sec/batch\n",
      "Epoch 15/40  Iteration 4007/11040 Training loss: 1.3455 0.4715 sec/batch\n",
      "Epoch 15/40  Iteration 4008/11040 Training loss: 1.3453 0.4749 sec/batch\n",
      "Epoch 15/40  Iteration 4009/11040 Training loss: 1.3453 0.4783 sec/batch\n",
      "Epoch 15/40  Iteration 4010/11040 Training loss: 1.3452 0.4723 sec/batch\n",
      "Epoch 15/40  Iteration 4011/11040 Training loss: 1.3453 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 4012/11040 Training loss: 1.3452 0.4750 sec/batch\n",
      "Epoch 15/40  Iteration 4013/11040 Training loss: 1.3449 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4014/11040 Training loss: 1.3449 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4015/11040 Training loss: 1.3449 0.4692 sec/batch\n",
      "Epoch 15/40  Iteration 4016/11040 Training loss: 1.3450 0.4791 sec/batch\n",
      "Epoch 15/40  Iteration 4017/11040 Training loss: 1.3449 0.4740 sec/batch\n",
      "Epoch 15/40  Iteration 4018/11040 Training loss: 1.3450 0.4616 sec/batch\n",
      "Epoch 15/40  Iteration 4019/11040 Training loss: 1.3451 0.4731 sec/batch\n",
      "Epoch 15/40  Iteration 4020/11040 Training loss: 1.3450 0.4749 sec/batch\n",
      "Epoch 15/40  Iteration 4021/11040 Training loss: 1.3448 0.4760 sec/batch\n",
      "Epoch 15/40  Iteration 4022/11040 Training loss: 1.3449 0.4731 sec/batch\n",
      "Epoch 15/40  Iteration 4023/11040 Training loss: 1.3450 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4024/11040 Training loss: 1.3450 0.4753 sec/batch\n",
      "Epoch 15/40  Iteration 4025/11040 Training loss: 1.3451 0.4731 sec/batch\n",
      "Epoch 15/40  Iteration 4026/11040 Training loss: 1.3451 0.4604 sec/batch\n",
      "Epoch 15/40  Iteration 4027/11040 Training loss: 1.3451 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4028/11040 Training loss: 1.3452 0.4547 sec/batch\n",
      "Epoch 15/40  Iteration 4029/11040 Training loss: 1.3452 0.4644 sec/batch\n",
      "Epoch 15/40  Iteration 4030/11040 Training loss: 1.3454 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 4031/11040 Training loss: 1.3454 0.4729 sec/batch\n",
      "Epoch 15/40  Iteration 4032/11040 Training loss: 1.3453 0.4661 sec/batch\n",
      "Epoch 15/40  Iteration 4033/11040 Training loss: 1.3452 0.4832 sec/batch\n",
      "Epoch 15/40  Iteration 4034/11040 Training loss: 1.3452 0.4759 sec/batch\n",
      "Epoch 15/40  Iteration 4035/11040 Training loss: 1.3454 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 4036/11040 Training loss: 1.3454 0.4784 sec/batch\n",
      "Epoch 15/40  Iteration 4037/11040 Training loss: 1.3455 0.4704 sec/batch\n",
      "Epoch 15/40  Iteration 4038/11040 Training loss: 1.3454 0.4725 sec/batch\n",
      "Epoch 15/40  Iteration 4039/11040 Training loss: 1.3454 0.4605 sec/batch\n",
      "Epoch 15/40  Iteration 4040/11040 Training loss: 1.3455 0.4609 sec/batch\n",
      "Epoch 15/40  Iteration 4041/11040 Training loss: 1.3455 0.4738 sec/batch\n",
      "Epoch 15/40  Iteration 4042/11040 Training loss: 1.3455 0.4734 sec/batch\n",
      "Epoch 15/40  Iteration 4043/11040 Training loss: 1.3457 0.4756 sec/batch\n",
      "Epoch 15/40  Iteration 4044/11040 Training loss: 1.3457 0.4602 sec/batch\n",
      "Epoch 15/40  Iteration 4045/11040 Training loss: 1.3458 0.4595 sec/batch\n",
      "Epoch 15/40  Iteration 4046/11040 Training loss: 1.3458 0.4573 sec/batch\n",
      "Epoch 15/40  Iteration 4047/11040 Training loss: 1.3456 0.4588 sec/batch\n",
      "Epoch 15/40  Iteration 4048/11040 Training loss: 1.3457 0.4748 sec/batch\n",
      "Epoch 15/40  Iteration 4049/11040 Training loss: 1.3457 0.4764 sec/batch\n",
      "Epoch 15/40  Iteration 4050/11040 Training loss: 1.3458 0.4755 sec/batch\n",
      "Epoch 15/40  Iteration 4051/11040 Training loss: 1.3460 0.4588 sec/batch\n",
      "Epoch 15/40  Iteration 4052/11040 Training loss: 1.3461 0.4574 sec/batch\n",
      "Epoch 15/40  Iteration 4053/11040 Training loss: 1.3460 0.4580 sec/batch\n",
      "Epoch 15/40  Iteration 4054/11040 Training loss: 1.3460 0.4674 sec/batch\n",
      "Epoch 15/40  Iteration 4055/11040 Training loss: 1.3459 0.4828 sec/batch\n",
      "Epoch 15/40  Iteration 4056/11040 Training loss: 1.3459 0.4753 sec/batch\n",
      "Epoch 15/40  Iteration 4057/11040 Training loss: 1.3459 0.4740 sec/batch\n",
      "Epoch 15/40  Iteration 4058/11040 Training loss: 1.3459 0.4746 sec/batch\n",
      "Epoch 15/40  Iteration 4059/11040 Training loss: 1.3462 0.4745 sec/batch\n",
      "Epoch 15/40  Iteration 4060/11040 Training loss: 1.3460 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4061/11040 Training loss: 1.3462 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4062/11040 Training loss: 1.3462 0.4596 sec/batch\n",
      "Epoch 15/40  Iteration 4063/11040 Training loss: 1.3463 0.4592 sec/batch\n",
      "Epoch 15/40  Iteration 4064/11040 Training loss: 1.3464 0.4661 sec/batch\n",
      "Epoch 15/40  Iteration 4065/11040 Training loss: 1.3465 0.4835 sec/batch\n",
      "Epoch 15/40  Iteration 4066/11040 Training loss: 1.3467 0.4747 sec/batch\n",
      "Epoch 15/40  Iteration 4067/11040 Training loss: 1.3468 0.4909 sec/batch\n",
      "Epoch 15/40  Iteration 4068/11040 Training loss: 1.3469 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 4069/11040 Training loss: 1.3469 0.4760 sec/batch\n",
      "Epoch 15/40  Iteration 4070/11040 Training loss: 1.3468 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4071/11040 Training loss: 1.3468 0.4584 sec/batch\n",
      "Epoch 15/40  Iteration 4072/11040 Training loss: 1.3468 0.4732 sec/batch\n",
      "Epoch 15/40  Iteration 4073/11040 Training loss: 1.3467 0.4631 sec/batch\n",
      "Epoch 15/40  Iteration 4074/11040 Training loss: 1.3467 0.4596 sec/batch\n",
      "Epoch 15/40  Iteration 4075/11040 Training loss: 1.3466 0.4724 sec/batch\n",
      "Epoch 15/40  Iteration 4076/11040 Training loss: 1.3466 0.4750 sec/batch\n",
      "Epoch 15/40  Iteration 4077/11040 Training loss: 1.3466 0.4752 sec/batch\n",
      "Epoch 15/40  Iteration 4078/11040 Training loss: 1.3466 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 4079/11040 Training loss: 1.3465 0.4748 sec/batch\n",
      "Epoch 15/40  Iteration 4080/11040 Training loss: 1.3464 0.4644 sec/batch\n",
      "Epoch 15/40  Iteration 4081/11040 Training loss: 1.3463 0.4707 sec/batch\n",
      "Epoch 15/40  Iteration 4082/11040 Training loss: 1.3462 0.4735 sec/batch\n",
      "Epoch 15/40  Iteration 4083/11040 Training loss: 1.3461 0.4597 sec/batch\n",
      "Epoch 15/40  Iteration 4084/11040 Training loss: 1.3461 0.4576 sec/batch\n",
      "Epoch 15/40  Iteration 4085/11040 Training loss: 1.3460 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 4086/11040 Training loss: 1.3458 0.4750 sec/batch\n",
      "Epoch 15/40  Iteration 4087/11040 Training loss: 1.3457 0.4756 sec/batch\n",
      "Epoch 15/40  Iteration 4088/11040 Training loss: 1.3458 0.4750 sec/batch\n",
      "Epoch 15/40  Iteration 4089/11040 Training loss: 1.3458 0.4744 sec/batch\n",
      "Epoch 15/40  Iteration 4090/11040 Training loss: 1.3458 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4091/11040 Training loss: 1.3456 0.4742 sec/batch\n",
      "Epoch 15/40  Iteration 4092/11040 Training loss: 1.3455 0.4602 sec/batch\n",
      "Epoch 15/40  Iteration 4093/11040 Training loss: 1.3455 0.4588 sec/batch\n",
      "Epoch 15/40  Iteration 4094/11040 Training loss: 1.3456 0.4733 sec/batch\n",
      "Epoch 15/40  Iteration 4095/11040 Training loss: 1.3456 0.4751 sec/batch\n",
      "Epoch 15/40  Iteration 4096/11040 Training loss: 1.3456 0.4758 sec/batch\n",
      "Epoch 15/40  Iteration 4097/11040 Training loss: 1.3455 0.4586 sec/batch\n",
      "Epoch 15/40  Iteration 4098/11040 Training loss: 1.3454 0.4594 sec/batch\n",
      "Epoch 15/40  Iteration 4099/11040 Training loss: 1.3454 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 4100/11040 Training loss: 1.3454 0.4605 sec/batch\n",
      "Epoch 15/40  Iteration 4101/11040 Training loss: 1.3454 0.4759 sec/batch\n",
      "Epoch 15/40  Iteration 4102/11040 Training loss: 1.3454 0.4843 sec/batch\n",
      "Epoch 15/40  Iteration 4103/11040 Training loss: 1.3455 0.4634 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40  Iteration 4104/11040 Training loss: 1.3455 0.4600 sec/batch\n",
      "Epoch 15/40  Iteration 4105/11040 Training loss: 1.3455 0.4575 sec/batch\n",
      "Epoch 15/40  Iteration 4106/11040 Training loss: 1.3455 0.4604 sec/batch\n",
      "Epoch 15/40  Iteration 4107/11040 Training loss: 1.3454 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 4108/11040 Training loss: 1.3453 0.4760 sec/batch\n",
      "Epoch 15/40  Iteration 4109/11040 Training loss: 1.3452 0.4731 sec/batch\n",
      "Epoch 15/40  Iteration 4110/11040 Training loss: 1.3452 0.4617 sec/batch\n",
      "Epoch 15/40  Iteration 4111/11040 Training loss: 1.3452 0.4722 sec/batch\n",
      "Epoch 15/40  Iteration 4112/11040 Training loss: 1.3451 0.4761 sec/batch\n",
      "Epoch 15/40  Iteration 4113/11040 Training loss: 1.3451 0.4584 sec/batch\n",
      "Epoch 15/40  Iteration 4114/11040 Training loss: 1.3451 0.4597 sec/batch\n",
      "Epoch 15/40  Iteration 4115/11040 Training loss: 1.3450 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 4116/11040 Training loss: 1.3449 0.4740 sec/batch\n",
      "Epoch 15/40  Iteration 4117/11040 Training loss: 1.3449 0.4752 sec/batch\n",
      "Epoch 15/40  Iteration 4118/11040 Training loss: 1.3449 0.4743 sec/batch\n",
      "Epoch 15/40  Iteration 4119/11040 Training loss: 1.3449 0.4773 sec/batch\n",
      "Epoch 15/40  Iteration 4120/11040 Training loss: 1.3448 0.4577 sec/batch\n",
      "Epoch 15/40  Iteration 4121/11040 Training loss: 1.3449 0.4592 sec/batch\n",
      "Epoch 15/40  Iteration 4122/11040 Training loss: 1.3450 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 4123/11040 Training loss: 1.3449 0.4605 sec/batch\n",
      "Epoch 15/40  Iteration 4124/11040 Training loss: 1.3448 0.4739 sec/batch\n",
      "Epoch 15/40  Iteration 4125/11040 Training loss: 1.3448 0.4750 sec/batch\n",
      "Epoch 15/40  Iteration 4126/11040 Training loss: 1.3449 0.4756 sec/batch\n",
      "Epoch 15/40  Iteration 4127/11040 Training loss: 1.3450 0.4595 sec/batch\n",
      "Epoch 15/40  Iteration 4128/11040 Training loss: 1.3450 0.4741 sec/batch\n",
      "Epoch 15/40  Iteration 4129/11040 Training loss: 1.3450 0.4761 sec/batch\n",
      "Epoch 15/40  Iteration 4130/11040 Training loss: 1.3451 0.4573 sec/batch\n",
      "Epoch 15/40  Iteration 4131/11040 Training loss: 1.3452 0.4740 sec/batch\n",
      "Epoch 15/40  Iteration 4132/11040 Training loss: 1.3452 0.4770 sec/batch\n",
      "Epoch 15/40  Iteration 4133/11040 Training loss: 1.3453 0.4726 sec/batch\n",
      "Epoch 15/40  Iteration 4134/11040 Training loss: 1.3454 0.4764 sec/batch\n",
      "Epoch 15/40  Iteration 4135/11040 Training loss: 1.3457 0.4730 sec/batch\n",
      "Epoch 15/40  Iteration 4136/11040 Training loss: 1.3458 0.4761 sec/batch\n",
      "Epoch 15/40  Iteration 4137/11040 Training loss: 1.3459 0.4601 sec/batch\n",
      "Epoch 15/40  Iteration 4138/11040 Training loss: 1.3459 0.4661 sec/batch\n",
      "Epoch 15/40  Iteration 4139/11040 Training loss: 1.3459 0.4658 sec/batch\n",
      "Epoch 15/40  Iteration 4140/11040 Training loss: 1.3459 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4141/11040 Training loss: 1.4165 0.4589 sec/batch\n",
      "Epoch 16/40  Iteration 4142/11040 Training loss: 1.3754 0.4614 sec/batch\n",
      "Epoch 16/40  Iteration 4143/11040 Training loss: 1.3718 0.4592 sec/batch\n",
      "Epoch 16/40  Iteration 4144/11040 Training loss: 1.3708 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4145/11040 Training loss: 1.3701 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4146/11040 Training loss: 1.3650 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4147/11040 Training loss: 1.3567 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4148/11040 Training loss: 1.3557 0.4762 sec/batch\n",
      "Epoch 16/40  Iteration 4149/11040 Training loss: 1.3518 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4150/11040 Training loss: 1.3518 0.4745 sec/batch\n",
      "Epoch 16/40  Iteration 4151/11040 Training loss: 1.3491 0.4759 sec/batch\n",
      "Epoch 16/40  Iteration 4152/11040 Training loss: 1.3484 0.4742 sec/batch\n",
      "Epoch 16/40  Iteration 4153/11040 Training loss: 1.3467 0.4773 sec/batch\n",
      "Epoch 16/40  Iteration 4154/11040 Training loss: 1.3436 0.4722 sec/batch\n",
      "Epoch 16/40  Iteration 4155/11040 Training loss: 1.3431 0.4591 sec/batch\n",
      "Epoch 16/40  Iteration 4156/11040 Training loss: 1.3438 0.4747 sec/batch\n",
      "Epoch 16/40  Iteration 4157/11040 Training loss: 1.3412 0.4770 sec/batch\n",
      "Epoch 16/40  Iteration 4158/11040 Training loss: 1.3391 0.4721 sec/batch\n",
      "Epoch 16/40  Iteration 4159/11040 Training loss: 1.3382 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4160/11040 Training loss: 1.3382 0.4738 sec/batch\n",
      "Epoch 16/40  Iteration 4161/11040 Training loss: 1.3395 0.4770 sec/batch\n",
      "Epoch 16/40  Iteration 4162/11040 Training loss: 1.3400 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4163/11040 Training loss: 1.3399 0.4593 sec/batch\n",
      "Epoch 16/40  Iteration 4164/11040 Training loss: 1.3411 0.4586 sec/batch\n",
      "Epoch 16/40  Iteration 4165/11040 Training loss: 1.3400 0.4745 sec/batch\n",
      "Epoch 16/40  Iteration 4166/11040 Training loss: 1.3394 0.4695 sec/batch\n",
      "Epoch 16/40  Iteration 4167/11040 Training loss: 1.3392 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4168/11040 Training loss: 1.3393 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4169/11040 Training loss: 1.3392 0.4759 sec/batch\n",
      "Epoch 16/40  Iteration 4170/11040 Training loss: 1.3389 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4171/11040 Training loss: 1.3387 0.4581 sec/batch\n",
      "Epoch 16/40  Iteration 4172/11040 Training loss: 1.3384 0.4591 sec/batch\n",
      "Epoch 16/40  Iteration 4173/11040 Training loss: 1.3378 0.4587 sec/batch\n",
      "Epoch 16/40  Iteration 4174/11040 Training loss: 1.3373 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4175/11040 Training loss: 1.3372 0.4746 sec/batch\n",
      "Epoch 16/40  Iteration 4176/11040 Training loss: 1.3376 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4177/11040 Training loss: 1.3375 0.4769 sec/batch\n",
      "Epoch 16/40  Iteration 4178/11040 Training loss: 1.3365 0.4763 sec/batch\n",
      "Epoch 16/40  Iteration 4179/11040 Training loss: 1.3356 0.4725 sec/batch\n",
      "Epoch 16/40  Iteration 4180/11040 Training loss: 1.3359 0.4740 sec/batch\n",
      "Epoch 16/40  Iteration 4181/11040 Training loss: 1.3354 0.4595 sec/batch\n",
      "Epoch 16/40  Iteration 4182/11040 Training loss: 1.3343 0.4752 sec/batch\n",
      "Epoch 16/40  Iteration 4183/11040 Training loss: 1.3346 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4184/11040 Training loss: 1.3342 0.4575 sec/batch\n",
      "Epoch 16/40  Iteration 4185/11040 Training loss: 1.3339 0.4752 sec/batch\n",
      "Epoch 16/40  Iteration 4186/11040 Training loss: 1.3334 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4187/11040 Training loss: 1.3327 0.4782 sec/batch\n",
      "Epoch 16/40  Iteration 4188/11040 Training loss: 1.3322 0.4721 sec/batch\n",
      "Epoch 16/40  Iteration 4189/11040 Training loss: 1.3316 0.4582 sec/batch\n",
      "Epoch 16/40  Iteration 4190/11040 Training loss: 1.3319 0.4608 sec/batch\n",
      "Epoch 16/40  Iteration 4191/11040 Training loss: 1.3317 0.4599 sec/batch\n",
      "Epoch 16/40  Iteration 4192/11040 Training loss: 1.3314 0.4584 sec/batch\n",
      "Epoch 16/40  Iteration 4193/11040 Training loss: 1.3317 0.4744 sec/batch\n",
      "Epoch 16/40  Iteration 4194/11040 Training loss: 1.3314 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4195/11040 Training loss: 1.3314 0.4608 sec/batch\n",
      "Epoch 16/40  Iteration 4196/11040 Training loss: 1.3313 0.4598 sec/batch\n",
      "Epoch 16/40  Iteration 4197/11040 Training loss: 1.3311 0.4589 sec/batch\n",
      "Epoch 16/40  Iteration 4198/11040 Training loss: 1.3309 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4199/11040 Training loss: 1.3312 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4200/11040 Training loss: 1.3307 0.4771 sec/batch\n",
      "Epoch 16/40  Iteration 4201/11040 Training loss: 1.3308 0.4586 sec/batch\n",
      "Epoch 16/40  Iteration 4202/11040 Training loss: 1.3304 0.4591 sec/batch\n",
      "Epoch 16/40  Iteration 4203/11040 Training loss: 1.3304 0.4591 sec/batch\n",
      "Epoch 16/40  Iteration 4204/11040 Training loss: 1.3302 0.4583 sec/batch\n",
      "Epoch 16/40  Iteration 4205/11040 Training loss: 1.3303 0.4739 sec/batch\n",
      "Epoch 16/40  Iteration 4206/11040 Training loss: 1.3301 0.4773 sec/batch\n",
      "Epoch 16/40  Iteration 4207/11040 Training loss: 1.3299 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4208/11040 Training loss: 1.3300 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4209/11040 Training loss: 1.3298 0.4740 sec/batch\n",
      "Epoch 16/40  Iteration 4210/11040 Training loss: 1.3298 0.4748 sec/batch\n",
      "Epoch 16/40  Iteration 4211/11040 Training loss: 1.3293 0.4603 sec/batch\n",
      "Epoch 16/40  Iteration 4212/11040 Training loss: 1.3287 0.4585 sec/batch\n",
      "Epoch 16/40  Iteration 4213/11040 Training loss: 1.3282 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4214/11040 Training loss: 1.3280 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4215/11040 Training loss: 1.3277 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4216/11040 Training loss: 1.3280 0.4753 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40  Iteration 4217/11040 Training loss: 1.3276 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4218/11040 Training loss: 1.3272 0.4706 sec/batch\n",
      "Epoch 16/40  Iteration 4219/11040 Training loss: 1.3270 0.4780 sec/batch\n",
      "Epoch 16/40  Iteration 4220/11040 Training loss: 1.3269 0.4613 sec/batch\n",
      "Epoch 16/40  Iteration 4221/11040 Training loss: 1.3267 0.4725 sec/batch\n",
      "Epoch 16/40  Iteration 4222/11040 Training loss: 1.3268 0.4738 sec/batch\n",
      "Epoch 16/40  Iteration 4223/11040 Training loss: 1.3267 0.4560 sec/batch\n",
      "Epoch 16/40  Iteration 4224/11040 Training loss: 1.3266 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4225/11040 Training loss: 1.3263 0.4752 sec/batch\n",
      "Epoch 16/40  Iteration 4226/11040 Training loss: 1.3263 0.4762 sec/batch\n",
      "Epoch 16/40  Iteration 4227/11040 Training loss: 1.3258 0.4574 sec/batch\n",
      "Epoch 16/40  Iteration 4228/11040 Training loss: 1.3249 0.4698 sec/batch\n",
      "Epoch 16/40  Iteration 4229/11040 Training loss: 1.3249 0.4792 sec/batch\n",
      "Epoch 16/40  Iteration 4230/11040 Training loss: 1.3251 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4231/11040 Training loss: 1.3254 0.4740 sec/batch\n",
      "Epoch 16/40  Iteration 4232/11040 Training loss: 1.3254 0.4759 sec/batch\n",
      "Epoch 16/40  Iteration 4233/11040 Training loss: 1.3252 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4234/11040 Training loss: 1.3255 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4235/11040 Training loss: 1.3260 0.4748 sec/batch\n",
      "Epoch 16/40  Iteration 4236/11040 Training loss: 1.3262 0.4596 sec/batch\n",
      "Epoch 16/40  Iteration 4237/11040 Training loss: 1.3267 0.4585 sec/batch\n",
      "Epoch 16/40  Iteration 4238/11040 Training loss: 1.3271 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4239/11040 Training loss: 1.3273 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4240/11040 Training loss: 1.3270 0.4744 sec/batch\n",
      "Epoch 16/40  Iteration 4241/11040 Training loss: 1.3271 0.4738 sec/batch\n",
      "Epoch 16/40  Iteration 4242/11040 Training loss: 1.3271 0.4610 sec/batch\n",
      "Epoch 16/40  Iteration 4243/11040 Training loss: 1.3266 0.4592 sec/batch\n",
      "Epoch 16/40  Iteration 4244/11040 Training loss: 1.3267 0.4742 sec/batch\n",
      "Epoch 16/40  Iteration 4245/11040 Training loss: 1.3266 0.4762 sec/batch\n",
      "Epoch 16/40  Iteration 4246/11040 Training loss: 1.3266 0.4728 sec/batch\n",
      "Epoch 16/40  Iteration 4247/11040 Training loss: 1.3267 0.4760 sec/batch\n",
      "Epoch 16/40  Iteration 4248/11040 Training loss: 1.3269 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4249/11040 Training loss: 1.3271 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4250/11040 Training loss: 1.3271 0.4801 sec/batch\n",
      "Epoch 16/40  Iteration 4251/11040 Training loss: 1.3272 0.4714 sec/batch\n",
      "Epoch 16/40  Iteration 4252/11040 Training loss: 1.3274 0.4577 sec/batch\n",
      "Epoch 16/40  Iteration 4253/11040 Training loss: 1.3276 0.4744 sec/batch\n",
      "Epoch 16/40  Iteration 4254/11040 Training loss: 1.3277 0.4801 sec/batch\n",
      "Epoch 16/40  Iteration 4255/11040 Training loss: 1.3281 0.4705 sec/batch\n",
      "Epoch 16/40  Iteration 4256/11040 Training loss: 1.3284 0.4763 sec/batch\n",
      "Epoch 16/40  Iteration 4257/11040 Training loss: 1.3284 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4258/11040 Training loss: 1.3288 0.4599 sec/batch\n",
      "Epoch 16/40  Iteration 4259/11040 Training loss: 1.3289 0.4748 sec/batch\n",
      "Epoch 16/40  Iteration 4260/11040 Training loss: 1.3291 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4261/11040 Training loss: 1.3292 0.4593 sec/batch\n",
      "Epoch 16/40  Iteration 4262/11040 Training loss: 1.3292 0.4764 sec/batch\n",
      "Epoch 16/40  Iteration 4263/11040 Training loss: 1.3295 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4264/11040 Training loss: 1.3299 0.4744 sec/batch\n",
      "Epoch 16/40  Iteration 4265/11040 Training loss: 1.3300 0.4596 sec/batch\n",
      "Epoch 16/40  Iteration 4266/11040 Training loss: 1.3303 0.4727 sec/batch\n",
      "Epoch 16/40  Iteration 4267/11040 Training loss: 1.3303 0.4774 sec/batch\n",
      "Epoch 16/40  Iteration 4268/11040 Training loss: 1.3303 0.4730 sec/batch\n",
      "Epoch 16/40  Iteration 4269/11040 Training loss: 1.3301 0.4603 sec/batch\n",
      "Epoch 16/40  Iteration 4270/11040 Training loss: 1.3304 0.4603 sec/batch\n",
      "Epoch 16/40  Iteration 4271/11040 Training loss: 1.3303 0.4572 sec/batch\n",
      "Epoch 16/40  Iteration 4272/11040 Training loss: 1.3300 0.4753 sec/batch\n",
      "Epoch 16/40  Iteration 4273/11040 Training loss: 1.3300 0.4757 sec/batch\n",
      "Epoch 16/40  Iteration 4274/11040 Training loss: 1.3299 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4275/11040 Training loss: 1.3301 0.4760 sec/batch\n",
      "Epoch 16/40  Iteration 4276/11040 Training loss: 1.3302 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4277/11040 Training loss: 1.3301 0.4822 sec/batch\n",
      "Epoch 16/40  Iteration 4278/11040 Training loss: 1.3302 0.4675 sec/batch\n",
      "Epoch 16/40  Iteration 4279/11040 Training loss: 1.3304 0.4596 sec/batch\n",
      "Epoch 16/40  Iteration 4280/11040 Training loss: 1.3305 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4281/11040 Training loss: 1.3306 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4282/11040 Training loss: 1.3307 0.4755 sec/batch\n",
      "Epoch 16/40  Iteration 4283/11040 Training loss: 1.3307 0.4710 sec/batch\n",
      "Epoch 16/40  Iteration 4284/11040 Training loss: 1.3305 0.4615 sec/batch\n",
      "Epoch 16/40  Iteration 4285/11040 Training loss: 1.3305 0.4757 sec/batch\n",
      "Epoch 16/40  Iteration 4286/11040 Training loss: 1.3305 0.4731 sec/batch\n",
      "Epoch 16/40  Iteration 4287/11040 Training loss: 1.3305 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4288/11040 Training loss: 1.3304 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4289/11040 Training loss: 1.3303 0.4737 sec/batch\n",
      "Epoch 16/40  Iteration 4290/11040 Training loss: 1.3303 0.4771 sec/batch\n",
      "Epoch 16/40  Iteration 4291/11040 Training loss: 1.3303 0.4723 sec/batch\n",
      "Epoch 16/40  Iteration 4292/11040 Training loss: 1.3304 0.4601 sec/batch\n",
      "Epoch 16/40  Iteration 4293/11040 Training loss: 1.3304 0.4593 sec/batch\n",
      "Epoch 16/40  Iteration 4294/11040 Training loss: 1.3305 0.4588 sec/batch\n",
      "Epoch 16/40  Iteration 4295/11040 Training loss: 1.3305 0.4604 sec/batch\n",
      "Epoch 16/40  Iteration 4296/11040 Training loss: 1.3305 0.4580 sec/batch\n",
      "Epoch 16/40  Iteration 4297/11040 Training loss: 1.3303 0.4597 sec/batch\n",
      "Epoch 16/40  Iteration 4298/11040 Training loss: 1.3305 0.4600 sec/batch\n",
      "Epoch 16/40  Iteration 4299/11040 Training loss: 1.3306 0.4726 sec/batch\n",
      "Epoch 16/40  Iteration 4300/11040 Training loss: 1.3306 0.4739 sec/batch\n",
      "Epoch 16/40  Iteration 4301/11040 Training loss: 1.3307 0.4777 sec/batch\n",
      "Epoch 16/40  Iteration 4302/11040 Training loss: 1.3306 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4303/11040 Training loss: 1.3307 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4304/11040 Training loss: 1.3307 0.4747 sec/batch\n",
      "Epoch 16/40  Iteration 4305/11040 Training loss: 1.3308 0.4745 sec/batch\n",
      "Epoch 16/40  Iteration 4306/11040 Training loss: 1.3310 0.4594 sec/batch\n",
      "Epoch 16/40  Iteration 4307/11040 Training loss: 1.3310 0.4582 sec/batch\n",
      "Epoch 16/40  Iteration 4308/11040 Training loss: 1.3309 0.4590 sec/batch\n",
      "Epoch 16/40  Iteration 4309/11040 Training loss: 1.3308 0.4593 sec/batch\n",
      "Epoch 16/40  Iteration 4310/11040 Training loss: 1.3307 0.4601 sec/batch\n",
      "Epoch 16/40  Iteration 4311/11040 Training loss: 1.3309 0.4575 sec/batch\n",
      "Epoch 16/40  Iteration 4312/11040 Training loss: 1.3310 0.4608 sec/batch\n",
      "Epoch 16/40  Iteration 4313/11040 Training loss: 1.3310 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4314/11040 Training loss: 1.3310 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4315/11040 Training loss: 1.3310 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4316/11040 Training loss: 1.3310 0.4749 sec/batch\n",
      "Epoch 16/40  Iteration 4317/11040 Training loss: 1.3311 0.4744 sec/batch\n",
      "Epoch 16/40  Iteration 4318/11040 Training loss: 1.3311 0.4760 sec/batch\n",
      "Epoch 16/40  Iteration 4319/11040 Training loss: 1.3313 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4320/11040 Training loss: 1.3315 0.4586 sec/batch\n",
      "Epoch 16/40  Iteration 4321/11040 Training loss: 1.3315 0.4605 sec/batch\n",
      "Epoch 16/40  Iteration 4322/11040 Training loss: 1.3315 0.4602 sec/batch\n",
      "Epoch 16/40  Iteration 4323/11040 Training loss: 1.3313 0.4713 sec/batch\n",
      "Epoch 16/40  Iteration 4324/11040 Training loss: 1.3315 0.4763 sec/batch\n",
      "Epoch 16/40  Iteration 4325/11040 Training loss: 1.3315 0.4740 sec/batch\n",
      "Epoch 16/40  Iteration 4326/11040 Training loss: 1.3316 0.4746 sec/batch\n",
      "Epoch 16/40  Iteration 4327/11040 Training loss: 1.3318 0.4735 sec/batch\n",
      "Epoch 16/40  Iteration 4328/11040 Training loss: 1.3319 0.4612 sec/batch\n",
      "Epoch 16/40  Iteration 4329/11040 Training loss: 1.3319 0.4745 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40  Iteration 4330/11040 Training loss: 1.3319 0.4588 sec/batch\n",
      "Epoch 16/40  Iteration 4331/11040 Training loss: 1.3317 0.4588 sec/batch\n",
      "Epoch 16/40  Iteration 4332/11040 Training loss: 1.3317 0.4597 sec/batch\n",
      "Epoch 16/40  Iteration 4333/11040 Training loss: 1.3317 0.4583 sec/batch\n",
      "Epoch 16/40  Iteration 4334/11040 Training loss: 1.3317 0.4604 sec/batch\n",
      "Epoch 16/40  Iteration 4335/11040 Training loss: 1.3319 0.4580 sec/batch\n",
      "Epoch 16/40  Iteration 4336/11040 Training loss: 1.3319 0.4602 sec/batch\n",
      "Epoch 16/40  Iteration 4337/11040 Training loss: 1.3320 0.4571 sec/batch\n",
      "Epoch 16/40  Iteration 4338/11040 Training loss: 1.3321 0.4748 sec/batch\n",
      "Epoch 16/40  Iteration 4339/11040 Training loss: 1.3322 0.4605 sec/batch\n",
      "Epoch 16/40  Iteration 4340/11040 Training loss: 1.3323 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4341/11040 Training loss: 1.3325 0.4742 sec/batch\n",
      "Epoch 16/40  Iteration 4342/11040 Training loss: 1.3327 0.4770 sec/batch\n",
      "Epoch 16/40  Iteration 4343/11040 Training loss: 1.3328 0.4763 sec/batch\n",
      "Epoch 16/40  Iteration 4344/11040 Training loss: 1.3330 0.4725 sec/batch\n",
      "Epoch 16/40  Iteration 4345/11040 Training loss: 1.3330 0.4583 sec/batch\n",
      "Epoch 16/40  Iteration 4346/11040 Training loss: 1.3330 0.4752 sec/batch\n",
      "Epoch 16/40  Iteration 4347/11040 Training loss: 1.3330 0.4595 sec/batch\n",
      "Epoch 16/40  Iteration 4348/11040 Training loss: 1.3330 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4349/11040 Training loss: 1.3329 0.4762 sec/batch\n",
      "Epoch 16/40  Iteration 4350/11040 Training loss: 1.3328 0.4734 sec/batch\n",
      "Epoch 16/40  Iteration 4351/11040 Training loss: 1.3328 0.4758 sec/batch\n",
      "Epoch 16/40  Iteration 4352/11040 Training loss: 1.3329 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4353/11040 Training loss: 1.3329 0.4757 sec/batch\n",
      "Epoch 16/40  Iteration 4354/11040 Training loss: 1.3328 0.4756 sec/batch\n",
      "Epoch 16/40  Iteration 4355/11040 Training loss: 1.3326 0.4736 sec/batch\n",
      "Epoch 16/40  Iteration 4356/11040 Training loss: 1.3326 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4357/11040 Training loss: 1.3324 0.4739 sec/batch\n",
      "Epoch 16/40  Iteration 4358/11040 Training loss: 1.3323 0.4612 sec/batch\n",
      "Epoch 16/40  Iteration 4359/11040 Training loss: 1.3322 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4360/11040 Training loss: 1.3321 0.4587 sec/batch\n",
      "Epoch 16/40  Iteration 4361/11040 Training loss: 1.3320 0.4594 sec/batch\n",
      "Epoch 16/40  Iteration 4362/11040 Training loss: 1.3318 0.4585 sec/batch\n",
      "Epoch 16/40  Iteration 4363/11040 Training loss: 1.3317 0.4745 sec/batch\n",
      "Epoch 16/40  Iteration 4364/11040 Training loss: 1.3318 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4365/11040 Training loss: 1.3318 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4366/11040 Training loss: 1.3318 0.4738 sec/batch\n",
      "Epoch 16/40  Iteration 4367/11040 Training loss: 1.3316 0.4762 sec/batch\n",
      "Epoch 16/40  Iteration 4368/11040 Training loss: 1.3315 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4369/11040 Training loss: 1.3315 0.4758 sec/batch\n",
      "Epoch 16/40  Iteration 4370/11040 Training loss: 1.3316 0.4720 sec/batch\n",
      "Epoch 16/40  Iteration 4371/11040 Training loss: 1.3316 0.4768 sec/batch\n",
      "Epoch 16/40  Iteration 4372/11040 Training loss: 1.3316 0.4741 sec/batch\n",
      "Epoch 16/40  Iteration 4373/11040 Training loss: 1.3315 0.4581 sec/batch\n",
      "Epoch 16/40  Iteration 4374/11040 Training loss: 1.3314 0.4739 sec/batch\n",
      "Epoch 16/40  Iteration 4375/11040 Training loss: 1.3315 0.4758 sec/batch\n",
      "Epoch 16/40  Iteration 4376/11040 Training loss: 1.3315 0.4742 sec/batch\n",
      "Epoch 16/40  Iteration 4377/11040 Training loss: 1.3315 0.4606 sec/batch\n",
      "Epoch 16/40  Iteration 4378/11040 Training loss: 1.3315 0.4727 sec/batch\n",
      "Epoch 16/40  Iteration 4379/11040 Training loss: 1.3315 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4380/11040 Training loss: 1.3316 0.4750 sec/batch\n",
      "Epoch 16/40  Iteration 4381/11040 Training loss: 1.3316 0.4766 sec/batch\n",
      "Epoch 16/40  Iteration 4382/11040 Training loss: 1.3316 0.4733 sec/batch\n",
      "Epoch 16/40  Iteration 4383/11040 Training loss: 1.3315 0.4761 sec/batch\n",
      "Epoch 16/40  Iteration 4384/11040 Training loss: 1.3314 0.4574 sec/batch\n",
      "Epoch 16/40  Iteration 4385/11040 Training loss: 1.3312 0.4600 sec/batch\n",
      "Epoch 16/40  Iteration 4386/11040 Training loss: 1.3312 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4387/11040 Training loss: 1.3312 0.4742 sec/batch\n",
      "Epoch 16/40  Iteration 4388/11040 Training loss: 1.3312 0.4772 sec/batch\n",
      "Epoch 16/40  Iteration 4389/11040 Training loss: 1.3311 0.4724 sec/batch\n",
      "Epoch 16/40  Iteration 4390/11040 Training loss: 1.3311 0.4751 sec/batch\n",
      "Epoch 16/40  Iteration 4391/11040 Training loss: 1.3310 0.4770 sec/batch\n",
      "Epoch 16/40  Iteration 4392/11040 Training loss: 1.3310 0.4837 sec/batch\n",
      "Epoch 16/40  Iteration 4393/11040 Training loss: 1.3310 0.4800 sec/batch\n",
      "Epoch 16/40  Iteration 4394/11040 Training loss: 1.3310 0.4759 sec/batch\n",
      "Epoch 16/40  Iteration 4395/11040 Training loss: 1.3310 0.4755 sec/batch\n",
      "Epoch 16/40  Iteration 4396/11040 Training loss: 1.3310 0.4722 sec/batch\n",
      "Epoch 16/40  Iteration 4397/11040 Training loss: 1.3311 0.4762 sec/batch\n",
      "Epoch 16/40  Iteration 4398/11040 Training loss: 1.3311 0.4742 sec/batch\n",
      "Epoch 16/40  Iteration 4399/11040 Training loss: 1.3311 0.4606 sec/batch\n",
      "Epoch 16/40  Iteration 4400/11040 Training loss: 1.3310 0.4739 sec/batch\n",
      "Epoch 16/40  Iteration 4401/11040 Training loss: 1.3311 0.4604 sec/batch\n",
      "Epoch 16/40  Iteration 4402/11040 Training loss: 1.3312 0.4732 sec/batch\n",
      "Epoch 16/40  Iteration 4403/11040 Training loss: 1.3312 0.4748 sec/batch\n",
      "Epoch 16/40  Iteration 4404/11040 Training loss: 1.3313 0.4608 sec/batch\n",
      "Epoch 16/40  Iteration 4405/11040 Training loss: 1.3313 0.4626 sec/batch\n",
      "Epoch 16/40  Iteration 4406/11040 Training loss: 1.3314 0.4703 sec/batch\n",
      "Epoch 16/40  Iteration 4407/11040 Training loss: 1.3315 0.4655 sec/batch\n",
      "Epoch 16/40  Iteration 4408/11040 Training loss: 1.3315 0.4684 sec/batch\n",
      "Epoch 16/40  Iteration 4409/11040 Training loss: 1.3316 0.4826 sec/batch\n",
      "Epoch 16/40  Iteration 4410/11040 Training loss: 1.3316 0.4665 sec/batch\n",
      "Epoch 16/40  Iteration 4411/11040 Training loss: 1.3319 0.4743 sec/batch\n",
      "Epoch 16/40  Iteration 4412/11040 Training loss: 1.3321 0.4586 sec/batch\n",
      "Epoch 16/40  Iteration 4413/11040 Training loss: 1.3322 0.4606 sec/batch\n",
      "Epoch 16/40  Iteration 4414/11040 Training loss: 1.3322 0.4580 sec/batch\n",
      "Epoch 16/40  Iteration 4415/11040 Training loss: 1.3322 0.4585 sec/batch\n",
      "Epoch 16/40  Iteration 4416/11040 Training loss: 1.3322 0.4593 sec/batch\n",
      "Epoch 17/40  Iteration 4417/11040 Training loss: 1.3940 0.4748 sec/batch\n",
      "Epoch 17/40  Iteration 4418/11040 Training loss: 1.3584 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4419/11040 Training loss: 1.3537 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4420/11040 Training loss: 1.3519 0.4749 sec/batch\n",
      "Epoch 17/40  Iteration 4421/11040 Training loss: 1.3533 0.4589 sec/batch\n",
      "Epoch 17/40  Iteration 4422/11040 Training loss: 1.3503 0.4744 sec/batch\n",
      "Epoch 17/40  Iteration 4423/11040 Training loss: 1.3439 0.4751 sec/batch\n",
      "Epoch 17/40  Iteration 4424/11040 Training loss: 1.3421 0.4755 sec/batch\n",
      "Epoch 17/40  Iteration 4425/11040 Training loss: 1.3384 0.4591 sec/batch\n",
      "Epoch 17/40  Iteration 4426/11040 Training loss: 1.3379 0.4755 sec/batch\n",
      "Epoch 17/40  Iteration 4427/11040 Training loss: 1.3351 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4428/11040 Training loss: 1.3347 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4429/11040 Training loss: 1.3326 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4430/11040 Training loss: 1.3308 0.4616 sec/batch\n",
      "Epoch 17/40  Iteration 4431/11040 Training loss: 1.3306 0.4587 sec/batch\n",
      "Epoch 17/40  Iteration 4432/11040 Training loss: 1.3299 0.4591 sec/batch\n",
      "Epoch 17/40  Iteration 4433/11040 Training loss: 1.3275 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4434/11040 Training loss: 1.3255 0.4752 sec/batch\n",
      "Epoch 17/40  Iteration 4435/11040 Training loss: 1.3245 0.4753 sec/batch\n",
      "Epoch 17/40  Iteration 4436/11040 Training loss: 1.3248 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4437/11040 Training loss: 1.3266 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4438/11040 Training loss: 1.3272 0.4759 sec/batch\n",
      "Epoch 17/40  Iteration 4439/11040 Training loss: 1.3269 0.4753 sec/batch\n",
      "Epoch 17/40  Iteration 4440/11040 Training loss: 1.3272 0.4590 sec/batch\n",
      "Epoch 17/40  Iteration 4441/11040 Training loss: 1.3265 0.4754 sec/batch\n",
      "Epoch 17/40  Iteration 4442/11040 Training loss: 1.3256 0.4744 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40  Iteration 4443/11040 Training loss: 1.3250 0.4598 sec/batch\n",
      "Epoch 17/40  Iteration 4444/11040 Training loss: 1.3250 0.4731 sec/batch\n",
      "Epoch 17/40  Iteration 4445/11040 Training loss: 1.3250 0.4749 sec/batch\n",
      "Epoch 17/40  Iteration 4446/11040 Training loss: 1.3251 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4447/11040 Training loss: 1.3248 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4448/11040 Training loss: 1.3246 0.4764 sec/batch\n",
      "Epoch 17/40  Iteration 4449/11040 Training loss: 1.3240 0.4743 sec/batch\n",
      "Epoch 17/40  Iteration 4450/11040 Training loss: 1.3234 0.4751 sec/batch\n",
      "Epoch 17/40  Iteration 4451/11040 Training loss: 1.3234 0.4723 sec/batch\n",
      "Epoch 17/40  Iteration 4452/11040 Training loss: 1.3240 0.4606 sec/batch\n",
      "Epoch 17/40  Iteration 4453/11040 Training loss: 1.3239 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4454/11040 Training loss: 1.3231 0.4744 sec/batch\n",
      "Epoch 17/40  Iteration 4455/11040 Training loss: 1.3223 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4456/11040 Training loss: 1.3222 0.4721 sec/batch\n",
      "Epoch 17/40  Iteration 4457/11040 Training loss: 1.3217 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4458/11040 Training loss: 1.3205 0.4606 sec/batch\n",
      "Epoch 17/40  Iteration 4459/11040 Training loss: 1.3210 0.4604 sec/batch\n",
      "Epoch 17/40  Iteration 4460/11040 Training loss: 1.3207 0.4728 sec/batch\n",
      "Epoch 17/40  Iteration 4461/11040 Training loss: 1.3203 0.4751 sec/batch\n",
      "Epoch 17/40  Iteration 4462/11040 Training loss: 1.3200 0.4790 sec/batch\n",
      "Epoch 17/40  Iteration 4463/11040 Training loss: 1.3192 0.4714 sec/batch\n",
      "Epoch 17/40  Iteration 4464/11040 Training loss: 1.3190 0.4738 sec/batch\n",
      "Epoch 17/40  Iteration 4465/11040 Training loss: 1.3184 0.4756 sec/batch\n",
      "Epoch 17/40  Iteration 4466/11040 Training loss: 1.3186 0.4755 sec/batch\n",
      "Epoch 17/40  Iteration 4467/11040 Training loss: 1.3183 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4468/11040 Training loss: 1.3181 0.4605 sec/batch\n",
      "Epoch 17/40  Iteration 4469/11040 Training loss: 1.3182 0.4702 sec/batch\n",
      "Epoch 17/40  Iteration 4470/11040 Training loss: 1.3180 0.4659 sec/batch\n",
      "Epoch 17/40  Iteration 4471/11040 Training loss: 1.3180 0.4680 sec/batch\n",
      "Epoch 17/40  Iteration 4472/11040 Training loss: 1.3180 0.4757 sec/batch\n",
      "Epoch 17/40  Iteration 4473/11040 Training loss: 1.3177 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4474/11040 Training loss: 1.3176 0.4724 sec/batch\n",
      "Epoch 17/40  Iteration 4475/11040 Training loss: 1.3180 0.4722 sec/batch\n",
      "Epoch 17/40  Iteration 4476/11040 Training loss: 1.3177 0.4694 sec/batch\n",
      "Epoch 17/40  Iteration 4477/11040 Training loss: 1.3177 0.4627 sec/batch\n",
      "Epoch 17/40  Iteration 4478/11040 Training loss: 1.3175 0.4744 sec/batch\n",
      "Epoch 17/40  Iteration 4479/11040 Training loss: 1.3174 0.4823 sec/batch\n",
      "Epoch 17/40  Iteration 4480/11040 Training loss: 1.3172 0.4677 sec/batch\n",
      "Epoch 17/40  Iteration 4481/11040 Training loss: 1.3173 0.4706 sec/batch\n",
      "Epoch 17/40  Iteration 4482/11040 Training loss: 1.3170 0.4631 sec/batch\n",
      "Epoch 17/40  Iteration 4483/11040 Training loss: 1.3168 0.4743 sec/batch\n",
      "Epoch 17/40  Iteration 4484/11040 Training loss: 1.3168 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4485/11040 Training loss: 1.3166 0.4782 sec/batch\n",
      "Epoch 17/40  Iteration 4486/11040 Training loss: 1.3166 0.4722 sec/batch\n",
      "Epoch 17/40  Iteration 4487/11040 Training loss: 1.3161 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4488/11040 Training loss: 1.3156 0.4579 sec/batch\n",
      "Epoch 17/40  Iteration 4489/11040 Training loss: 1.3151 0.4589 sec/batch\n",
      "Epoch 17/40  Iteration 4490/11040 Training loss: 1.3147 0.4569 sec/batch\n",
      "Epoch 17/40  Iteration 4491/11040 Training loss: 1.3145 0.4576 sec/batch\n",
      "Epoch 17/40  Iteration 4492/11040 Training loss: 1.3147 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4493/11040 Training loss: 1.3143 0.4746 sec/batch\n",
      "Epoch 17/40  Iteration 4494/11040 Training loss: 1.3140 0.4598 sec/batch\n",
      "Epoch 17/40  Iteration 4495/11040 Training loss: 1.3137 0.4594 sec/batch\n",
      "Epoch 17/40  Iteration 4496/11040 Training loss: 1.3136 0.4576 sec/batch\n",
      "Epoch 17/40  Iteration 4497/11040 Training loss: 1.3133 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4498/11040 Training loss: 1.3134 0.4744 sec/batch\n",
      "Epoch 17/40  Iteration 4499/11040 Training loss: 1.3134 0.4735 sec/batch\n",
      "Epoch 17/40  Iteration 4500/11040 Training loss: 1.3131 0.4622 sec/batch\n",
      "Validation loss: 1.29838 Saving checkpoint!\n",
      "Epoch 17/40  Iteration 4501/11040 Training loss: 1.3144 0.4689 sec/batch\n",
      "Epoch 17/40  Iteration 4502/11040 Training loss: 1.3143 0.4628 sec/batch\n",
      "Epoch 17/40  Iteration 4503/11040 Training loss: 1.3139 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4504/11040 Training loss: 1.3130 0.4608 sec/batch\n",
      "Epoch 17/40  Iteration 4505/11040 Training loss: 1.3131 0.4728 sec/batch\n",
      "Epoch 17/40  Iteration 4506/11040 Training loss: 1.3133 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4507/11040 Training loss: 1.3135 0.4612 sec/batch\n",
      "Epoch 17/40  Iteration 4508/11040 Training loss: 1.3135 0.4716 sec/batch\n",
      "Epoch 17/40  Iteration 4509/11040 Training loss: 1.3133 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4510/11040 Training loss: 1.3137 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4511/11040 Training loss: 1.3142 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4512/11040 Training loss: 1.3144 0.4603 sec/batch\n",
      "Epoch 17/40  Iteration 4513/11040 Training loss: 1.3148 0.4591 sec/batch\n",
      "Epoch 17/40  Iteration 4514/11040 Training loss: 1.3150 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4515/11040 Training loss: 1.3152 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4516/11040 Training loss: 1.3150 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4517/11040 Training loss: 1.3151 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4518/11040 Training loss: 1.3152 0.4749 sec/batch\n",
      "Epoch 17/40  Iteration 4519/11040 Training loss: 1.3148 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4520/11040 Training loss: 1.3148 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4521/11040 Training loss: 1.3147 0.4607 sec/batch\n",
      "Epoch 17/40  Iteration 4522/11040 Training loss: 1.3146 0.4591 sec/batch\n",
      "Epoch 17/40  Iteration 4523/11040 Training loss: 1.3146 0.4731 sec/batch\n",
      "Epoch 17/40  Iteration 4524/11040 Training loss: 1.3148 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4525/11040 Training loss: 1.3150 0.4728 sec/batch\n",
      "Epoch 17/40  Iteration 4526/11040 Training loss: 1.3150 0.4759 sec/batch\n",
      "Epoch 17/40  Iteration 4527/11040 Training loss: 1.3151 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4528/11040 Training loss: 1.3154 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4529/11040 Training loss: 1.3155 0.4735 sec/batch\n",
      "Epoch 17/40  Iteration 4530/11040 Training loss: 1.3156 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4531/11040 Training loss: 1.3160 0.4585 sec/batch\n",
      "Epoch 17/40  Iteration 4532/11040 Training loss: 1.3163 0.4592 sec/batch\n",
      "Epoch 17/40  Iteration 4533/11040 Training loss: 1.3162 0.4594 sec/batch\n",
      "Epoch 17/40  Iteration 4534/11040 Training loss: 1.3165 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4535/11040 Training loss: 1.3166 0.4773 sec/batch\n",
      "Epoch 17/40  Iteration 4536/11040 Training loss: 1.3168 0.4722 sec/batch\n",
      "Epoch 17/40  Iteration 4537/11040 Training loss: 1.3168 0.4748 sec/batch\n",
      "Epoch 17/40  Iteration 4538/11040 Training loss: 1.3168 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4539/11040 Training loss: 1.3171 0.4771 sec/batch\n",
      "Epoch 17/40  Iteration 4540/11040 Training loss: 1.3176 0.4638 sec/batch\n",
      "Epoch 17/40  Iteration 4541/11040 Training loss: 1.3176 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4542/11040 Training loss: 1.3179 0.4674 sec/batch\n",
      "Epoch 17/40  Iteration 4543/11040 Training loss: 1.3179 0.4586 sec/batch\n",
      "Epoch 17/40  Iteration 4544/11040 Training loss: 1.3180 0.4603 sec/batch\n",
      "Epoch 17/40  Iteration 4545/11040 Training loss: 1.3178 0.4575 sec/batch\n",
      "Epoch 17/40  Iteration 4546/11040 Training loss: 1.3180 0.4749 sec/batch\n",
      "Epoch 17/40  Iteration 4547/11040 Training loss: 1.3178 0.4740 sec/batch\n",
      "Epoch 17/40  Iteration 4548/11040 Training loss: 1.3176 0.4752 sec/batch\n",
      "Epoch 17/40  Iteration 4549/11040 Training loss: 1.3177 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4550/11040 Training loss: 1.3176 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4551/11040 Training loss: 1.3178 0.4771 sec/batch\n",
      "Epoch 17/40  Iteration 4552/11040 Training loss: 1.3180 0.4729 sec/batch\n",
      "Epoch 17/40  Iteration 4553/11040 Training loss: 1.3179 0.4759 sec/batch\n",
      "Epoch 17/40  Iteration 4554/11040 Training loss: 1.3181 0.4730 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40  Iteration 4555/11040 Training loss: 1.3183 0.4607 sec/batch\n",
      "Epoch 17/40  Iteration 4556/11040 Training loss: 1.3183 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4557/11040 Training loss: 1.3183 0.4606 sec/batch\n",
      "Epoch 17/40  Iteration 4558/11040 Training loss: 1.3184 0.4717 sec/batch\n",
      "Epoch 17/40  Iteration 4559/11040 Training loss: 1.3184 0.4627 sec/batch\n",
      "Epoch 17/40  Iteration 4560/11040 Training loss: 1.3182 0.4576 sec/batch\n",
      "Epoch 17/40  Iteration 4561/11040 Training loss: 1.3182 0.4606 sec/batch\n",
      "Epoch 17/40  Iteration 4562/11040 Training loss: 1.3182 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4563/11040 Training loss: 1.3182 0.4759 sec/batch\n",
      "Epoch 17/40  Iteration 4564/11040 Training loss: 1.3181 0.4587 sec/batch\n",
      "Epoch 17/40  Iteration 4565/11040 Training loss: 1.3179 0.4597 sec/batch\n",
      "Epoch 17/40  Iteration 4566/11040 Training loss: 1.3179 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4567/11040 Training loss: 1.3179 0.4604 sec/batch\n",
      "Epoch 17/40  Iteration 4568/11040 Training loss: 1.3180 0.4583 sec/batch\n",
      "Epoch 17/40  Iteration 4569/11040 Training loss: 1.3179 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4570/11040 Training loss: 1.3180 0.4748 sec/batch\n",
      "Epoch 17/40  Iteration 4571/11040 Training loss: 1.3181 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4572/11040 Training loss: 1.3180 0.4753 sec/batch\n",
      "Epoch 17/40  Iteration 4573/11040 Training loss: 1.3179 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4574/11040 Training loss: 1.3180 0.4762 sec/batch\n",
      "Epoch 17/40  Iteration 4575/11040 Training loss: 1.3181 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4576/11040 Training loss: 1.3181 0.4731 sec/batch\n",
      "Epoch 17/40  Iteration 4577/11040 Training loss: 1.3182 0.4614 sec/batch\n",
      "Epoch 17/40  Iteration 4578/11040 Training loss: 1.3181 0.4582 sec/batch\n",
      "Epoch 17/40  Iteration 4579/11040 Training loss: 1.3182 0.4744 sec/batch\n",
      "Epoch 17/40  Iteration 4580/11040 Training loss: 1.3183 0.4752 sec/batch\n",
      "Epoch 17/40  Iteration 4581/11040 Training loss: 1.3184 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4582/11040 Training loss: 1.3187 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4583/11040 Training loss: 1.3186 0.4582 sec/batch\n",
      "Epoch 17/40  Iteration 4584/11040 Training loss: 1.3185 0.4598 sec/batch\n",
      "Epoch 17/40  Iteration 4585/11040 Training loss: 1.3184 0.4585 sec/batch\n",
      "Epoch 17/40  Iteration 4586/11040 Training loss: 1.3182 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4587/11040 Training loss: 1.3184 0.4772 sec/batch\n",
      "Epoch 17/40  Iteration 4588/11040 Training loss: 1.3186 0.4743 sec/batch\n",
      "Epoch 17/40  Iteration 4589/11040 Training loss: 1.3186 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4590/11040 Training loss: 1.3185 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4591/11040 Training loss: 1.3185 0.4771 sec/batch\n",
      "Epoch 17/40  Iteration 4592/11040 Training loss: 1.3185 0.4720 sec/batch\n",
      "Epoch 17/40  Iteration 4593/11040 Training loss: 1.3186 0.4604 sec/batch\n",
      "Epoch 17/40  Iteration 4594/11040 Training loss: 1.3186 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4595/11040 Training loss: 1.3188 0.4774 sec/batch\n",
      "Epoch 17/40  Iteration 4596/11040 Training loss: 1.3189 0.4722 sec/batch\n",
      "Epoch 17/40  Iteration 4597/11040 Training loss: 1.3189 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4598/11040 Training loss: 1.3189 0.4731 sec/batch\n",
      "Epoch 17/40  Iteration 4599/11040 Training loss: 1.3188 0.4601 sec/batch\n",
      "Epoch 17/40  Iteration 4600/11040 Training loss: 1.3189 0.4745 sec/batch\n",
      "Epoch 17/40  Iteration 4601/11040 Training loss: 1.3189 0.4751 sec/batch\n",
      "Epoch 17/40  Iteration 4602/11040 Training loss: 1.3190 0.4733 sec/batch\n",
      "Epoch 17/40  Iteration 4603/11040 Training loss: 1.3192 0.4740 sec/batch\n",
      "Epoch 17/40  Iteration 4604/11040 Training loss: 1.3192 0.4769 sec/batch\n",
      "Epoch 17/40  Iteration 4605/11040 Training loss: 1.3193 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4606/11040 Training loss: 1.3192 0.4751 sec/batch\n",
      "Epoch 17/40  Iteration 4607/11040 Training loss: 1.3191 0.4753 sec/batch\n",
      "Epoch 17/40  Iteration 4608/11040 Training loss: 1.3191 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4609/11040 Training loss: 1.3191 0.4719 sec/batch\n",
      "Epoch 17/40  Iteration 4610/11040 Training loss: 1.3191 0.4604 sec/batch\n",
      "Epoch 17/40  Iteration 4611/11040 Training loss: 1.3194 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4612/11040 Training loss: 1.3193 0.4746 sec/batch\n",
      "Epoch 17/40  Iteration 4613/11040 Training loss: 1.3195 0.4737 sec/batch\n",
      "Epoch 17/40  Iteration 4614/11040 Training loss: 1.3195 0.4587 sec/batch\n",
      "Epoch 17/40  Iteration 4615/11040 Training loss: 1.3196 0.4603 sec/batch\n",
      "Epoch 17/40  Iteration 4616/11040 Training loss: 1.3197 0.4758 sec/batch\n",
      "Epoch 17/40  Iteration 4617/11040 Training loss: 1.3199 0.4723 sec/batch\n",
      "Epoch 17/40  Iteration 4618/11040 Training loss: 1.3200 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4619/11040 Training loss: 1.3202 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4620/11040 Training loss: 1.3204 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4621/11040 Training loss: 1.3204 0.4752 sec/batch\n",
      "Epoch 17/40  Iteration 4622/11040 Training loss: 1.3203 0.4737 sec/batch\n",
      "Epoch 17/40  Iteration 4623/11040 Training loss: 1.3203 0.4760 sec/batch\n",
      "Epoch 17/40  Iteration 4624/11040 Training loss: 1.3204 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4625/11040 Training loss: 1.3202 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4626/11040 Training loss: 1.3202 0.4775 sec/batch\n",
      "Epoch 17/40  Iteration 4627/11040 Training loss: 1.3202 0.4728 sec/batch\n",
      "Epoch 17/40  Iteration 4628/11040 Training loss: 1.3202 0.4738 sec/batch\n",
      "Epoch 17/40  Iteration 4629/11040 Training loss: 1.3202 0.4775 sec/batch\n",
      "Epoch 17/40  Iteration 4630/11040 Training loss: 1.3201 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4631/11040 Training loss: 1.3200 0.4755 sec/batch\n",
      "Epoch 17/40  Iteration 4632/11040 Training loss: 1.3200 0.4735 sec/batch\n",
      "Epoch 17/40  Iteration 4633/11040 Training loss: 1.3198 0.4643 sec/batch\n",
      "Epoch 17/40  Iteration 4634/11040 Training loss: 1.3197 0.4732 sec/batch\n",
      "Epoch 17/40  Iteration 4635/11040 Training loss: 1.3196 0.4570 sec/batch\n",
      "Epoch 17/40  Iteration 4636/11040 Training loss: 1.3196 0.4598 sec/batch\n",
      "Epoch 17/40  Iteration 4637/11040 Training loss: 1.3195 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4638/11040 Training loss: 1.3193 0.4748 sec/batch\n",
      "Epoch 17/40  Iteration 4639/11040 Training loss: 1.3192 0.4738 sec/batch\n",
      "Epoch 17/40  Iteration 4640/11040 Training loss: 1.3193 0.4753 sec/batch\n",
      "Epoch 17/40  Iteration 4641/11040 Training loss: 1.3192 0.4766 sec/batch\n",
      "Epoch 17/40  Iteration 4642/11040 Training loss: 1.3192 0.4733 sec/batch\n",
      "Epoch 17/40  Iteration 4643/11040 Training loss: 1.3190 0.4749 sec/batch\n",
      "Epoch 17/40  Iteration 4644/11040 Training loss: 1.3189 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4645/11040 Training loss: 1.3190 0.4740 sec/batch\n",
      "Epoch 17/40  Iteration 4646/11040 Training loss: 1.3190 0.4604 sec/batch\n",
      "Epoch 17/40  Iteration 4647/11040 Training loss: 1.3190 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4648/11040 Training loss: 1.3190 0.4748 sec/batch\n",
      "Epoch 17/40  Iteration 4649/11040 Training loss: 1.3189 0.4746 sec/batch\n",
      "Epoch 17/40  Iteration 4650/11040 Training loss: 1.3187 0.4723 sec/batch\n",
      "Epoch 17/40  Iteration 4651/11040 Training loss: 1.3188 0.4593 sec/batch\n",
      "Epoch 17/40  Iteration 4652/11040 Training loss: 1.3188 0.4575 sec/batch\n",
      "Epoch 17/40  Iteration 4653/11040 Training loss: 1.3188 0.4599 sec/batch\n",
      "Epoch 17/40  Iteration 4654/11040 Training loss: 1.3189 0.4693 sec/batch\n",
      "Epoch 17/40  Iteration 4655/11040 Training loss: 1.3190 0.4796 sec/batch\n",
      "Epoch 17/40  Iteration 4656/11040 Training loss: 1.3190 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4657/11040 Training loss: 1.3191 0.4623 sec/batch\n",
      "Epoch 17/40  Iteration 4658/11040 Training loss: 1.3190 0.4723 sec/batch\n",
      "Epoch 17/40  Iteration 4659/11040 Training loss: 1.3190 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4660/11040 Training loss: 1.3189 0.4603 sec/batch\n",
      "Epoch 17/40  Iteration 4661/11040 Training loss: 1.3187 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4662/11040 Training loss: 1.3187 0.4755 sec/batch\n",
      "Epoch 17/40  Iteration 4663/11040 Training loss: 1.3187 0.4742 sec/batch\n",
      "Epoch 17/40  Iteration 4664/11040 Training loss: 1.3185 0.4757 sec/batch\n",
      "Epoch 17/40  Iteration 4665/11040 Training loss: 1.3185 0.4729 sec/batch\n",
      "Epoch 17/40  Iteration 4666/11040 Training loss: 1.3185 0.4761 sec/batch\n",
      "Epoch 17/40  Iteration 4667/11040 Training loss: 1.3184 0.4731 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40  Iteration 4668/11040 Training loss: 1.3184 0.4618 sec/batch\n",
      "Epoch 17/40  Iteration 4669/11040 Training loss: 1.3184 0.4575 sec/batch\n",
      "Epoch 17/40  Iteration 4670/11040 Training loss: 1.3184 0.4593 sec/batch\n",
      "Epoch 17/40  Iteration 4671/11040 Training loss: 1.3184 0.4584 sec/batch\n",
      "Epoch 17/40  Iteration 4672/11040 Training loss: 1.3183 0.4594 sec/batch\n",
      "Epoch 17/40  Iteration 4673/11040 Training loss: 1.3184 0.4589 sec/batch\n",
      "Epoch 17/40  Iteration 4674/11040 Training loss: 1.3184 0.4599 sec/batch\n",
      "Epoch 17/40  Iteration 4675/11040 Training loss: 1.3183 0.4595 sec/batch\n",
      "Epoch 17/40  Iteration 4676/11040 Training loss: 1.3183 0.4730 sec/batch\n",
      "Epoch 17/40  Iteration 4677/11040 Training loss: 1.3183 0.4617 sec/batch\n",
      "Epoch 17/40  Iteration 4678/11040 Training loss: 1.3184 0.4734 sec/batch\n",
      "Epoch 17/40  Iteration 4679/11040 Training loss: 1.3185 0.4713 sec/batch\n",
      "Epoch 17/40  Iteration 4680/11040 Training loss: 1.3186 0.4585 sec/batch\n",
      "Epoch 17/40  Iteration 4681/11040 Training loss: 1.3186 0.4755 sec/batch\n",
      "Epoch 17/40  Iteration 4682/11040 Training loss: 1.3187 0.4595 sec/batch\n",
      "Epoch 17/40  Iteration 4683/11040 Training loss: 1.3188 0.4739 sec/batch\n",
      "Epoch 17/40  Iteration 4684/11040 Training loss: 1.3188 0.4616 sec/batch\n",
      "Epoch 17/40  Iteration 4685/11040 Training loss: 1.3189 0.4722 sec/batch\n",
      "Epoch 17/40  Iteration 4686/11040 Training loss: 1.3190 0.4758 sec/batch\n",
      "Epoch 17/40  Iteration 4687/11040 Training loss: 1.3193 0.4731 sec/batch\n",
      "Epoch 17/40  Iteration 4688/11040 Training loss: 1.3195 0.4758 sec/batch\n",
      "Epoch 17/40  Iteration 4689/11040 Training loss: 1.3196 0.4741 sec/batch\n",
      "Epoch 17/40  Iteration 4690/11040 Training loss: 1.3196 0.4587 sec/batch\n",
      "Epoch 17/40  Iteration 4691/11040 Training loss: 1.3195 0.4750 sec/batch\n",
      "Epoch 17/40  Iteration 4692/11040 Training loss: 1.3196 0.4842 sec/batch\n",
      "Epoch 18/40  Iteration 4693/11040 Training loss: 1.3853 0.4662 sec/batch\n",
      "Epoch 18/40  Iteration 4694/11040 Training loss: 1.3468 0.4595 sec/batch\n",
      "Epoch 18/40  Iteration 4695/11040 Training loss: 1.3401 0.4728 sec/batch\n",
      "Epoch 18/40  Iteration 4696/11040 Training loss: 1.3411 0.4662 sec/batch\n",
      "Epoch 18/40  Iteration 4697/11040 Training loss: 1.3423 0.4695 sec/batch\n",
      "Epoch 18/40  Iteration 4698/11040 Training loss: 1.3391 0.4747 sec/batch\n",
      "Epoch 18/40  Iteration 4699/11040 Training loss: 1.3311 0.4745 sec/batch\n",
      "Epoch 18/40  Iteration 4700/11040 Training loss: 1.3291 0.4845 sec/batch\n",
      "Epoch 18/40  Iteration 4701/11040 Training loss: 1.3252 0.4615 sec/batch\n",
      "Epoch 18/40  Iteration 4702/11040 Training loss: 1.3253 0.4795 sec/batch\n",
      "Epoch 18/40  Iteration 4703/11040 Training loss: 1.3224 0.4694 sec/batch\n",
      "Epoch 18/40  Iteration 4704/11040 Training loss: 1.3217 0.4744 sec/batch\n",
      "Epoch 18/40  Iteration 4705/11040 Training loss: 1.3182 0.4621 sec/batch\n",
      "Epoch 18/40  Iteration 4706/11040 Training loss: 1.3162 0.4835 sec/batch\n",
      "Epoch 18/40  Iteration 4707/11040 Training loss: 1.3159 0.4654 sec/batch\n",
      "Epoch 18/40  Iteration 4708/11040 Training loss: 1.3161 0.4588 sec/batch\n",
      "Epoch 18/40  Iteration 4709/11040 Training loss: 1.3137 0.4742 sec/batch\n",
      "Epoch 18/40  Iteration 4710/11040 Training loss: 1.3121 0.4717 sec/batch\n",
      "Epoch 18/40  Iteration 4711/11040 Training loss: 1.3115 0.4613 sec/batch\n",
      "Epoch 18/40  Iteration 4712/11040 Training loss: 1.3119 0.4739 sec/batch\n",
      "Epoch 18/40  Iteration 4713/11040 Training loss: 1.3132 0.4758 sec/batch\n",
      "Epoch 18/40  Iteration 4714/11040 Training loss: 1.3140 0.4602 sec/batch\n",
      "Epoch 18/40  Iteration 4715/11040 Training loss: 1.3135 0.4585 sec/batch\n",
      "Epoch 18/40  Iteration 4716/11040 Training loss: 1.3138 0.4743 sec/batch\n",
      "Epoch 18/40  Iteration 4717/11040 Training loss: 1.3128 0.4753 sec/batch\n",
      "Epoch 18/40  Iteration 4718/11040 Training loss: 1.3123 0.4606 sec/batch\n",
      "Epoch 18/40  Iteration 4719/11040 Training loss: 1.3120 0.4589 sec/batch\n",
      "Epoch 18/40  Iteration 4720/11040 Training loss: 1.3117 0.4777 sec/batch\n",
      "Epoch 18/40  Iteration 4721/11040 Training loss: 1.3119 0.4724 sec/batch\n",
      "Epoch 18/40  Iteration 4722/11040 Training loss: 1.3120 0.4584 sec/batch\n",
      "Epoch 18/40  Iteration 4723/11040 Training loss: 1.3113 0.4739 sec/batch\n",
      "Epoch 18/40  Iteration 4724/11040 Training loss: 1.3112 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4725/11040 Training loss: 1.3105 0.4747 sec/batch\n",
      "Epoch 18/40  Iteration 4726/11040 Training loss: 1.3102 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4727/11040 Training loss: 1.3102 0.4730 sec/batch\n",
      "Epoch 18/40  Iteration 4728/11040 Training loss: 1.3106 0.4601 sec/batch\n",
      "Epoch 18/40  Iteration 4729/11040 Training loss: 1.3103 0.4593 sec/batch\n",
      "Epoch 18/40  Iteration 4730/11040 Training loss: 1.3093 0.4603 sec/batch\n",
      "Epoch 18/40  Iteration 4731/11040 Training loss: 1.3086 0.4738 sec/batch\n",
      "Epoch 18/40  Iteration 4732/11040 Training loss: 1.3088 0.4740 sec/batch\n",
      "Epoch 18/40  Iteration 4733/11040 Training loss: 1.3081 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4734/11040 Training loss: 1.3071 0.4587 sec/batch\n",
      "Epoch 18/40  Iteration 4735/11040 Training loss: 1.3074 0.4595 sec/batch\n",
      "Epoch 18/40  Iteration 4736/11040 Training loss: 1.3068 0.4756 sec/batch\n",
      "Epoch 18/40  Iteration 4737/11040 Training loss: 1.3065 0.4582 sec/batch\n",
      "Epoch 18/40  Iteration 4738/11040 Training loss: 1.3060 0.4595 sec/batch\n",
      "Epoch 18/40  Iteration 4739/11040 Training loss: 1.3052 0.4623 sec/batch\n",
      "Epoch 18/40  Iteration 4740/11040 Training loss: 1.3048 0.4725 sec/batch\n",
      "Epoch 18/40  Iteration 4741/11040 Training loss: 1.3043 0.4723 sec/batch\n",
      "Epoch 18/40  Iteration 4742/11040 Training loss: 1.3047 0.4758 sec/batch\n",
      "Epoch 18/40  Iteration 4743/11040 Training loss: 1.3045 0.4749 sec/batch\n",
      "Epoch 18/40  Iteration 4744/11040 Training loss: 1.3041 0.4746 sec/batch\n",
      "Epoch 18/40  Iteration 4745/11040 Training loss: 1.3043 0.4725 sec/batch\n",
      "Epoch 18/40  Iteration 4746/11040 Training loss: 1.3039 0.4765 sec/batch\n",
      "Epoch 18/40  Iteration 4747/11040 Training loss: 1.3038 0.4755 sec/batch\n",
      "Epoch 18/40  Iteration 4748/11040 Training loss: 1.3040 0.4718 sec/batch\n",
      "Epoch 18/40  Iteration 4749/11040 Training loss: 1.3038 0.4764 sec/batch\n",
      "Epoch 18/40  Iteration 4750/11040 Training loss: 1.3038 0.4730 sec/batch\n",
      "Epoch 18/40  Iteration 4751/11040 Training loss: 1.3041 0.4761 sec/batch\n",
      "Epoch 18/40  Iteration 4752/11040 Training loss: 1.3038 0.4749 sec/batch\n",
      "Epoch 18/40  Iteration 4753/11040 Training loss: 1.3038 0.4750 sec/batch\n",
      "Epoch 18/40  Iteration 4754/11040 Training loss: 1.3035 0.4723 sec/batch\n",
      "Epoch 18/40  Iteration 4755/11040 Training loss: 1.3034 0.4603 sec/batch\n",
      "Epoch 18/40  Iteration 4756/11040 Training loss: 1.3033 0.4601 sec/batch\n",
      "Epoch 18/40  Iteration 4757/11040 Training loss: 1.3035 0.4737 sec/batch\n",
      "Epoch 18/40  Iteration 4758/11040 Training loss: 1.3031 0.4748 sec/batch\n",
      "Epoch 18/40  Iteration 4759/11040 Training loss: 1.3030 0.4769 sec/batch\n",
      "Epoch 18/40  Iteration 4760/11040 Training loss: 1.3030 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4761/11040 Training loss: 1.3028 0.4602 sec/batch\n",
      "Epoch 18/40  Iteration 4762/11040 Training loss: 1.3028 0.4606 sec/batch\n",
      "Epoch 18/40  Iteration 4763/11040 Training loss: 1.3024 0.4571 sec/batch\n",
      "Epoch 18/40  Iteration 4764/11040 Training loss: 1.3019 0.4735 sec/batch\n",
      "Epoch 18/40  Iteration 4765/11040 Training loss: 1.3014 0.4618 sec/batch\n",
      "Epoch 18/40  Iteration 4766/11040 Training loss: 1.3011 0.4575 sec/batch\n",
      "Epoch 18/40  Iteration 4767/11040 Training loss: 1.3010 0.4747 sec/batch\n",
      "Epoch 18/40  Iteration 4768/11040 Training loss: 1.3013 0.4742 sec/batch\n",
      "Epoch 18/40  Iteration 4769/11040 Training loss: 1.3009 0.4750 sec/batch\n",
      "Epoch 18/40  Iteration 4770/11040 Training loss: 1.3006 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4771/11040 Training loss: 1.3004 0.4750 sec/batch\n",
      "Epoch 18/40  Iteration 4772/11040 Training loss: 1.3004 0.4753 sec/batch\n",
      "Epoch 18/40  Iteration 4773/11040 Training loss: 1.3002 0.4744 sec/batch\n",
      "Epoch 18/40  Iteration 4774/11040 Training loss: 1.3003 0.4737 sec/batch\n",
      "Epoch 18/40  Iteration 4775/11040 Training loss: 1.3002 0.4634 sec/batch\n",
      "Epoch 18/40  Iteration 4776/11040 Training loss: 1.3001 0.4559 sec/batch\n",
      "Epoch 18/40  Iteration 4777/11040 Training loss: 1.3000 0.4728 sec/batch\n",
      "Epoch 18/40  Iteration 4778/11040 Training loss: 1.3000 0.4606 sec/batch\n",
      "Epoch 18/40  Iteration 4779/11040 Training loss: 1.2995 0.4586 sec/batch\n",
      "Epoch 18/40  Iteration 4780/11040 Training loss: 1.2987 0.4731 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40  Iteration 4781/11040 Training loss: 1.2986 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4782/11040 Training loss: 1.2988 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4783/11040 Training loss: 1.2990 0.4814 sec/batch\n",
      "Epoch 18/40  Iteration 4784/11040 Training loss: 1.2991 0.4704 sec/batch\n",
      "Epoch 18/40  Iteration 4785/11040 Training loss: 1.2989 0.4719 sec/batch\n",
      "Epoch 18/40  Iteration 4786/11040 Training loss: 1.2993 0.4614 sec/batch\n",
      "Epoch 18/40  Iteration 4787/11040 Training loss: 1.2999 0.4605 sec/batch\n",
      "Epoch 18/40  Iteration 4788/11040 Training loss: 1.3002 0.4721 sec/batch\n",
      "Epoch 18/40  Iteration 4789/11040 Training loss: 1.3006 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4790/11040 Training loss: 1.3010 0.4761 sec/batch\n",
      "Epoch 18/40  Iteration 4791/11040 Training loss: 1.3011 0.4743 sec/batch\n",
      "Epoch 18/40  Iteration 4792/11040 Training loss: 1.3009 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4793/11040 Training loss: 1.3011 0.4574 sec/batch\n",
      "Epoch 18/40  Iteration 4794/11040 Training loss: 1.3012 0.4618 sec/batch\n",
      "Epoch 18/40  Iteration 4795/11040 Training loss: 1.3007 0.4656 sec/batch\n",
      "Epoch 18/40  Iteration 4796/11040 Training loss: 1.3010 0.4821 sec/batch\n",
      "Epoch 18/40  Iteration 4797/11040 Training loss: 1.3007 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4798/11040 Training loss: 1.3006 0.4728 sec/batch\n",
      "Epoch 18/40  Iteration 4799/11040 Training loss: 1.3007 0.4789 sec/batch\n",
      "Epoch 18/40  Iteration 4800/11040 Training loss: 1.3009 0.4682 sec/batch\n",
      "Epoch 18/40  Iteration 4801/11040 Training loss: 1.3012 0.4717 sec/batch\n",
      "Epoch 18/40  Iteration 4802/11040 Training loss: 1.3012 0.4749 sec/batch\n",
      "Epoch 18/40  Iteration 4803/11040 Training loss: 1.3014 0.4757 sec/batch\n",
      "Epoch 18/40  Iteration 4804/11040 Training loss: 1.3016 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4805/11040 Training loss: 1.3018 0.4722 sec/batch\n",
      "Epoch 18/40  Iteration 4806/11040 Training loss: 1.3019 0.4615 sec/batch\n",
      "Epoch 18/40  Iteration 4807/11040 Training loss: 1.3022 0.4757 sec/batch\n",
      "Epoch 18/40  Iteration 4808/11040 Training loss: 1.3026 0.4584 sec/batch\n",
      "Epoch 18/40  Iteration 4809/11040 Training loss: 1.3025 0.4584 sec/batch\n",
      "Epoch 18/40  Iteration 4810/11040 Training loss: 1.3028 0.4739 sec/batch\n",
      "Epoch 18/40  Iteration 4811/11040 Training loss: 1.3030 0.4753 sec/batch\n",
      "Epoch 18/40  Iteration 4812/11040 Training loss: 1.3032 0.4740 sec/batch\n",
      "Epoch 18/40  Iteration 4813/11040 Training loss: 1.3032 0.4761 sec/batch\n",
      "Epoch 18/40  Iteration 4814/11040 Training loss: 1.3032 0.4730 sec/batch\n",
      "Epoch 18/40  Iteration 4815/11040 Training loss: 1.3035 0.4763 sec/batch\n",
      "Epoch 18/40  Iteration 4816/11040 Training loss: 1.3039 0.4751 sec/batch\n",
      "Epoch 18/40  Iteration 4817/11040 Training loss: 1.3040 0.4746 sec/batch\n",
      "Epoch 18/40  Iteration 4818/11040 Training loss: 1.3043 0.4737 sec/batch\n",
      "Epoch 18/40  Iteration 4819/11040 Training loss: 1.3044 0.4654 sec/batch\n",
      "Epoch 18/40  Iteration 4820/11040 Training loss: 1.3044 0.4861 sec/batch\n",
      "Epoch 18/40  Iteration 4821/11040 Training loss: 1.3042 0.4707 sec/batch\n",
      "Epoch 18/40  Iteration 4822/11040 Training loss: 1.3045 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4823/11040 Training loss: 1.3044 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4824/11040 Training loss: 1.3042 0.4591 sec/batch\n",
      "Epoch 18/40  Iteration 4825/11040 Training loss: 1.3043 0.4599 sec/batch\n",
      "Epoch 18/40  Iteration 4826/11040 Training loss: 1.3043 0.4585 sec/batch\n",
      "Epoch 18/40  Iteration 4827/11040 Training loss: 1.3044 0.4735 sec/batch\n",
      "Epoch 18/40  Iteration 4828/11040 Training loss: 1.3045 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4829/11040 Training loss: 1.3044 0.4740 sec/batch\n",
      "Epoch 18/40  Iteration 4830/11040 Training loss: 1.3046 0.4769 sec/batch\n",
      "Epoch 18/40  Iteration 4831/11040 Training loss: 1.3048 0.4720 sec/batch\n",
      "Epoch 18/40  Iteration 4832/11040 Training loss: 1.3049 0.4601 sec/batch\n",
      "Epoch 18/40  Iteration 4833/11040 Training loss: 1.3050 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4834/11040 Training loss: 1.3051 0.4744 sec/batch\n",
      "Epoch 18/40  Iteration 4835/11040 Training loss: 1.3051 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4836/11040 Training loss: 1.3049 0.4732 sec/batch\n",
      "Epoch 18/40  Iteration 4837/11040 Training loss: 1.3049 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4838/11040 Training loss: 1.3048 0.4601 sec/batch\n",
      "Epoch 18/40  Iteration 4839/11040 Training loss: 1.3048 0.4562 sec/batch\n",
      "Epoch 18/40  Iteration 4840/11040 Training loss: 1.3047 0.4607 sec/batch\n",
      "Epoch 18/40  Iteration 4841/11040 Training loss: 1.3045 0.4574 sec/batch\n",
      "Epoch 18/40  Iteration 4842/11040 Training loss: 1.3045 0.4741 sec/batch\n",
      "Epoch 18/40  Iteration 4843/11040 Training loss: 1.3045 0.4592 sec/batch\n",
      "Epoch 18/40  Iteration 4844/11040 Training loss: 1.3046 0.4586 sec/batch\n",
      "Epoch 18/40  Iteration 4845/11040 Training loss: 1.3046 0.4605 sec/batch\n",
      "Epoch 18/40  Iteration 4846/11040 Training loss: 1.3048 0.4739 sec/batch\n",
      "Epoch 18/40  Iteration 4847/11040 Training loss: 1.3048 0.4742 sec/batch\n",
      "Epoch 18/40  Iteration 4848/11040 Training loss: 1.3047 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4849/11040 Training loss: 1.3046 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4850/11040 Training loss: 1.3047 0.4615 sec/batch\n",
      "Epoch 18/40  Iteration 4851/11040 Training loss: 1.3048 0.4743 sec/batch\n",
      "Epoch 18/40  Iteration 4852/11040 Training loss: 1.3049 0.4573 sec/batch\n",
      "Epoch 18/40  Iteration 4853/11040 Training loss: 1.3050 0.4677 sec/batch\n",
      "Epoch 18/40  Iteration 4854/11040 Training loss: 1.3050 0.4687 sec/batch\n",
      "Epoch 18/40  Iteration 4855/11040 Training loss: 1.3051 0.4746 sec/batch\n",
      "Epoch 18/40  Iteration 4856/11040 Training loss: 1.3052 0.4717 sec/batch\n",
      "Epoch 18/40  Iteration 4857/11040 Training loss: 1.3054 0.4758 sec/batch\n",
      "Epoch 18/40  Iteration 4858/11040 Training loss: 1.3056 0.4748 sec/batch\n",
      "Epoch 18/40  Iteration 4859/11040 Training loss: 1.3056 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4860/11040 Training loss: 1.3055 0.4590 sec/batch\n",
      "Epoch 18/40  Iteration 4861/11040 Training loss: 1.3054 0.4610 sec/batch\n",
      "Epoch 18/40  Iteration 4862/11040 Training loss: 1.3053 0.4583 sec/batch\n",
      "Epoch 18/40  Iteration 4863/11040 Training loss: 1.3055 0.4753 sec/batch\n",
      "Epoch 18/40  Iteration 4864/11040 Training loss: 1.3056 0.4746 sec/batch\n",
      "Epoch 18/40  Iteration 4865/11040 Training loss: 1.3056 0.4737 sec/batch\n",
      "Epoch 18/40  Iteration 4866/11040 Training loss: 1.3056 0.4752 sec/batch\n",
      "Epoch 18/40  Iteration 4867/11040 Training loss: 1.3056 0.4754 sec/batch\n",
      "Epoch 18/40  Iteration 4868/11040 Training loss: 1.3056 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4869/11040 Training loss: 1.3057 0.4617 sec/batch\n",
      "Epoch 18/40  Iteration 4870/11040 Training loss: 1.3057 0.4574 sec/batch\n",
      "Epoch 18/40  Iteration 4871/11040 Training loss: 1.3059 0.4603 sec/batch\n",
      "Epoch 18/40  Iteration 4872/11040 Training loss: 1.3061 0.4780 sec/batch\n",
      "Epoch 18/40  Iteration 4873/11040 Training loss: 1.3061 0.4827 sec/batch\n",
      "Epoch 18/40  Iteration 4874/11040 Training loss: 1.3061 0.4786 sec/batch\n",
      "Epoch 18/40  Iteration 4875/11040 Training loss: 1.3060 0.4752 sec/batch\n",
      "Epoch 18/40  Iteration 4876/11040 Training loss: 1.3062 0.4740 sec/batch\n",
      "Epoch 18/40  Iteration 4877/11040 Training loss: 1.3062 0.4763 sec/batch\n",
      "Epoch 18/40  Iteration 4878/11040 Training loss: 1.3063 0.4743 sec/batch\n",
      "Epoch 18/40  Iteration 4879/11040 Training loss: 1.3066 0.4749 sec/batch\n",
      "Epoch 18/40  Iteration 4880/11040 Training loss: 1.3067 0.4684 sec/batch\n",
      "Epoch 18/40  Iteration 4881/11040 Training loss: 1.3066 0.4673 sec/batch\n",
      "Epoch 18/40  Iteration 4882/11040 Training loss: 1.3066 0.4722 sec/batch\n",
      "Epoch 18/40  Iteration 4883/11040 Training loss: 1.3065 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4884/11040 Training loss: 1.3065 0.4729 sec/batch\n",
      "Epoch 18/40  Iteration 4885/11040 Training loss: 1.3065 0.4607 sec/batch\n",
      "Epoch 18/40  Iteration 4886/11040 Training loss: 1.3066 0.4741 sec/batch\n",
      "Epoch 18/40  Iteration 4887/11040 Training loss: 1.3068 0.4603 sec/batch\n",
      "Epoch 18/40  Iteration 4888/11040 Training loss: 1.3067 0.4732 sec/batch\n",
      "Epoch 18/40  Iteration 4889/11040 Training loss: 1.3068 0.4748 sec/batch\n",
      "Epoch 18/40  Iteration 4890/11040 Training loss: 1.3069 0.4783 sec/batch\n",
      "Epoch 18/40  Iteration 4891/11040 Training loss: 1.3070 0.4706 sec/batch\n",
      "Epoch 18/40  Iteration 4892/11040 Training loss: 1.3071 0.4774 sec/batch\n",
      "Epoch 18/40  Iteration 4893/11040 Training loss: 1.3073 0.4719 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40  Iteration 4894/11040 Training loss: 1.3074 0.4749 sec/batch\n",
      "Epoch 18/40  Iteration 4895/11040 Training loss: 1.3076 0.4758 sec/batch\n",
      "Epoch 18/40  Iteration 4896/11040 Training loss: 1.3078 0.4598 sec/batch\n",
      "Epoch 18/40  Iteration 4897/11040 Training loss: 1.3077 0.4753 sec/batch\n",
      "Epoch 18/40  Iteration 4898/11040 Training loss: 1.3077 0.4738 sec/batch\n",
      "Epoch 18/40  Iteration 4899/11040 Training loss: 1.3076 0.4749 sec/batch\n",
      "Epoch 18/40  Iteration 4900/11040 Training loss: 1.3077 0.4741 sec/batch\n",
      "Epoch 18/40  Iteration 4901/11040 Training loss: 1.3076 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4902/11040 Training loss: 1.3076 0.4720 sec/batch\n",
      "Epoch 18/40  Iteration 4903/11040 Training loss: 1.3076 0.4764 sec/batch\n",
      "Epoch 18/40  Iteration 4904/11040 Training loss: 1.3076 0.4585 sec/batch\n",
      "Epoch 18/40  Iteration 4905/11040 Training loss: 1.3076 0.4757 sec/batch\n",
      "Epoch 18/40  Iteration 4906/11040 Training loss: 1.3076 0.4740 sec/batch\n",
      "Epoch 18/40  Iteration 4907/11040 Training loss: 1.3074 0.4606 sec/batch\n",
      "Epoch 18/40  Iteration 4908/11040 Training loss: 1.3073 0.4742 sec/batch\n",
      "Epoch 18/40  Iteration 4909/11040 Training loss: 1.3072 0.4585 sec/batch\n",
      "Epoch 18/40  Iteration 4910/11040 Training loss: 1.3071 0.4596 sec/batch\n",
      "Epoch 18/40  Iteration 4911/11040 Training loss: 1.3070 0.4588 sec/batch\n",
      "Epoch 18/40  Iteration 4912/11040 Training loss: 1.3069 0.4600 sec/batch\n",
      "Epoch 18/40  Iteration 4913/11040 Training loss: 1.3068 0.4729 sec/batch\n",
      "Epoch 18/40  Iteration 4914/11040 Training loss: 1.3066 0.4718 sec/batch\n",
      "Epoch 18/40  Iteration 4915/11040 Training loss: 1.3066 0.4788 sec/batch\n",
      "Epoch 18/40  Iteration 4916/11040 Training loss: 1.3066 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4917/11040 Training loss: 1.3066 0.4720 sec/batch\n",
      "Epoch 18/40  Iteration 4918/11040 Training loss: 1.3066 0.4736 sec/batch\n",
      "Epoch 18/40  Iteration 4919/11040 Training loss: 1.3064 0.4741 sec/batch\n",
      "Epoch 18/40  Iteration 4920/11040 Training loss: 1.3063 0.4634 sec/batch\n",
      "Epoch 18/40  Iteration 4921/11040 Training loss: 1.3064 0.4581 sec/batch\n",
      "Epoch 18/40  Iteration 4922/11040 Training loss: 1.3064 0.4736 sec/batch\n",
      "Epoch 18/40  Iteration 4923/11040 Training loss: 1.3064 0.4757 sec/batch\n",
      "Epoch 18/40  Iteration 4924/11040 Training loss: 1.3065 0.4730 sec/batch\n",
      "Epoch 18/40  Iteration 4925/11040 Training loss: 1.3064 0.4739 sec/batch\n",
      "Epoch 18/40  Iteration 4926/11040 Training loss: 1.3062 0.4772 sec/batch\n",
      "Epoch 18/40  Iteration 4927/11040 Training loss: 1.3063 0.4730 sec/batch\n",
      "Epoch 18/40  Iteration 4928/11040 Training loss: 1.3063 0.4633 sec/batch\n",
      "Epoch 18/40  Iteration 4929/11040 Training loss: 1.3063 0.4746 sec/batch\n",
      "Epoch 18/40  Iteration 4930/11040 Training loss: 1.3064 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4931/11040 Training loss: 1.3065 0.4814 sec/batch\n",
      "Epoch 18/40  Iteration 4932/11040 Training loss: 1.3065 0.4676 sec/batch\n",
      "Epoch 18/40  Iteration 4933/11040 Training loss: 1.3065 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4934/11040 Training loss: 1.3065 0.4723 sec/batch\n",
      "Epoch 18/40  Iteration 4935/11040 Training loss: 1.3065 0.4760 sec/batch\n",
      "Epoch 18/40  Iteration 4936/11040 Training loss: 1.3064 0.4743 sec/batch\n",
      "Epoch 18/40  Iteration 4937/11040 Training loss: 1.3063 0.4747 sec/batch\n",
      "Epoch 18/40  Iteration 4938/11040 Training loss: 1.3063 0.4733 sec/batch\n",
      "Epoch 18/40  Iteration 4939/11040 Training loss: 1.3063 0.4597 sec/batch\n",
      "Epoch 18/40  Iteration 4940/11040 Training loss: 1.3062 0.4764 sec/batch\n",
      "Epoch 18/40  Iteration 4941/11040 Training loss: 1.3062 0.4571 sec/batch\n",
      "Epoch 18/40  Iteration 4942/11040 Training loss: 1.3062 0.4751 sec/batch\n",
      "Epoch 18/40  Iteration 4943/11040 Training loss: 1.3061 0.4747 sec/batch\n",
      "Epoch 18/40  Iteration 4944/11040 Training loss: 1.3060 0.4739 sec/batch\n",
      "Epoch 18/40  Iteration 4945/11040 Training loss: 1.3060 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4946/11040 Training loss: 1.3061 0.4748 sec/batch\n",
      "Epoch 18/40  Iteration 4947/11040 Training loss: 1.3061 0.4732 sec/batch\n",
      "Epoch 18/40  Iteration 4948/11040 Training loss: 1.3061 0.4606 sec/batch\n",
      "Epoch 18/40  Iteration 4949/11040 Training loss: 1.3061 0.4738 sec/batch\n",
      "Epoch 18/40  Iteration 4950/11040 Training loss: 1.3062 0.4759 sec/batch\n",
      "Epoch 18/40  Iteration 4951/11040 Training loss: 1.3061 0.4752 sec/batch\n",
      "Epoch 18/40  Iteration 4952/11040 Training loss: 1.3061 0.4747 sec/batch\n",
      "Epoch 18/40  Iteration 4953/11040 Training loss: 1.3061 0.4824 sec/batch\n",
      "Epoch 18/40  Iteration 4954/11040 Training loss: 1.3062 0.4593 sec/batch\n",
      "Epoch 18/40  Iteration 4955/11040 Training loss: 1.3063 0.4753 sec/batch\n",
      "Epoch 18/40  Iteration 4956/11040 Training loss: 1.3064 0.4743 sec/batch\n",
      "Epoch 18/40  Iteration 4957/11040 Training loss: 1.3063 0.4731 sec/batch\n",
      "Epoch 18/40  Iteration 4958/11040 Training loss: 1.3064 0.4783 sec/batch\n",
      "Epoch 18/40  Iteration 4959/11040 Training loss: 1.3066 0.4744 sec/batch\n",
      "Epoch 18/40  Iteration 4960/11040 Training loss: 1.3066 0.4741 sec/batch\n",
      "Epoch 18/40  Iteration 4961/11040 Training loss: 1.3067 0.4735 sec/batch\n",
      "Epoch 18/40  Iteration 4962/11040 Training loss: 1.3068 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4963/11040 Training loss: 1.3070 0.4762 sec/batch\n",
      "Epoch 18/40  Iteration 4964/11040 Training loss: 1.3073 0.4715 sec/batch\n",
      "Epoch 18/40  Iteration 4965/11040 Training loss: 1.3073 0.4615 sec/batch\n",
      "Epoch 18/40  Iteration 4966/11040 Training loss: 1.3074 0.4721 sec/batch\n",
      "Epoch 18/40  Iteration 4967/11040 Training loss: 1.3074 0.4737 sec/batch\n",
      "Epoch 18/40  Iteration 4968/11040 Training loss: 1.3074 0.4760 sec/batch\n",
      "Epoch 19/40  Iteration 4969/11040 Training loss: 1.3703 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 4970/11040 Training loss: 1.3386 0.4750 sec/batch\n",
      "Epoch 19/40  Iteration 4971/11040 Training loss: 1.3353 0.4753 sec/batch\n",
      "Epoch 19/40  Iteration 4972/11040 Training loss: 1.3337 0.4729 sec/batch\n",
      "Epoch 19/40  Iteration 4973/11040 Training loss: 1.3323 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 4974/11040 Training loss: 1.3296 0.4612 sec/batch\n",
      "Epoch 19/40  Iteration 4975/11040 Training loss: 1.3218 0.4595 sec/batch\n",
      "Epoch 19/40  Iteration 4976/11040 Training loss: 1.3194 0.4594 sec/batch\n",
      "Epoch 19/40  Iteration 4977/11040 Training loss: 1.3162 0.4594 sec/batch\n",
      "Epoch 19/40  Iteration 4978/11040 Training loss: 1.3163 0.4574 sec/batch\n",
      "Epoch 19/40  Iteration 4979/11040 Training loss: 1.3129 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 4980/11040 Training loss: 1.3118 0.4758 sec/batch\n",
      "Epoch 19/40  Iteration 4981/11040 Training loss: 1.3086 0.4729 sec/batch\n",
      "Epoch 19/40  Iteration 4982/11040 Training loss: 1.3063 0.4760 sec/batch\n",
      "Epoch 19/40  Iteration 4983/11040 Training loss: 1.3051 0.4733 sec/batch\n",
      "Epoch 19/40  Iteration 4984/11040 Training loss: 1.3053 0.4748 sec/batch\n",
      "Epoch 19/40  Iteration 4985/11040 Training loss: 1.3030 0.4741 sec/batch\n",
      "Epoch 19/40  Iteration 4986/11040 Training loss: 1.3009 0.4806 sec/batch\n",
      "Epoch 19/40  Iteration 4987/11040 Training loss: 1.3002 0.4723 sec/batch\n",
      "Epoch 19/40  Iteration 4988/11040 Training loss: 1.3005 0.4733 sec/batch\n",
      "Epoch 19/40  Iteration 4989/11040 Training loss: 1.3010 0.4745 sec/batch\n",
      "Epoch 19/40  Iteration 4990/11040 Training loss: 1.3019 0.4744 sec/batch\n",
      "Epoch 19/40  Iteration 4991/11040 Training loss: 1.3017 0.4738 sec/batch\n",
      "Epoch 19/40  Iteration 4992/11040 Training loss: 1.3023 0.4770 sec/batch\n",
      "Epoch 19/40  Iteration 4993/11040 Training loss: 1.3016 0.4729 sec/batch\n",
      "Epoch 19/40  Iteration 4994/11040 Training loss: 1.3008 0.4595 sec/batch\n",
      "Epoch 19/40  Iteration 4995/11040 Training loss: 1.3004 0.4754 sec/batch\n",
      "Epoch 19/40  Iteration 4996/11040 Training loss: 1.3001 0.4731 sec/batch\n",
      "Epoch 19/40  Iteration 4997/11040 Training loss: 1.3002 0.4761 sec/batch\n",
      "Epoch 19/40  Iteration 4998/11040 Training loss: 1.3001 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 4999/11040 Training loss: 1.2997 0.4622 sec/batch\n",
      "Epoch 19/40  Iteration 5000/11040 Training loss: 1.2996 0.4590 sec/batch\n",
      "Validation loss: 1.28506 Saving checkpoint!\n",
      "Epoch 19/40  Iteration 5001/11040 Training loss: 1.3026 0.4688 sec/batch\n",
      "Epoch 19/40  Iteration 5002/11040 Training loss: 1.3021 0.4642 sec/batch\n",
      "Epoch 19/40  Iteration 5003/11040 Training loss: 1.3020 0.4734 sec/batch\n",
      "Epoch 19/40  Iteration 5004/11040 Training loss: 1.3024 0.4615 sec/batch\n",
      "Epoch 19/40  Iteration 5005/11040 Training loss: 1.3021 0.4733 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40  Iteration 5006/11040 Training loss: 1.3014 0.4731 sec/batch\n",
      "Epoch 19/40  Iteration 5007/11040 Training loss: 1.3008 0.4615 sec/batch\n",
      "Epoch 19/40  Iteration 5008/11040 Training loss: 1.3008 0.4566 sec/batch\n",
      "Epoch 19/40  Iteration 5009/11040 Training loss: 1.3002 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 5010/11040 Training loss: 1.2989 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5011/11040 Training loss: 1.2989 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5012/11040 Training loss: 1.2984 0.4769 sec/batch\n",
      "Epoch 19/40  Iteration 5013/11040 Training loss: 1.2979 0.4718 sec/batch\n",
      "Epoch 19/40  Iteration 5014/11040 Training loss: 1.2975 0.4762 sec/batch\n",
      "Epoch 19/40  Iteration 5015/11040 Training loss: 1.2967 0.4742 sec/batch\n",
      "Epoch 19/40  Iteration 5016/11040 Training loss: 1.2964 0.4593 sec/batch\n",
      "Epoch 19/40  Iteration 5017/11040 Training loss: 1.2957 0.4589 sec/batch\n",
      "Epoch 19/40  Iteration 5018/11040 Training loss: 1.2960 0.4596 sec/batch\n",
      "Epoch 19/40  Iteration 5019/11040 Training loss: 1.2956 0.4587 sec/batch\n",
      "Epoch 19/40  Iteration 5020/11040 Training loss: 1.2954 0.4742 sec/batch\n",
      "Epoch 19/40  Iteration 5021/11040 Training loss: 1.2957 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5022/11040 Training loss: 1.2952 0.4739 sec/batch\n",
      "Epoch 19/40  Iteration 5023/11040 Training loss: 1.2952 0.4602 sec/batch\n",
      "Epoch 19/40  Iteration 5024/11040 Training loss: 1.2951 0.4605 sec/batch\n",
      "Epoch 19/40  Iteration 5025/11040 Training loss: 1.2949 0.4732 sec/batch\n",
      "Epoch 19/40  Iteration 5026/11040 Training loss: 1.2948 0.4750 sec/batch\n",
      "Epoch 19/40  Iteration 5027/11040 Training loss: 1.2951 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 5028/11040 Training loss: 1.2947 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5029/11040 Training loss: 1.2947 0.4666 sec/batch\n",
      "Epoch 19/40  Iteration 5030/11040 Training loss: 1.2944 0.4682 sec/batch\n",
      "Epoch 19/40  Iteration 5031/11040 Training loss: 1.2944 0.4614 sec/batch\n",
      "Epoch 19/40  Iteration 5032/11040 Training loss: 1.2942 0.4685 sec/batch\n",
      "Epoch 19/40  Iteration 5033/11040 Training loss: 1.2945 0.4782 sec/batch\n",
      "Epoch 19/40  Iteration 5034/11040 Training loss: 1.2941 0.4736 sec/batch\n",
      "Epoch 19/40  Iteration 5035/11040 Training loss: 1.2937 0.4936 sec/batch\n",
      "Epoch 19/40  Iteration 5036/11040 Training loss: 1.2939 0.4735 sec/batch\n",
      "Epoch 19/40  Iteration 5037/11040 Training loss: 1.2937 0.4605 sec/batch\n",
      "Epoch 19/40  Iteration 5038/11040 Training loss: 1.2937 0.4584 sec/batch\n",
      "Epoch 19/40  Iteration 5039/11040 Training loss: 1.2933 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5040/11040 Training loss: 1.2926 0.4750 sec/batch\n",
      "Epoch 19/40  Iteration 5041/11040 Training loss: 1.2921 0.4732 sec/batch\n",
      "Epoch 19/40  Iteration 5042/11040 Training loss: 1.2917 0.4603 sec/batch\n",
      "Epoch 19/40  Iteration 5043/11040 Training loss: 1.2915 0.4599 sec/batch\n",
      "Epoch 19/40  Iteration 5044/11040 Training loss: 1.2918 0.4596 sec/batch\n",
      "Epoch 19/40  Iteration 5045/11040 Training loss: 1.2913 0.4732 sec/batch\n",
      "Epoch 19/40  Iteration 5046/11040 Training loss: 1.2910 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5047/11040 Training loss: 1.2907 0.4728 sec/batch\n",
      "Epoch 19/40  Iteration 5048/11040 Training loss: 1.2906 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5049/11040 Training loss: 1.2904 0.4764 sec/batch\n",
      "Epoch 19/40  Iteration 5050/11040 Training loss: 1.2904 0.4730 sec/batch\n",
      "Epoch 19/40  Iteration 5051/11040 Training loss: 1.2903 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5052/11040 Training loss: 1.2902 0.4754 sec/batch\n",
      "Epoch 19/40  Iteration 5053/11040 Training loss: 1.2901 0.4749 sec/batch\n",
      "Epoch 19/40  Iteration 5054/11040 Training loss: 1.2901 0.4588 sec/batch\n",
      "Epoch 19/40  Iteration 5055/11040 Training loss: 1.2895 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5056/11040 Training loss: 1.2886 0.4755 sec/batch\n",
      "Epoch 19/40  Iteration 5057/11040 Training loss: 1.2886 0.4728 sec/batch\n",
      "Epoch 19/40  Iteration 5058/11040 Training loss: 1.2888 0.4738 sec/batch\n",
      "Epoch 19/40  Iteration 5059/11040 Training loss: 1.2890 0.4761 sec/batch\n",
      "Epoch 19/40  Iteration 5060/11040 Training loss: 1.2890 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5061/11040 Training loss: 1.2888 0.4757 sec/batch\n",
      "Epoch 19/40  Iteration 5062/11040 Training loss: 1.2891 0.4584 sec/batch\n",
      "Epoch 19/40  Iteration 5063/11040 Training loss: 1.2897 0.4591 sec/batch\n",
      "Epoch 19/40  Iteration 5064/11040 Training loss: 1.2899 0.4604 sec/batch\n",
      "Epoch 19/40  Iteration 5065/11040 Training loss: 1.2903 0.4589 sec/batch\n",
      "Epoch 19/40  Iteration 5066/11040 Training loss: 1.2908 0.4733 sec/batch\n",
      "Epoch 19/40  Iteration 5067/11040 Training loss: 1.2909 0.4614 sec/batch\n",
      "Epoch 19/40  Iteration 5068/11040 Training loss: 1.2907 0.4574 sec/batch\n",
      "Epoch 19/40  Iteration 5069/11040 Training loss: 1.2908 0.4558 sec/batch\n",
      "Epoch 19/40  Iteration 5070/11040 Training loss: 1.2909 0.4626 sec/batch\n",
      "Epoch 19/40  Iteration 5071/11040 Training loss: 1.2904 0.4760 sec/batch\n",
      "Epoch 19/40  Iteration 5072/11040 Training loss: 1.2906 0.4585 sec/batch\n",
      "Epoch 19/40  Iteration 5073/11040 Training loss: 1.2905 0.4602 sec/batch\n",
      "Epoch 19/40  Iteration 5074/11040 Training loss: 1.2904 0.4595 sec/batch\n",
      "Epoch 19/40  Iteration 5075/11040 Training loss: 1.2906 0.4741 sec/batch\n",
      "Epoch 19/40  Iteration 5076/11040 Training loss: 1.2908 0.4739 sec/batch\n",
      "Epoch 19/40  Iteration 5077/11040 Training loss: 1.2911 0.4742 sec/batch\n",
      "Epoch 19/40  Iteration 5078/11040 Training loss: 1.2911 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5079/11040 Training loss: 1.2912 0.4729 sec/batch\n",
      "Epoch 19/40  Iteration 5080/11040 Training loss: 1.2914 0.4607 sec/batch\n",
      "Epoch 19/40  Iteration 5081/11040 Training loss: 1.2915 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5082/11040 Training loss: 1.2916 0.4722 sec/batch\n",
      "Epoch 19/40  Iteration 5083/11040 Training loss: 1.2919 0.4604 sec/batch\n",
      "Epoch 19/40  Iteration 5084/11040 Training loss: 1.2923 0.4593 sec/batch\n",
      "Epoch 19/40  Iteration 5085/11040 Training loss: 1.2923 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5086/11040 Training loss: 1.2925 0.4744 sec/batch\n",
      "Epoch 19/40  Iteration 5087/11040 Training loss: 1.2926 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5088/11040 Training loss: 1.2927 0.4741 sec/batch\n",
      "Epoch 19/40  Iteration 5089/11040 Training loss: 1.2927 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5090/11040 Training loss: 1.2928 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 5091/11040 Training loss: 1.2931 0.4616 sec/batch\n",
      "Epoch 19/40  Iteration 5092/11040 Training loss: 1.2935 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5093/11040 Training loss: 1.2936 0.4734 sec/batch\n",
      "Epoch 19/40  Iteration 5094/11040 Training loss: 1.2939 0.4601 sec/batch\n",
      "Epoch 19/40  Iteration 5095/11040 Training loss: 1.2939 0.4631 sec/batch\n",
      "Epoch 19/40  Iteration 5096/11040 Training loss: 1.2938 0.4706 sec/batch\n",
      "Epoch 19/40  Iteration 5097/11040 Training loss: 1.2936 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5098/11040 Training loss: 1.2938 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5099/11040 Training loss: 1.2937 0.4757 sec/batch\n",
      "Epoch 19/40  Iteration 5100/11040 Training loss: 1.2935 0.4593 sec/batch\n",
      "Epoch 19/40  Iteration 5101/11040 Training loss: 1.2935 0.4569 sec/batch\n",
      "Epoch 19/40  Iteration 5102/11040 Training loss: 1.2934 0.4600 sec/batch\n",
      "Epoch 19/40  Iteration 5103/11040 Training loss: 1.2936 0.4741 sec/batch\n",
      "Epoch 19/40  Iteration 5104/11040 Training loss: 1.2937 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5105/11040 Training loss: 1.2936 0.4729 sec/batch\n",
      "Epoch 19/40  Iteration 5106/11040 Training loss: 1.2938 0.4606 sec/batch\n",
      "Epoch 19/40  Iteration 5107/11040 Training loss: 1.2939 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 5108/11040 Training loss: 1.2940 0.4765 sec/batch\n",
      "Epoch 19/40  Iteration 5109/11040 Training loss: 1.2940 0.4725 sec/batch\n",
      "Epoch 19/40  Iteration 5110/11040 Training loss: 1.2940 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5111/11040 Training loss: 1.2940 0.4762 sec/batch\n",
      "Epoch 19/40  Iteration 5112/11040 Training loss: 1.2938 0.4722 sec/batch\n",
      "Epoch 19/40  Iteration 5113/11040 Training loss: 1.2938 0.4586 sec/batch\n",
      "Epoch 19/40  Iteration 5114/11040 Training loss: 1.2938 0.4746 sec/batch\n",
      "Epoch 19/40  Iteration 5115/11040 Training loss: 1.2938 0.4764 sec/batch\n",
      "Epoch 19/40  Iteration 5116/11040 Training loss: 1.2937 0.4728 sec/batch\n",
      "Epoch 19/40  Iteration 5117/11040 Training loss: 1.2935 0.4767 sec/batch\n",
      "Epoch 19/40  Iteration 5118/11040 Training loss: 1.2935 0.4596 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40  Iteration 5119/11040 Training loss: 1.2935 0.4734 sec/batch\n",
      "Epoch 19/40  Iteration 5120/11040 Training loss: 1.2937 0.4713 sec/batch\n",
      "Epoch 19/40  Iteration 5121/11040 Training loss: 1.2938 0.4776 sec/batch\n",
      "Epoch 19/40  Iteration 5122/11040 Training loss: 1.2939 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5123/11040 Training loss: 1.2939 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5124/11040 Training loss: 1.2939 0.4733 sec/batch\n",
      "Epoch 19/40  Iteration 5125/11040 Training loss: 1.2938 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5126/11040 Training loss: 1.2940 0.4756 sec/batch\n",
      "Epoch 19/40  Iteration 5127/11040 Training loss: 1.2941 0.4733 sec/batch\n",
      "Epoch 19/40  Iteration 5128/11040 Training loss: 1.2941 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5129/11040 Training loss: 1.2942 0.4754 sec/batch\n",
      "Epoch 19/40  Iteration 5130/11040 Training loss: 1.2942 0.4886 sec/batch\n",
      "Epoch 19/40  Iteration 5131/11040 Training loss: 1.2943 0.4606 sec/batch\n",
      "Epoch 19/40  Iteration 5132/11040 Training loss: 1.2945 0.4685 sec/batch\n",
      "Epoch 19/40  Iteration 5133/11040 Training loss: 1.2946 0.4655 sec/batch\n",
      "Epoch 19/40  Iteration 5134/11040 Training loss: 1.2949 0.4725 sec/batch\n",
      "Epoch 19/40  Iteration 5135/11040 Training loss: 1.2949 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5136/11040 Training loss: 1.2948 0.4599 sec/batch\n",
      "Epoch 19/40  Iteration 5137/11040 Training loss: 1.2947 0.4594 sec/batch\n",
      "Epoch 19/40  Iteration 5138/11040 Training loss: 1.2947 0.4584 sec/batch\n",
      "Epoch 19/40  Iteration 5139/11040 Training loss: 1.2949 0.4593 sec/batch\n",
      "Epoch 19/40  Iteration 5140/11040 Training loss: 1.2950 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5141/11040 Training loss: 1.2950 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5142/11040 Training loss: 1.2949 0.4774 sec/batch\n",
      "Epoch 19/40  Iteration 5143/11040 Training loss: 1.2949 0.4731 sec/batch\n",
      "Epoch 19/40  Iteration 5144/11040 Training loss: 1.2949 0.4591 sec/batch\n",
      "Epoch 19/40  Iteration 5145/11040 Training loss: 1.2950 0.4587 sec/batch\n",
      "Epoch 19/40  Iteration 5146/11040 Training loss: 1.2950 0.4602 sec/batch\n",
      "Epoch 19/40  Iteration 5147/11040 Training loss: 1.2953 0.4731 sec/batch\n",
      "Epoch 19/40  Iteration 5148/11040 Training loss: 1.2953 0.4760 sec/batch\n",
      "Epoch 19/40  Iteration 5149/11040 Training loss: 1.2954 0.4732 sec/batch\n",
      "Epoch 19/40  Iteration 5150/11040 Training loss: 1.2955 0.4766 sec/batch\n",
      "Epoch 19/40  Iteration 5151/11040 Training loss: 1.2954 0.4755 sec/batch\n",
      "Epoch 19/40  Iteration 5152/11040 Training loss: 1.2956 0.4589 sec/batch\n",
      "Epoch 19/40  Iteration 5153/11040 Training loss: 1.2956 0.4728 sec/batch\n",
      "Epoch 19/40  Iteration 5154/11040 Training loss: 1.2957 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5155/11040 Training loss: 1.2959 0.4739 sec/batch\n",
      "Epoch 19/40  Iteration 5156/11040 Training loss: 1.2960 0.4769 sec/batch\n",
      "Epoch 19/40  Iteration 5157/11040 Training loss: 1.2960 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5158/11040 Training loss: 1.2960 0.4721 sec/batch\n",
      "Epoch 19/40  Iteration 5159/11040 Training loss: 1.2959 0.4605 sec/batch\n",
      "Epoch 19/40  Iteration 5160/11040 Training loss: 1.2959 0.4748 sec/batch\n",
      "Epoch 19/40  Iteration 5161/11040 Training loss: 1.2959 0.4750 sec/batch\n",
      "Epoch 19/40  Iteration 5162/11040 Training loss: 1.2960 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5163/11040 Training loss: 1.2963 0.4606 sec/batch\n",
      "Epoch 19/40  Iteration 5164/11040 Training loss: 1.2962 0.4591 sec/batch\n",
      "Epoch 19/40  Iteration 5165/11040 Training loss: 1.2964 0.4657 sec/batch\n",
      "Epoch 19/40  Iteration 5166/11040 Training loss: 1.2964 0.4818 sec/batch\n",
      "Epoch 19/40  Iteration 5167/11040 Training loss: 1.2966 0.4749 sec/batch\n",
      "Epoch 19/40  Iteration 5168/11040 Training loss: 1.2967 0.4749 sec/batch\n",
      "Epoch 19/40  Iteration 5169/11040 Training loss: 1.2968 0.4746 sec/batch\n",
      "Epoch 19/40  Iteration 5170/11040 Training loss: 1.2970 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 5171/11040 Training loss: 1.2971 0.4761 sec/batch\n",
      "Epoch 19/40  Iteration 5172/11040 Training loss: 1.2972 0.4718 sec/batch\n",
      "Epoch 19/40  Iteration 5173/11040 Training loss: 1.2972 0.4616 sec/batch\n",
      "Epoch 19/40  Iteration 5174/11040 Training loss: 1.2971 0.4760 sec/batch\n",
      "Epoch 19/40  Iteration 5175/11040 Training loss: 1.2971 0.4577 sec/batch\n",
      "Epoch 19/40  Iteration 5176/11040 Training loss: 1.2972 0.4739 sec/batch\n",
      "Epoch 19/40  Iteration 5177/11040 Training loss: 1.2971 0.4608 sec/batch\n",
      "Epoch 19/40  Iteration 5178/11040 Training loss: 1.2970 0.4731 sec/batch\n",
      "Epoch 19/40  Iteration 5179/11040 Training loss: 1.2970 0.4742 sec/batch\n",
      "Epoch 19/40  Iteration 5180/11040 Training loss: 1.2970 0.4755 sec/batch\n",
      "Epoch 19/40  Iteration 5181/11040 Training loss: 1.2970 0.4739 sec/batch\n",
      "Epoch 19/40  Iteration 5182/11040 Training loss: 1.2969 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5183/11040 Training loss: 1.2968 0.4604 sec/batch\n",
      "Epoch 19/40  Iteration 5184/11040 Training loss: 1.2967 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5185/11040 Training loss: 1.2966 0.4744 sec/batch\n",
      "Epoch 19/40  Iteration 5186/11040 Training loss: 1.2965 0.4619 sec/batch\n",
      "Epoch 19/40  Iteration 5187/11040 Training loss: 1.2964 0.4716 sec/batch\n",
      "Epoch 19/40  Iteration 5188/11040 Training loss: 1.2963 0.4732 sec/batch\n",
      "Epoch 19/40  Iteration 5189/11040 Training loss: 1.2962 0.4800 sec/batch\n",
      "Epoch 19/40  Iteration 5190/11040 Training loss: 1.2960 0.4706 sec/batch\n",
      "Epoch 19/40  Iteration 5191/11040 Training loss: 1.2960 0.4741 sec/batch\n",
      "Epoch 19/40  Iteration 5192/11040 Training loss: 1.2961 0.4616 sec/batch\n",
      "Epoch 19/40  Iteration 5193/11040 Training loss: 1.2960 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5194/11040 Training loss: 1.2960 0.4722 sec/batch\n",
      "Epoch 19/40  Iteration 5195/11040 Training loss: 1.2958 0.4758 sec/batch\n",
      "Epoch 19/40  Iteration 5196/11040 Training loss: 1.2957 0.4740 sec/batch\n",
      "Epoch 19/40  Iteration 5197/11040 Training loss: 1.2958 0.4758 sec/batch\n",
      "Epoch 19/40  Iteration 5198/11040 Training loss: 1.2958 0.4742 sec/batch\n",
      "Epoch 19/40  Iteration 5199/11040 Training loss: 1.2958 0.4589 sec/batch\n",
      "Epoch 19/40  Iteration 5200/11040 Training loss: 1.2958 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5201/11040 Training loss: 1.2957 0.4763 sec/batch\n",
      "Epoch 19/40  Iteration 5202/11040 Training loss: 1.2956 0.4754 sec/batch\n",
      "Epoch 19/40  Iteration 5203/11040 Training loss: 1.2957 0.4576 sec/batch\n",
      "Epoch 19/40  Iteration 5204/11040 Training loss: 1.2957 0.4595 sec/batch\n",
      "Epoch 19/40  Iteration 5205/11040 Training loss: 1.2957 0.4603 sec/batch\n",
      "Epoch 19/40  Iteration 5206/11040 Training loss: 1.2957 0.4584 sec/batch\n",
      "Epoch 19/40  Iteration 5207/11040 Training loss: 1.2958 0.4732 sec/batch\n",
      "Epoch 19/40  Iteration 5208/11040 Training loss: 1.2959 0.4759 sec/batch\n",
      "Epoch 19/40  Iteration 5209/11040 Training loss: 1.2959 0.4752 sec/batch\n",
      "Epoch 19/40  Iteration 5210/11040 Training loss: 1.2958 0.4753 sec/batch\n",
      "Epoch 19/40  Iteration 5211/11040 Training loss: 1.2958 0.4738 sec/batch\n",
      "Epoch 19/40  Iteration 5212/11040 Training loss: 1.2957 0.4728 sec/batch\n",
      "Epoch 19/40  Iteration 5213/11040 Training loss: 1.2956 0.4750 sec/batch\n",
      "Epoch 19/40  Iteration 5214/11040 Training loss: 1.2956 0.4751 sec/batch\n",
      "Epoch 19/40  Iteration 5215/11040 Training loss: 1.2956 0.4734 sec/batch\n",
      "Epoch 19/40  Iteration 5216/11040 Training loss: 1.2955 0.4764 sec/batch\n",
      "Epoch 19/40  Iteration 5217/11040 Training loss: 1.2955 0.4743 sec/batch\n",
      "Epoch 19/40  Iteration 5218/11040 Training loss: 1.2955 0.4773 sec/batch\n",
      "Epoch 19/40  Iteration 5219/11040 Training loss: 1.2954 0.4723 sec/batch\n",
      "Epoch 19/40  Iteration 5220/11040 Training loss: 1.2954 0.4767 sec/batch\n",
      "Epoch 19/40  Iteration 5221/11040 Training loss: 1.2954 0.4724 sec/batch\n",
      "Epoch 19/40  Iteration 5222/11040 Training loss: 1.2955 0.4793 sec/batch\n",
      "Epoch 19/40  Iteration 5223/11040 Training loss: 1.2954 0.4713 sec/batch\n",
      "Epoch 19/40  Iteration 5224/11040 Training loss: 1.2955 0.4747 sec/batch\n",
      "Epoch 19/40  Iteration 5225/11040 Training loss: 1.2956 0.4588 sec/batch\n",
      "Epoch 19/40  Iteration 5226/11040 Training loss: 1.2956 0.4598 sec/batch\n",
      "Epoch 19/40  Iteration 5227/11040 Training loss: 1.2955 0.4731 sec/batch\n",
      "Epoch 19/40  Iteration 5228/11040 Training loss: 1.2955 0.4733 sec/batch\n",
      "Epoch 19/40  Iteration 5229/11040 Training loss: 1.2955 0.4765 sec/batch\n",
      "Epoch 19/40  Iteration 5230/11040 Training loss: 1.2956 0.4742 sec/batch\n",
      "Epoch 19/40  Iteration 5231/11040 Training loss: 1.2956 0.4755 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40  Iteration 5232/11040 Training loss: 1.2957 0.4690 sec/batch\n",
      "Epoch 19/40  Iteration 5233/11040 Training loss: 1.2956 0.4817 sec/batch\n",
      "Epoch 19/40  Iteration 5234/11040 Training loss: 1.2957 0.4722 sec/batch\n",
      "Epoch 19/40  Iteration 5235/11040 Training loss: 1.2959 0.4603 sec/batch\n",
      "Epoch 19/40  Iteration 5236/11040 Training loss: 1.2959 0.4741 sec/batch\n",
      "Epoch 19/40  Iteration 5237/11040 Training loss: 1.2960 0.4749 sec/batch\n",
      "Epoch 19/40  Iteration 5238/11040 Training loss: 1.2961 0.4760 sec/batch\n",
      "Epoch 19/40  Iteration 5239/11040 Training loss: 1.2964 0.4730 sec/batch\n",
      "Epoch 19/40  Iteration 5240/11040 Training loss: 1.2966 0.4606 sec/batch\n",
      "Epoch 19/40  Iteration 5241/11040 Training loss: 1.2967 0.4744 sec/batch\n",
      "Epoch 19/40  Iteration 5242/11040 Training loss: 1.2967 0.4603 sec/batch\n",
      "Epoch 19/40  Iteration 5243/11040 Training loss: 1.2967 0.4730 sec/batch\n",
      "Epoch 19/40  Iteration 5244/11040 Training loss: 1.2968 0.4744 sec/batch\n",
      "Epoch 20/40  Iteration 5245/11040 Training loss: 1.3634 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5246/11040 Training loss: 1.3320 0.4614 sec/batch\n",
      "Epoch 20/40  Iteration 5247/11040 Training loss: 1.3293 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5248/11040 Training loss: 1.3278 0.4758 sec/batch\n",
      "Epoch 20/40  Iteration 5249/11040 Training loss: 1.3258 0.4738 sec/batch\n",
      "Epoch 20/40  Iteration 5250/11040 Training loss: 1.3219 0.4753 sec/batch\n",
      "Epoch 20/40  Iteration 5251/11040 Training loss: 1.3123 0.4607 sec/batch\n",
      "Epoch 20/40  Iteration 5252/11040 Training loss: 1.3090 0.4588 sec/batch\n",
      "Epoch 20/40  Iteration 5253/11040 Training loss: 1.3059 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5254/11040 Training loss: 1.3052 0.4730 sec/batch\n",
      "Epoch 20/40  Iteration 5255/11040 Training loss: 1.3018 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5256/11040 Training loss: 1.3012 0.4746 sec/batch\n",
      "Epoch 20/40  Iteration 5257/11040 Training loss: 1.2986 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5258/11040 Training loss: 1.2958 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5259/11040 Training loss: 1.2950 0.4625 sec/batch\n",
      "Epoch 20/40  Iteration 5260/11040 Training loss: 1.2950 0.4722 sec/batch\n",
      "Epoch 20/40  Iteration 5261/11040 Training loss: 1.2926 0.4601 sec/batch\n",
      "Epoch 20/40  Iteration 5262/11040 Training loss: 1.2904 0.4595 sec/batch\n",
      "Epoch 20/40  Iteration 5263/11040 Training loss: 1.2897 0.4596 sec/batch\n",
      "Epoch 20/40  Iteration 5264/11040 Training loss: 1.2898 0.4589 sec/batch\n",
      "Epoch 20/40  Iteration 5265/11040 Training loss: 1.2915 0.4573 sec/batch\n",
      "Epoch 20/40  Iteration 5266/11040 Training loss: 1.2923 0.4594 sec/batch\n",
      "Epoch 20/40  Iteration 5267/11040 Training loss: 1.2920 0.4584 sec/batch\n",
      "Epoch 20/40  Iteration 5268/11040 Training loss: 1.2924 0.4592 sec/batch\n",
      "Epoch 20/40  Iteration 5269/11040 Training loss: 1.2917 0.4595 sec/batch\n",
      "Epoch 20/40  Iteration 5270/11040 Training loss: 1.2910 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5271/11040 Training loss: 1.2907 0.4750 sec/batch\n",
      "Epoch 20/40  Iteration 5272/11040 Training loss: 1.2899 0.4759 sec/batch\n",
      "Epoch 20/40  Iteration 5273/11040 Training loss: 1.2901 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5274/11040 Training loss: 1.2902 0.4757 sec/batch\n",
      "Epoch 20/40  Iteration 5275/11040 Training loss: 1.2896 0.4798 sec/batch\n",
      "Epoch 20/40  Iteration 5276/11040 Training loss: 1.2894 0.4692 sec/batch\n",
      "Epoch 20/40  Iteration 5277/11040 Training loss: 1.2887 0.4607 sec/batch\n",
      "Epoch 20/40  Iteration 5278/11040 Training loss: 1.2882 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5279/11040 Training loss: 1.2881 0.4725 sec/batch\n",
      "Epoch 20/40  Iteration 5280/11040 Training loss: 1.2885 0.4771 sec/batch\n",
      "Epoch 20/40  Iteration 5281/11040 Training loss: 1.2883 0.4732 sec/batch\n",
      "Epoch 20/40  Iteration 5282/11040 Training loss: 1.2875 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5283/11040 Training loss: 1.2867 0.4724 sec/batch\n",
      "Epoch 20/40  Iteration 5284/11040 Training loss: 1.2866 0.4678 sec/batch\n",
      "Epoch 20/40  Iteration 5285/11040 Training loss: 1.2863 0.4831 sec/batch\n",
      "Epoch 20/40  Iteration 5286/11040 Training loss: 1.2854 0.4755 sec/batch\n",
      "Epoch 20/40  Iteration 5287/11040 Training loss: 1.2856 0.4886 sec/batch\n",
      "Epoch 20/40  Iteration 5288/11040 Training loss: 1.2848 0.4760 sec/batch\n",
      "Epoch 20/40  Iteration 5289/11040 Training loss: 1.2845 0.4763 sec/batch\n",
      "Epoch 20/40  Iteration 5290/11040 Training loss: 1.2842 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5291/11040 Training loss: 1.2833 0.4608 sec/batch\n",
      "Epoch 20/40  Iteration 5292/11040 Training loss: 1.2828 0.4755 sec/batch\n",
      "Epoch 20/40  Iteration 5293/11040 Training loss: 1.2823 0.4734 sec/batch\n",
      "Epoch 20/40  Iteration 5294/11040 Training loss: 1.2827 0.4780 sec/batch\n",
      "Epoch 20/40  Iteration 5295/11040 Training loss: 1.2826 0.4713 sec/batch\n",
      "Epoch 20/40  Iteration 5296/11040 Training loss: 1.2823 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5297/11040 Training loss: 1.2826 0.4758 sec/batch\n",
      "Epoch 20/40  Iteration 5298/11040 Training loss: 1.2822 0.4747 sec/batch\n",
      "Epoch 20/40  Iteration 5299/11040 Training loss: 1.2823 0.4770 sec/batch\n",
      "Epoch 20/40  Iteration 5300/11040 Training loss: 1.2822 0.4719 sec/batch\n",
      "Epoch 20/40  Iteration 5301/11040 Training loss: 1.2819 0.4617 sec/batch\n",
      "Epoch 20/40  Iteration 5302/11040 Training loss: 1.2819 0.4719 sec/batch\n",
      "Epoch 20/40  Iteration 5303/11040 Training loss: 1.2822 0.4617 sec/batch\n",
      "Epoch 20/40  Iteration 5304/11040 Training loss: 1.2818 0.4745 sec/batch\n",
      "Epoch 20/40  Iteration 5305/11040 Training loss: 1.2819 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5306/11040 Training loss: 1.2818 0.4747 sec/batch\n",
      "Epoch 20/40  Iteration 5307/11040 Training loss: 1.2818 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5308/11040 Training loss: 1.2817 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5309/11040 Training loss: 1.2819 0.4757 sec/batch\n",
      "Epoch 20/40  Iteration 5310/11040 Training loss: 1.2817 0.4772 sec/batch\n",
      "Epoch 20/40  Iteration 5311/11040 Training loss: 1.2815 0.4736 sec/batch\n",
      "Epoch 20/40  Iteration 5312/11040 Training loss: 1.2815 0.4742 sec/batch\n",
      "Epoch 20/40  Iteration 5313/11040 Training loss: 1.2814 0.4707 sec/batch\n",
      "Epoch 20/40  Iteration 5314/11040 Training loss: 1.2814 0.4615 sec/batch\n",
      "Epoch 20/40  Iteration 5315/11040 Training loss: 1.2810 0.4592 sec/batch\n",
      "Epoch 20/40  Iteration 5316/11040 Training loss: 1.2805 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5317/11040 Training loss: 1.2801 0.4753 sec/batch\n",
      "Epoch 20/40  Iteration 5318/11040 Training loss: 1.2797 0.4610 sec/batch\n",
      "Epoch 20/40  Iteration 5319/11040 Training loss: 1.2796 0.4732 sec/batch\n",
      "Epoch 20/40  Iteration 5320/11040 Training loss: 1.2798 0.4762 sec/batch\n",
      "Epoch 20/40  Iteration 5321/11040 Training loss: 1.2794 0.4735 sec/batch\n",
      "Epoch 20/40  Iteration 5322/11040 Training loss: 1.2791 0.4598 sec/batch\n",
      "Epoch 20/40  Iteration 5323/11040 Training loss: 1.2789 0.4745 sec/batch\n",
      "Epoch 20/40  Iteration 5324/11040 Training loss: 1.2788 0.4605 sec/batch\n",
      "Epoch 20/40  Iteration 5325/11040 Training loss: 1.2785 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5326/11040 Training loss: 1.2785 0.4735 sec/batch\n",
      "Epoch 20/40  Iteration 5327/11040 Training loss: 1.2784 0.4588 sec/batch\n",
      "Epoch 20/40  Iteration 5328/11040 Training loss: 1.2783 0.4755 sec/batch\n",
      "Epoch 20/40  Iteration 5329/11040 Training loss: 1.2782 0.4586 sec/batch\n",
      "Epoch 20/40  Iteration 5330/11040 Training loss: 1.2781 0.4733 sec/batch\n",
      "Epoch 20/40  Iteration 5331/11040 Training loss: 1.2776 0.4772 sec/batch\n",
      "Epoch 20/40  Iteration 5332/11040 Training loss: 1.2767 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5333/11040 Training loss: 1.2767 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5334/11040 Training loss: 1.2767 0.4752 sec/batch\n",
      "Epoch 20/40  Iteration 5335/11040 Training loss: 1.2769 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5336/11040 Training loss: 1.2769 0.4603 sec/batch\n",
      "Epoch 20/40  Iteration 5337/11040 Training loss: 1.2766 0.4594 sec/batch\n",
      "Epoch 20/40  Iteration 5338/11040 Training loss: 1.2770 0.4790 sec/batch\n",
      "Epoch 20/40  Iteration 5339/11040 Training loss: 1.2774 0.4707 sec/batch\n",
      "Epoch 20/40  Iteration 5340/11040 Training loss: 1.2778 0.4759 sec/batch\n",
      "Epoch 20/40  Iteration 5341/11040 Training loss: 1.2781 0.4720 sec/batch\n",
      "Epoch 20/40  Iteration 5342/11040 Training loss: 1.2785 0.4621 sec/batch\n",
      "Epoch 20/40  Iteration 5343/11040 Training loss: 1.2787 0.4586 sec/batch\n",
      "Epoch 20/40  Iteration 5344/11040 Training loss: 1.2785 0.4575 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40  Iteration 5345/11040 Training loss: 1.2786 0.4595 sec/batch\n",
      "Epoch 20/40  Iteration 5346/11040 Training loss: 1.2787 0.4585 sec/batch\n",
      "Epoch 20/40  Iteration 5347/11040 Training loss: 1.2783 0.4603 sec/batch\n",
      "Epoch 20/40  Iteration 5348/11040 Training loss: 1.2784 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5349/11040 Training loss: 1.2783 0.4761 sec/batch\n",
      "Epoch 20/40  Iteration 5350/11040 Training loss: 1.2781 0.4730 sec/batch\n",
      "Epoch 20/40  Iteration 5351/11040 Training loss: 1.2781 0.4776 sec/batch\n",
      "Epoch 20/40  Iteration 5352/11040 Training loss: 1.2783 0.4732 sec/batch\n",
      "Epoch 20/40  Iteration 5353/11040 Training loss: 1.2787 0.4744 sec/batch\n",
      "Epoch 20/40  Iteration 5354/11040 Training loss: 1.2787 0.4604 sec/batch\n",
      "Epoch 20/40  Iteration 5355/11040 Training loss: 1.2787 0.4729 sec/batch\n",
      "Epoch 20/40  Iteration 5356/11040 Training loss: 1.2790 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5357/11040 Training loss: 1.2792 0.4751 sec/batch\n",
      "Epoch 20/40  Iteration 5358/11040 Training loss: 1.2793 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5359/11040 Training loss: 1.2797 0.4752 sec/batch\n",
      "Epoch 20/40  Iteration 5360/11040 Training loss: 1.2802 0.4733 sec/batch\n",
      "Epoch 20/40  Iteration 5361/11040 Training loss: 1.2801 0.4761 sec/batch\n",
      "Epoch 20/40  Iteration 5362/11040 Training loss: 1.2804 0.4732 sec/batch\n",
      "Epoch 20/40  Iteration 5363/11040 Training loss: 1.2806 0.4615 sec/batch\n",
      "Epoch 20/40  Iteration 5364/11040 Training loss: 1.2807 0.4576 sec/batch\n",
      "Epoch 20/40  Iteration 5365/11040 Training loss: 1.2807 0.4599 sec/batch\n",
      "Epoch 20/40  Iteration 5366/11040 Training loss: 1.2808 0.4604 sec/batch\n",
      "Epoch 20/40  Iteration 5367/11040 Training loss: 1.2811 0.4578 sec/batch\n",
      "Epoch 20/40  Iteration 5368/11040 Training loss: 1.2816 0.4738 sec/batch\n",
      "Epoch 20/40  Iteration 5369/11040 Training loss: 1.2817 0.4660 sec/batch\n",
      "Epoch 20/40  Iteration 5370/11040 Training loss: 1.2819 0.4840 sec/batch\n",
      "Epoch 20/40  Iteration 5371/11040 Training loss: 1.2820 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5372/11040 Training loss: 1.2821 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5373/11040 Training loss: 1.2819 0.4745 sec/batch\n",
      "Epoch 20/40  Iteration 5374/11040 Training loss: 1.2821 0.4739 sec/batch\n",
      "Epoch 20/40  Iteration 5375/11040 Training loss: 1.2819 0.4768 sec/batch\n",
      "Epoch 20/40  Iteration 5376/11040 Training loss: 1.2817 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5377/11040 Training loss: 1.2818 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5378/11040 Training loss: 1.2817 0.4760 sec/batch\n",
      "Epoch 20/40  Iteration 5379/11040 Training loss: 1.2819 0.4760 sec/batch\n",
      "Epoch 20/40  Iteration 5380/11040 Training loss: 1.2820 0.4736 sec/batch\n",
      "Epoch 20/40  Iteration 5381/11040 Training loss: 1.2820 0.4743 sec/batch\n",
      "Epoch 20/40  Iteration 5382/11040 Training loss: 1.2821 0.4607 sec/batch\n",
      "Epoch 20/40  Iteration 5383/11040 Training loss: 1.2823 0.4742 sec/batch\n",
      "Epoch 20/40  Iteration 5384/11040 Training loss: 1.2824 0.4750 sec/batch\n",
      "Epoch 20/40  Iteration 5385/11040 Training loss: 1.2824 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5386/11040 Training loss: 1.2826 0.4751 sec/batch\n",
      "Epoch 20/40  Iteration 5387/11040 Training loss: 1.2826 0.4738 sec/batch\n",
      "Epoch 20/40  Iteration 5388/11040 Training loss: 1.2824 0.4593 sec/batch\n",
      "Epoch 20/40  Iteration 5389/11040 Training loss: 1.2824 0.4758 sec/batch\n",
      "Epoch 20/40  Iteration 5390/11040 Training loss: 1.2823 0.4730 sec/batch\n",
      "Epoch 20/40  Iteration 5391/11040 Training loss: 1.2824 0.4604 sec/batch\n",
      "Epoch 20/40  Iteration 5392/11040 Training loss: 1.2823 0.4759 sec/batch\n",
      "Epoch 20/40  Iteration 5393/11040 Training loss: 1.2822 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5394/11040 Training loss: 1.2822 0.4606 sec/batch\n",
      "Epoch 20/40  Iteration 5395/11040 Training loss: 1.2821 0.4730 sec/batch\n",
      "Epoch 20/40  Iteration 5396/11040 Training loss: 1.2823 0.4605 sec/batch\n",
      "Epoch 20/40  Iteration 5397/11040 Training loss: 1.2823 0.4599 sec/batch\n",
      "Epoch 20/40  Iteration 5398/11040 Training loss: 1.2825 0.4735 sec/batch\n",
      "Epoch 20/40  Iteration 5399/11040 Training loss: 1.2825 0.4737 sec/batch\n",
      "Epoch 20/40  Iteration 5400/11040 Training loss: 1.2825 0.4739 sec/batch\n",
      "Epoch 20/40  Iteration 5401/11040 Training loss: 1.2823 0.4762 sec/batch\n",
      "Epoch 20/40  Iteration 5402/11040 Training loss: 1.2825 0.4732 sec/batch\n",
      "Epoch 20/40  Iteration 5403/11040 Training loss: 1.2827 0.4771 sec/batch\n",
      "Epoch 20/40  Iteration 5404/11040 Training loss: 1.2827 0.4732 sec/batch\n",
      "Epoch 20/40  Iteration 5405/11040 Training loss: 1.2827 0.4596 sec/batch\n",
      "Epoch 20/40  Iteration 5406/11040 Training loss: 1.2827 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5407/11040 Training loss: 1.2828 0.4605 sec/batch\n",
      "Epoch 20/40  Iteration 5408/11040 Training loss: 1.2830 0.4573 sec/batch\n",
      "Epoch 20/40  Iteration 5409/11040 Training loss: 1.2832 0.4587 sec/batch\n",
      "Epoch 20/40  Iteration 5410/11040 Training loss: 1.2834 0.4596 sec/batch\n",
      "Epoch 20/40  Iteration 5411/11040 Training loss: 1.2834 0.4586 sec/batch\n",
      "Epoch 20/40  Iteration 5412/11040 Training loss: 1.2834 0.4753 sec/batch\n",
      "Epoch 20/40  Iteration 5413/11040 Training loss: 1.2833 0.4596 sec/batch\n",
      "Epoch 20/40  Iteration 5414/11040 Training loss: 1.2833 0.4739 sec/batch\n",
      "Epoch 20/40  Iteration 5415/11040 Training loss: 1.2834 0.4742 sec/batch\n",
      "Epoch 20/40  Iteration 5416/11040 Training loss: 1.2835 0.4742 sec/batch\n",
      "Epoch 20/40  Iteration 5417/11040 Training loss: 1.2836 0.4759 sec/batch\n",
      "Epoch 20/40  Iteration 5418/11040 Training loss: 1.2835 0.4733 sec/batch\n",
      "Epoch 20/40  Iteration 5419/11040 Training loss: 1.2836 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5420/11040 Training loss: 1.2836 0.4751 sec/batch\n",
      "Epoch 20/40  Iteration 5421/11040 Training loss: 1.2837 0.4774 sec/batch\n",
      "Epoch 20/40  Iteration 5422/11040 Training loss: 1.2837 0.4575 sec/batch\n",
      "Epoch 20/40  Iteration 5423/11040 Training loss: 1.2839 0.4750 sec/batch\n",
      "Epoch 20/40  Iteration 5424/11040 Training loss: 1.2841 0.4753 sec/batch\n",
      "Epoch 20/40  Iteration 5425/11040 Training loss: 1.2841 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5426/11040 Training loss: 1.2842 0.4755 sec/batch\n",
      "Epoch 20/40  Iteration 5427/11040 Training loss: 1.2841 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5428/11040 Training loss: 1.2842 0.4715 sec/batch\n",
      "Epoch 20/40  Iteration 5429/11040 Training loss: 1.2843 0.4604 sec/batch\n",
      "Epoch 20/40  Iteration 5430/11040 Training loss: 1.2844 0.4753 sec/batch\n",
      "Epoch 20/40  Iteration 5431/11040 Training loss: 1.2846 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5432/11040 Training loss: 1.2847 0.4734 sec/batch\n",
      "Epoch 20/40  Iteration 5433/11040 Training loss: 1.2847 0.4764 sec/batch\n",
      "Epoch 20/40  Iteration 5434/11040 Training loss: 1.2847 0.4745 sec/batch\n",
      "Epoch 20/40  Iteration 5435/11040 Training loss: 1.2846 0.4767 sec/batch\n",
      "Epoch 20/40  Iteration 5436/11040 Training loss: 1.2846 0.4729 sec/batch\n",
      "Epoch 20/40  Iteration 5437/11040 Training loss: 1.2846 0.4552 sec/batch\n",
      "Epoch 20/40  Iteration 5438/11040 Training loss: 1.2847 0.4576 sec/batch\n",
      "Epoch 20/40  Iteration 5439/11040 Training loss: 1.2850 0.4753 sec/batch\n",
      "Epoch 20/40  Iteration 5440/11040 Training loss: 1.2849 0.4635 sec/batch\n",
      "Epoch 20/40  Iteration 5441/11040 Training loss: 1.2850 0.4664 sec/batch\n",
      "Epoch 20/40  Iteration 5442/11040 Training loss: 1.2851 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5443/11040 Training loss: 1.2852 0.4760 sec/batch\n",
      "Epoch 20/40  Iteration 5444/11040 Training loss: 1.2853 0.4750 sec/batch\n",
      "Epoch 20/40  Iteration 5445/11040 Training loss: 1.2855 0.4739 sec/batch\n",
      "Epoch 20/40  Iteration 5446/11040 Training loss: 1.2856 0.4739 sec/batch\n",
      "Epoch 20/40  Iteration 5447/11040 Training loss: 1.2857 0.4561 sec/batch\n",
      "Epoch 20/40  Iteration 5448/11040 Training loss: 1.2859 0.4578 sec/batch\n",
      "Epoch 20/40  Iteration 5449/11040 Training loss: 1.2859 0.4744 sec/batch\n",
      "Epoch 20/40  Iteration 5450/11040 Training loss: 1.2859 0.4749 sec/batch\n",
      "Epoch 20/40  Iteration 5451/11040 Training loss: 1.2859 0.4750 sec/batch\n",
      "Epoch 20/40  Iteration 5452/11040 Training loss: 1.2859 0.4755 sec/batch\n",
      "Epoch 20/40  Iteration 5453/11040 Training loss: 1.2858 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5454/11040 Training loss: 1.2857 0.4740 sec/batch\n",
      "Epoch 20/40  Iteration 5455/11040 Training loss: 1.2857 0.4576 sec/batch\n",
      "Epoch 20/40  Iteration 5456/11040 Training loss: 1.2857 0.4625 sec/batch\n",
      "Epoch 20/40  Iteration 5457/11040 Training loss: 1.2857 0.4726 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40  Iteration 5458/11040 Training loss: 1.2857 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5459/11040 Training loss: 1.2855 0.4733 sec/batch\n",
      "Epoch 20/40  Iteration 5460/11040 Training loss: 1.2854 0.4757 sec/batch\n",
      "Epoch 20/40  Iteration 5461/11040 Training loss: 1.2853 0.4751 sec/batch\n",
      "Epoch 20/40  Iteration 5462/11040 Training loss: 1.2852 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5463/11040 Training loss: 1.2852 0.4742 sec/batch\n",
      "Epoch 20/40  Iteration 5464/11040 Training loss: 1.2851 0.4606 sec/batch\n",
      "Epoch 20/40  Iteration 5465/11040 Training loss: 1.2850 0.4574 sec/batch\n",
      "Epoch 20/40  Iteration 5466/11040 Training loss: 1.2848 0.4599 sec/batch\n",
      "Epoch 20/40  Iteration 5467/11040 Training loss: 1.2847 0.4741 sec/batch\n",
      "Epoch 20/40  Iteration 5468/11040 Training loss: 1.2848 0.4746 sec/batch\n",
      "Epoch 20/40  Iteration 5469/11040 Training loss: 1.2847 0.4758 sec/batch\n",
      "Epoch 20/40  Iteration 5470/11040 Training loss: 1.2847 0.4594 sec/batch\n",
      "Epoch 20/40  Iteration 5471/11040 Training loss: 1.2845 0.4607 sec/batch\n",
      "Epoch 20/40  Iteration 5472/11040 Training loss: 1.2844 0.4692 sec/batch\n",
      "Epoch 20/40  Iteration 5473/11040 Training loss: 1.2845 0.4789 sec/batch\n",
      "Epoch 20/40  Iteration 5474/11040 Training loss: 1.2846 0.4744 sec/batch\n",
      "Epoch 20/40  Iteration 5475/11040 Training loss: 1.2846 0.4758 sec/batch\n",
      "Epoch 20/40  Iteration 5476/11040 Training loss: 1.2846 0.4586 sec/batch\n",
      "Epoch 20/40  Iteration 5477/11040 Training loss: 1.2845 0.4583 sec/batch\n",
      "Epoch 20/40  Iteration 5478/11040 Training loss: 1.2844 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5479/11040 Training loss: 1.2845 0.4605 sec/batch\n",
      "Epoch 20/40  Iteration 5480/11040 Training loss: 1.2845 0.4585 sec/batch\n",
      "Epoch 20/40  Iteration 5481/11040 Training loss: 1.2846 0.4585 sec/batch\n",
      "Epoch 20/40  Iteration 5482/11040 Training loss: 1.2846 0.4596 sec/batch\n",
      "Epoch 20/40  Iteration 5483/11040 Training loss: 1.2847 0.4598 sec/batch\n",
      "Epoch 20/40  Iteration 5484/11040 Training loss: 1.2848 0.4582 sec/batch\n",
      "Epoch 20/40  Iteration 5485/11040 Training loss: 1.2848 0.4600 sec/batch\n",
      "Epoch 20/40  Iteration 5486/11040 Training loss: 1.2848 0.4729 sec/batch\n",
      "Epoch 20/40  Iteration 5487/11040 Training loss: 1.2847 0.4763 sec/batch\n",
      "Epoch 20/40  Iteration 5488/11040 Training loss: 1.2847 0.4738 sec/batch\n",
      "Epoch 20/40  Iteration 5489/11040 Training loss: 1.2846 0.4751 sec/batch\n",
      "Epoch 20/40  Iteration 5490/11040 Training loss: 1.2846 0.4596 sec/batch\n",
      "Epoch 20/40  Iteration 5491/11040 Training loss: 1.2846 0.4584 sec/batch\n",
      "Epoch 20/40  Iteration 5492/11040 Training loss: 1.2845 0.4580 sec/batch\n",
      "Epoch 20/40  Iteration 5493/11040 Training loss: 1.2845 0.4616 sec/batch\n",
      "Epoch 20/40  Iteration 5494/11040 Training loss: 1.2846 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5495/11040 Training loss: 1.2844 0.4750 sec/batch\n",
      "Epoch 20/40  Iteration 5496/11040 Training loss: 1.2844 0.4758 sec/batch\n",
      "Epoch 20/40  Iteration 5497/11040 Training loss: 1.2844 0.4748 sec/batch\n",
      "Epoch 20/40  Iteration 5498/11040 Training loss: 1.2845 0.4599 sec/batch\n",
      "Epoch 20/40  Iteration 5499/11040 Training loss: 1.2845 0.4717 sec/batch\n",
      "Epoch 20/40  Iteration 5500/11040 Training loss: 1.2845 0.4654 sec/batch\n",
      "Validation loss: 1.27583 Saving checkpoint!\n",
      "Epoch 20/40  Iteration 5501/11040 Training loss: 1.2850 0.4688 sec/batch\n",
      "Epoch 20/40  Iteration 5502/11040 Training loss: 1.2850 0.4777 sec/batch\n",
      "Epoch 20/40  Iteration 5503/11040 Training loss: 1.2850 0.4752 sec/batch\n",
      "Epoch 20/40  Iteration 5504/11040 Training loss: 1.2850 0.4743 sec/batch\n",
      "Epoch 20/40  Iteration 5505/11040 Training loss: 1.2850 0.4738 sec/batch\n",
      "Epoch 20/40  Iteration 5506/11040 Training loss: 1.2852 0.4602 sec/batch\n",
      "Epoch 20/40  Iteration 5507/11040 Training loss: 1.2853 0.4598 sec/batch\n",
      "Epoch 20/40  Iteration 5508/11040 Training loss: 1.2854 0.4746 sec/batch\n",
      "Epoch 20/40  Iteration 5509/11040 Training loss: 1.2854 0.4729 sec/batch\n",
      "Epoch 20/40  Iteration 5510/11040 Training loss: 1.2855 0.4812 sec/batch\n",
      "Epoch 20/40  Iteration 5511/11040 Training loss: 1.2856 0.4695 sec/batch\n",
      "Epoch 20/40  Iteration 5512/11040 Training loss: 1.2856 0.4734 sec/batch\n",
      "Epoch 20/40  Iteration 5513/11040 Training loss: 1.2857 0.4760 sec/batch\n",
      "Epoch 20/40  Iteration 5514/11040 Training loss: 1.2858 0.4584 sec/batch\n",
      "Epoch 20/40  Iteration 5515/11040 Training loss: 1.2861 0.4590 sec/batch\n",
      "Epoch 20/40  Iteration 5516/11040 Training loss: 1.2863 0.4609 sec/batch\n",
      "Epoch 20/40  Iteration 5517/11040 Training loss: 1.2864 0.4731 sec/batch\n",
      "Epoch 20/40  Iteration 5518/11040 Training loss: 1.2864 0.4728 sec/batch\n",
      "Epoch 20/40  Iteration 5519/11040 Training loss: 1.2864 0.4631 sec/batch\n",
      "Epoch 20/40  Iteration 5520/11040 Training loss: 1.2865 0.4960 sec/batch\n",
      "Epoch 21/40  Iteration 5521/11040 Training loss: 1.3558 0.4685 sec/batch\n",
      "Epoch 21/40  Iteration 5522/11040 Training loss: 1.3197 0.4597 sec/batch\n",
      "Epoch 21/40  Iteration 5523/11040 Training loss: 1.3138 0.4611 sec/batch\n",
      "Epoch 21/40  Iteration 5524/11040 Training loss: 1.3144 0.4725 sec/batch\n",
      "Epoch 21/40  Iteration 5525/11040 Training loss: 1.3131 0.4752 sec/batch\n",
      "Epoch 21/40  Iteration 5526/11040 Training loss: 1.3090 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5527/11040 Training loss: 1.2984 0.4584 sec/batch\n",
      "Epoch 21/40  Iteration 5528/11040 Training loss: 1.2968 0.4617 sec/batch\n",
      "Epoch 21/40  Iteration 5529/11040 Training loss: 1.2926 0.4766 sec/batch\n",
      "Epoch 21/40  Iteration 5530/11040 Training loss: 1.2920 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5531/11040 Training loss: 1.2892 0.4730 sec/batch\n",
      "Epoch 21/40  Iteration 5532/11040 Training loss: 1.2883 0.4605 sec/batch\n",
      "Epoch 21/40  Iteration 5533/11040 Training loss: 1.2856 0.4745 sec/batch\n",
      "Epoch 21/40  Iteration 5534/11040 Training loss: 1.2825 0.4741 sec/batch\n",
      "Epoch 21/40  Iteration 5535/11040 Training loss: 1.2814 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5536/11040 Training loss: 1.2815 0.4593 sec/batch\n",
      "Epoch 21/40  Iteration 5537/11040 Training loss: 1.2788 0.4797 sec/batch\n",
      "Epoch 21/40  Iteration 5538/11040 Training loss: 1.2771 0.4706 sec/batch\n",
      "Epoch 21/40  Iteration 5539/11040 Training loss: 1.2768 0.4592 sec/batch\n",
      "Epoch 21/40  Iteration 5540/11040 Training loss: 1.2769 0.4582 sec/batch\n",
      "Epoch 21/40  Iteration 5541/11040 Training loss: 1.2787 0.4762 sec/batch\n",
      "Epoch 21/40  Iteration 5542/11040 Training loss: 1.2791 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5543/11040 Training loss: 1.2789 0.4728 sec/batch\n",
      "Epoch 21/40  Iteration 5544/11040 Training loss: 1.2793 0.4603 sec/batch\n",
      "Epoch 21/40  Iteration 5545/11040 Training loss: 1.2784 0.4596 sec/batch\n",
      "Epoch 21/40  Iteration 5546/11040 Training loss: 1.2781 0.4739 sec/batch\n",
      "Epoch 21/40  Iteration 5547/11040 Training loss: 1.2776 0.4689 sec/batch\n",
      "Epoch 21/40  Iteration 5548/11040 Training loss: 1.2773 0.4658 sec/batch\n",
      "Epoch 21/40  Iteration 5549/11040 Training loss: 1.2775 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5550/11040 Training loss: 1.2775 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5551/11040 Training loss: 1.2773 0.4757 sec/batch\n",
      "Epoch 21/40  Iteration 5552/11040 Training loss: 1.2771 0.4584 sec/batch\n",
      "Epoch 21/40  Iteration 5553/11040 Training loss: 1.2767 0.4580 sec/batch\n",
      "Epoch 21/40  Iteration 5554/11040 Training loss: 1.2764 0.4748 sec/batch\n",
      "Epoch 21/40  Iteration 5555/11040 Training loss: 1.2761 0.4750 sec/batch\n",
      "Epoch 21/40  Iteration 5556/11040 Training loss: 1.2767 0.4764 sec/batch\n",
      "Epoch 21/40  Iteration 5557/11040 Training loss: 1.2764 0.4723 sec/batch\n",
      "Epoch 21/40  Iteration 5558/11040 Training loss: 1.2757 0.4750 sec/batch\n",
      "Epoch 21/40  Iteration 5559/11040 Training loss: 1.2750 0.4759 sec/batch\n",
      "Epoch 21/40  Iteration 5560/11040 Training loss: 1.2750 0.4741 sec/batch\n",
      "Epoch 21/40  Iteration 5561/11040 Training loss: 1.2743 0.4595 sec/batch\n",
      "Epoch 21/40  Iteration 5562/11040 Training loss: 1.2734 0.4741 sec/batch\n",
      "Epoch 21/40  Iteration 5563/11040 Training loss: 1.2737 0.4752 sec/batch\n",
      "Epoch 21/40  Iteration 5564/11040 Training loss: 1.2732 0.4744 sec/batch\n",
      "Epoch 21/40  Iteration 5565/11040 Training loss: 1.2730 0.4755 sec/batch\n",
      "Epoch 21/40  Iteration 5566/11040 Training loss: 1.2728 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5567/11040 Training loss: 1.2719 0.4686 sec/batch\n",
      "Epoch 21/40  Iteration 5568/11040 Training loss: 1.2717 0.4646 sec/batch\n",
      "Epoch 21/40  Iteration 5569/11040 Training loss: 1.2711 0.4592 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40  Iteration 5570/11040 Training loss: 1.2715 0.4588 sec/batch\n",
      "Epoch 21/40  Iteration 5571/11040 Training loss: 1.2712 0.4604 sec/batch\n",
      "Epoch 21/40  Iteration 5572/11040 Training loss: 1.2710 0.4588 sec/batch\n",
      "Epoch 21/40  Iteration 5573/11040 Training loss: 1.2712 0.4575 sec/batch\n",
      "Epoch 21/40  Iteration 5574/11040 Training loss: 1.2706 0.4602 sec/batch\n",
      "Epoch 21/40  Iteration 5575/11040 Training loss: 1.2705 0.4733 sec/batch\n",
      "Epoch 21/40  Iteration 5576/11040 Training loss: 1.2705 0.4604 sec/batch\n",
      "Epoch 21/40  Iteration 5577/11040 Training loss: 1.2705 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5578/11040 Training loss: 1.2704 0.4606 sec/batch\n",
      "Epoch 21/40  Iteration 5579/11040 Training loss: 1.2706 0.4748 sec/batch\n",
      "Epoch 21/40  Iteration 5580/11040 Training loss: 1.2703 0.4752 sec/batch\n",
      "Epoch 21/40  Iteration 5581/11040 Training loss: 1.2704 0.4593 sec/batch\n",
      "Epoch 21/40  Iteration 5582/11040 Training loss: 1.2703 0.4744 sec/batch\n",
      "Epoch 21/40  Iteration 5583/11040 Training loss: 1.2703 0.4594 sec/batch\n",
      "Epoch 21/40  Iteration 5584/11040 Training loss: 1.2702 0.4745 sec/batch\n",
      "Epoch 21/40  Iteration 5585/11040 Training loss: 1.2705 0.4745 sec/batch\n",
      "Epoch 21/40  Iteration 5586/11040 Training loss: 1.2703 0.4741 sec/batch\n",
      "Epoch 21/40  Iteration 5587/11040 Training loss: 1.2702 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5588/11040 Training loss: 1.2703 0.4769 sec/batch\n",
      "Epoch 21/40  Iteration 5589/11040 Training loss: 1.2702 0.4731 sec/batch\n",
      "Epoch 21/40  Iteration 5590/11040 Training loss: 1.2703 0.4594 sec/batch\n",
      "Epoch 21/40  Iteration 5591/11040 Training loss: 1.2699 0.4583 sec/batch\n",
      "Epoch 21/40  Iteration 5592/11040 Training loss: 1.2694 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5593/11040 Training loss: 1.2690 0.4748 sec/batch\n",
      "Epoch 21/40  Iteration 5594/11040 Training loss: 1.2686 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5595/11040 Training loss: 1.2686 0.4743 sec/batch\n",
      "Epoch 21/40  Iteration 5596/11040 Training loss: 1.2688 0.4763 sec/batch\n",
      "Epoch 21/40  Iteration 5597/11040 Training loss: 1.2683 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5598/11040 Training loss: 1.2681 0.4754 sec/batch\n",
      "Epoch 21/40  Iteration 5599/11040 Training loss: 1.2678 0.4595 sec/batch\n",
      "Epoch 21/40  Iteration 5600/11040 Training loss: 1.2677 0.4764 sec/batch\n",
      "Epoch 21/40  Iteration 5601/11040 Training loss: 1.2675 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5602/11040 Training loss: 1.2675 0.4750 sec/batch\n",
      "Epoch 21/40  Iteration 5603/11040 Training loss: 1.2675 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5604/11040 Training loss: 1.2674 0.4759 sec/batch\n",
      "Epoch 21/40  Iteration 5605/11040 Training loss: 1.2672 0.4827 sec/batch\n",
      "Epoch 21/40  Iteration 5606/11040 Training loss: 1.2672 0.4680 sec/batch\n",
      "Epoch 21/40  Iteration 5607/11040 Training loss: 1.2666 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5608/11040 Training loss: 1.2656 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5609/11040 Training loss: 1.2656 0.4734 sec/batch\n",
      "Epoch 21/40  Iteration 5610/11040 Training loss: 1.2657 0.4607 sec/batch\n",
      "Epoch 21/40  Iteration 5611/11040 Training loss: 1.2659 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5612/11040 Training loss: 1.2659 0.4592 sec/batch\n",
      "Epoch 21/40  Iteration 5613/11040 Training loss: 1.2657 0.4597 sec/batch\n",
      "Epoch 21/40  Iteration 5614/11040 Training loss: 1.2661 0.4748 sec/batch\n",
      "Epoch 21/40  Iteration 5615/11040 Training loss: 1.2665 0.4738 sec/batch\n",
      "Epoch 21/40  Iteration 5616/11040 Training loss: 1.2668 0.4752 sec/batch\n",
      "Epoch 21/40  Iteration 5617/11040 Training loss: 1.2673 0.4699 sec/batch\n",
      "Epoch 21/40  Iteration 5618/11040 Training loss: 1.2678 0.4746 sec/batch\n",
      "Epoch 21/40  Iteration 5619/11040 Training loss: 1.2680 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5620/11040 Training loss: 1.2678 0.4745 sec/batch\n",
      "Epoch 21/40  Iteration 5621/11040 Training loss: 1.2679 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5622/11040 Training loss: 1.2681 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5623/11040 Training loss: 1.2677 0.4730 sec/batch\n",
      "Epoch 21/40  Iteration 5624/11040 Training loss: 1.2679 0.4773 sec/batch\n",
      "Epoch 21/40  Iteration 5625/11040 Training loss: 1.2676 0.4746 sec/batch\n",
      "Epoch 21/40  Iteration 5626/11040 Training loss: 1.2675 0.4744 sec/batch\n",
      "Epoch 21/40  Iteration 5627/11040 Training loss: 1.2675 0.4593 sec/batch\n",
      "Epoch 21/40  Iteration 5628/11040 Training loss: 1.2677 0.4586 sec/batch\n",
      "Epoch 21/40  Iteration 5629/11040 Training loss: 1.2681 0.4610 sec/batch\n",
      "Epoch 21/40  Iteration 5630/11040 Training loss: 1.2681 0.4575 sec/batch\n",
      "Epoch 21/40  Iteration 5631/11040 Training loss: 1.2682 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5632/11040 Training loss: 1.2684 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5633/11040 Training loss: 1.2686 0.4750 sec/batch\n",
      "Epoch 21/40  Iteration 5634/11040 Training loss: 1.2687 0.4590 sec/batch\n",
      "Epoch 21/40  Iteration 5635/11040 Training loss: 1.2691 0.4595 sec/batch\n",
      "Epoch 21/40  Iteration 5636/11040 Training loss: 1.2695 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5637/11040 Training loss: 1.2694 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5638/11040 Training loss: 1.2697 0.4737 sec/batch\n",
      "Epoch 21/40  Iteration 5639/11040 Training loss: 1.2699 0.4593 sec/batch\n",
      "Epoch 21/40  Iteration 5640/11040 Training loss: 1.2700 0.4745 sec/batch\n",
      "Epoch 21/40  Iteration 5641/11040 Training loss: 1.2700 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5642/11040 Training loss: 1.2701 0.4743 sec/batch\n",
      "Epoch 21/40  Iteration 5643/11040 Training loss: 1.2703 0.4753 sec/batch\n",
      "Epoch 21/40  Iteration 5644/11040 Training loss: 1.2708 0.4759 sec/batch\n",
      "Epoch 21/40  Iteration 5645/11040 Training loss: 1.2708 0.4738 sec/batch\n",
      "Epoch 21/40  Iteration 5646/11040 Training loss: 1.2711 0.4750 sec/batch\n",
      "Epoch 21/40  Iteration 5647/11040 Training loss: 1.2711 0.4731 sec/batch\n",
      "Epoch 21/40  Iteration 5648/11040 Training loss: 1.2711 0.4618 sec/batch\n",
      "Epoch 21/40  Iteration 5649/11040 Training loss: 1.2709 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5650/11040 Training loss: 1.2712 0.4734 sec/batch\n",
      "Epoch 21/40  Iteration 5651/11040 Training loss: 1.2711 0.4750 sec/batch\n",
      "Epoch 21/40  Iteration 5652/11040 Training loss: 1.2709 0.4744 sec/batch\n",
      "Epoch 21/40  Iteration 5653/11040 Training loss: 1.2709 0.4760 sec/batch\n",
      "Epoch 21/40  Iteration 5654/11040 Training loss: 1.2709 0.4744 sec/batch\n",
      "Epoch 21/40  Iteration 5655/11040 Training loss: 1.2710 0.4586 sec/batch\n",
      "Epoch 21/40  Iteration 5656/11040 Training loss: 1.2712 0.4619 sec/batch\n",
      "Epoch 21/40  Iteration 5657/11040 Training loss: 1.2711 0.4729 sec/batch\n",
      "Epoch 21/40  Iteration 5658/11040 Training loss: 1.2714 0.4593 sec/batch\n",
      "Epoch 21/40  Iteration 5659/11040 Training loss: 1.2715 0.4719 sec/batch\n",
      "Epoch 21/40  Iteration 5660/11040 Training loss: 1.2716 0.4772 sec/batch\n",
      "Epoch 21/40  Iteration 5661/11040 Training loss: 1.2716 0.4744 sec/batch\n",
      "Epoch 21/40  Iteration 5662/11040 Training loss: 1.2717 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5663/11040 Training loss: 1.2717 0.4761 sec/batch\n",
      "Epoch 21/40  Iteration 5664/11040 Training loss: 1.2715 0.4757 sec/batch\n",
      "Epoch 21/40  Iteration 5665/11040 Training loss: 1.2715 0.4735 sec/batch\n",
      "Epoch 21/40  Iteration 5666/11040 Training loss: 1.2715 0.4752 sec/batch\n",
      "Epoch 21/40  Iteration 5667/11040 Training loss: 1.2715 0.4739 sec/batch\n",
      "Epoch 21/40  Iteration 5668/11040 Training loss: 1.2715 0.4607 sec/batch\n",
      "Epoch 21/40  Iteration 5669/11040 Training loss: 1.2714 0.4761 sec/batch\n",
      "Epoch 21/40  Iteration 5670/11040 Training loss: 1.2714 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5671/11040 Training loss: 1.2714 0.4716 sec/batch\n",
      "Epoch 21/40  Iteration 5672/11040 Training loss: 1.2715 0.4763 sec/batch\n",
      "Epoch 21/40  Iteration 5673/11040 Training loss: 1.2716 0.4854 sec/batch\n",
      "Epoch 21/40  Iteration 5674/11040 Training loss: 1.2717 0.4634 sec/batch\n",
      "Epoch 21/40  Iteration 5675/11040 Training loss: 1.2717 0.4760 sec/batch\n",
      "Epoch 21/40  Iteration 5676/11040 Training loss: 1.2716 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5677/11040 Training loss: 1.2715 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5678/11040 Training loss: 1.2717 0.4759 sec/batch\n",
      "Epoch 21/40  Iteration 5679/11040 Training loss: 1.2719 0.4731 sec/batch\n",
      "Epoch 21/40  Iteration 5680/11040 Training loss: 1.2718 0.4644 sec/batch\n",
      "Epoch 21/40  Iteration 5681/11040 Training loss: 1.2719 0.4713 sec/batch\n",
      "Epoch 21/40  Iteration 5682/11040 Training loss: 1.2719 0.4746 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40  Iteration 5683/11040 Training loss: 1.2720 0.4731 sec/batch\n",
      "Epoch 21/40  Iteration 5684/11040 Training loss: 1.2722 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5685/11040 Training loss: 1.2723 0.4600 sec/batch\n",
      "Epoch 21/40  Iteration 5686/11040 Training loss: 1.2726 0.4590 sec/batch\n",
      "Epoch 21/40  Iteration 5687/11040 Training loss: 1.2726 0.4596 sec/batch\n",
      "Epoch 21/40  Iteration 5688/11040 Training loss: 1.2725 0.4584 sec/batch\n",
      "Epoch 21/40  Iteration 5689/11040 Training loss: 1.2725 0.4760 sec/batch\n",
      "Epoch 21/40  Iteration 5690/11040 Training loss: 1.2725 0.4730 sec/batch\n",
      "Epoch 21/40  Iteration 5691/11040 Training loss: 1.2727 0.4618 sec/batch\n",
      "Epoch 21/40  Iteration 5692/11040 Training loss: 1.2728 0.4575 sec/batch\n",
      "Epoch 21/40  Iteration 5693/11040 Training loss: 1.2728 0.4754 sec/batch\n",
      "Epoch 21/40  Iteration 5694/11040 Training loss: 1.2728 0.4754 sec/batch\n",
      "Epoch 21/40  Iteration 5695/11040 Training loss: 1.2729 0.4740 sec/batch\n",
      "Epoch 21/40  Iteration 5696/11040 Training loss: 1.2729 0.4735 sec/batch\n",
      "Epoch 21/40  Iteration 5697/11040 Training loss: 1.2730 0.4763 sec/batch\n",
      "Epoch 21/40  Iteration 5698/11040 Training loss: 1.2729 0.4720 sec/batch\n",
      "Epoch 21/40  Iteration 5699/11040 Training loss: 1.2732 0.4771 sec/batch\n",
      "Epoch 21/40  Iteration 5700/11040 Training loss: 1.2733 0.4729 sec/batch\n",
      "Epoch 21/40  Iteration 5701/11040 Training loss: 1.2733 0.4603 sec/batch\n",
      "Epoch 21/40  Iteration 5702/11040 Training loss: 1.2734 0.4605 sec/batch\n",
      "Epoch 21/40  Iteration 5703/11040 Training loss: 1.2732 0.4730 sec/batch\n",
      "Epoch 21/40  Iteration 5704/11040 Training loss: 1.2734 0.4749 sec/batch\n",
      "Epoch 21/40  Iteration 5705/11040 Training loss: 1.2734 0.4606 sec/batch\n",
      "Epoch 21/40  Iteration 5706/11040 Training loss: 1.2735 0.4585 sec/batch\n",
      "Epoch 21/40  Iteration 5707/11040 Training loss: 1.2737 0.4592 sec/batch\n",
      "Epoch 21/40  Iteration 5708/11040 Training loss: 1.2738 0.4588 sec/batch\n",
      "Epoch 21/40  Iteration 5709/11040 Training loss: 1.2738 0.4755 sec/batch\n",
      "Epoch 21/40  Iteration 5710/11040 Training loss: 1.2738 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5711/11040 Training loss: 1.2737 0.4711 sec/batch\n",
      "Epoch 21/40  Iteration 5712/11040 Training loss: 1.2737 0.5900 sec/batch\n",
      "Epoch 21/40  Iteration 5713/11040 Training loss: 1.2737 0.6531 sec/batch\n",
      "Epoch 21/40  Iteration 5714/11040 Training loss: 1.2737 0.6664 sec/batch\n",
      "Epoch 21/40  Iteration 5715/11040 Training loss: 1.2740 0.7157 sec/batch\n",
      "Epoch 21/40  Iteration 5716/11040 Training loss: 1.2739 0.6339 sec/batch\n",
      "Epoch 21/40  Iteration 5717/11040 Training loss: 1.2741 0.6089 sec/batch\n",
      "Epoch 21/40  Iteration 5718/11040 Training loss: 1.2742 0.6423 sec/batch\n",
      "Epoch 21/40  Iteration 5719/11040 Training loss: 1.2743 0.6679 sec/batch\n",
      "Epoch 21/40  Iteration 5720/11040 Training loss: 1.2744 0.6874 sec/batch\n",
      "Epoch 21/40  Iteration 5721/11040 Training loss: 1.2745 0.6779 sec/batch\n",
      "Epoch 21/40  Iteration 5722/11040 Training loss: 1.2747 0.8100 sec/batch\n",
      "Epoch 21/40  Iteration 5723/11040 Training loss: 1.2748 0.7348 sec/batch\n",
      "Epoch 21/40  Iteration 5724/11040 Training loss: 1.2750 0.7465 sec/batch\n",
      "Epoch 21/40  Iteration 5725/11040 Training loss: 1.2749 0.7325 sec/batch\n",
      "Epoch 21/40  Iteration 5726/11040 Training loss: 1.2749 0.6906 sec/batch\n",
      "Epoch 21/40  Iteration 5727/11040 Training loss: 1.2749 0.6962 sec/batch\n",
      "Epoch 21/40  Iteration 5728/11040 Training loss: 1.2749 0.6705 sec/batch\n",
      "Epoch 21/40  Iteration 5729/11040 Training loss: 1.2747 0.6891 sec/batch\n",
      "Epoch 21/40  Iteration 5730/11040 Training loss: 1.2747 0.5990 sec/batch\n",
      "Epoch 21/40  Iteration 5731/11040 Training loss: 1.2746 0.6452 sec/batch\n",
      "Epoch 21/40  Iteration 5732/11040 Training loss: 1.2747 0.6272 sec/batch\n",
      "Epoch 21/40  Iteration 5733/11040 Training loss: 1.2747 0.6429 sec/batch\n",
      "Epoch 21/40  Iteration 5734/11040 Training loss: 1.2747 0.6124 sec/batch\n",
      "Epoch 21/40  Iteration 5735/11040 Training loss: 1.2746 0.6123 sec/batch\n",
      "Epoch 21/40  Iteration 5736/11040 Training loss: 1.2744 0.6424 sec/batch\n",
      "Epoch 21/40  Iteration 5737/11040 Training loss: 1.2743 0.6143 sec/batch\n",
      "Epoch 21/40  Iteration 5738/11040 Training loss: 1.2742 0.5952 sec/batch\n",
      "Epoch 21/40  Iteration 5739/11040 Training loss: 1.2741 0.6101 sec/batch\n",
      "Epoch 21/40  Iteration 5740/11040 Training loss: 1.2741 0.6129 sec/batch\n",
      "Epoch 21/40  Iteration 5741/11040 Training loss: 1.2739 0.6105 sec/batch\n",
      "Epoch 21/40  Iteration 5742/11040 Training loss: 1.2738 0.6129 sec/batch\n",
      "Epoch 21/40  Iteration 5743/11040 Training loss: 1.2737 0.6106 sec/batch\n",
      "Epoch 21/40  Iteration 5744/11040 Training loss: 1.2737 0.6117 sec/batch\n",
      "Epoch 21/40  Iteration 5745/11040 Training loss: 1.2737 0.6269 sec/batch\n",
      "Epoch 21/40  Iteration 5746/11040 Training loss: 1.2737 0.5968 sec/batch\n",
      "Epoch 21/40  Iteration 5747/11040 Training loss: 1.2735 0.6245 sec/batch\n",
      "Epoch 21/40  Iteration 5748/11040 Training loss: 1.2734 0.6293 sec/batch\n",
      "Epoch 21/40  Iteration 5749/11040 Training loss: 1.2735 0.6011 sec/batch\n",
      "Epoch 21/40  Iteration 5750/11040 Training loss: 1.2735 0.6053 sec/batch\n",
      "Epoch 21/40  Iteration 5751/11040 Training loss: 1.2735 0.6274 sec/batch\n",
      "Epoch 21/40  Iteration 5752/11040 Training loss: 1.2735 0.6413 sec/batch\n",
      "Epoch 21/40  Iteration 5753/11040 Training loss: 1.2735 0.6150 sec/batch\n",
      "Epoch 21/40  Iteration 5754/11040 Training loss: 1.2733 0.6126 sec/batch\n",
      "Epoch 21/40  Iteration 5755/11040 Training loss: 1.2734 0.6053 sec/batch\n",
      "Epoch 21/40  Iteration 5756/11040 Training loss: 1.2735 0.6079 sec/batch\n",
      "Epoch 21/40  Iteration 5757/11040 Training loss: 1.2735 0.6400 sec/batch\n",
      "Epoch 21/40  Iteration 5758/11040 Training loss: 1.2736 0.6210 sec/batch\n",
      "Epoch 21/40  Iteration 5759/11040 Training loss: 1.2737 0.6191 sec/batch\n",
      "Epoch 21/40  Iteration 5760/11040 Training loss: 1.2738 0.6131 sec/batch\n",
      "Epoch 21/40  Iteration 5761/11040 Training loss: 1.2738 0.6255 sec/batch\n",
      "Epoch 21/40  Iteration 5762/11040 Training loss: 1.2738 0.6123 sec/batch\n",
      "Epoch 21/40  Iteration 5763/11040 Training loss: 1.2737 0.6279 sec/batch\n",
      "Epoch 21/40  Iteration 5764/11040 Training loss: 1.2737 0.6128 sec/batch\n",
      "Epoch 21/40  Iteration 5765/11040 Training loss: 1.2736 0.5319 sec/batch\n",
      "Epoch 21/40  Iteration 5766/11040 Training loss: 1.2736 0.4652 sec/batch\n",
      "Epoch 21/40  Iteration 5767/11040 Training loss: 1.2736 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5768/11040 Training loss: 1.2735 0.4732 sec/batch\n",
      "Epoch 21/40  Iteration 5769/11040 Training loss: 1.2735 0.4766 sec/batch\n",
      "Epoch 21/40  Iteration 5770/11040 Training loss: 1.2736 0.4728 sec/batch\n",
      "Epoch 21/40  Iteration 5771/11040 Training loss: 1.2735 0.4601 sec/batch\n",
      "Epoch 21/40  Iteration 5772/11040 Training loss: 1.2735 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5773/11040 Training loss: 1.2735 0.4734 sec/batch\n",
      "Epoch 21/40  Iteration 5774/11040 Training loss: 1.2736 0.4602 sec/batch\n",
      "Epoch 21/40  Iteration 5775/11040 Training loss: 1.2735 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5776/11040 Training loss: 1.2736 0.4748 sec/batch\n",
      "Epoch 21/40  Iteration 5777/11040 Training loss: 1.2737 0.4585 sec/batch\n",
      "Epoch 21/40  Iteration 5778/11040 Training loss: 1.2737 0.4761 sec/batch\n",
      "Epoch 21/40  Iteration 5779/11040 Training loss: 1.2737 0.4737 sec/batch\n",
      "Epoch 21/40  Iteration 5780/11040 Training loss: 1.2737 0.4751 sec/batch\n",
      "Epoch 21/40  Iteration 5781/11040 Training loss: 1.2737 0.4609 sec/batch\n",
      "Epoch 21/40  Iteration 5782/11040 Training loss: 1.2739 0.4731 sec/batch\n",
      "Epoch 21/40  Iteration 5783/11040 Training loss: 1.2740 0.4599 sec/batch\n",
      "Epoch 21/40  Iteration 5784/11040 Training loss: 1.2741 0.4739 sec/batch\n",
      "Epoch 21/40  Iteration 5785/11040 Training loss: 1.2740 0.4620 sec/batch\n",
      "Epoch 21/40  Iteration 5786/11040 Training loss: 1.2741 0.4712 sec/batch\n",
      "Epoch 21/40  Iteration 5787/11040 Training loss: 1.2743 0.4742 sec/batch\n",
      "Epoch 21/40  Iteration 5788/11040 Training loss: 1.2744 0.4768 sec/batch\n",
      "Epoch 21/40  Iteration 5789/11040 Training loss: 1.2745 0.4765 sec/batch\n",
      "Epoch 21/40  Iteration 5790/11040 Training loss: 1.2746 0.4727 sec/batch\n",
      "Epoch 21/40  Iteration 5791/11040 Training loss: 1.2749 0.4586 sec/batch\n",
      "Epoch 21/40  Iteration 5792/11040 Training loss: 1.2751 0.4616 sec/batch\n",
      "Epoch 21/40  Iteration 5793/11040 Training loss: 1.2752 0.4726 sec/batch\n",
      "Epoch 21/40  Iteration 5794/11040 Training loss: 1.2753 0.4748 sec/batch\n",
      "Epoch 21/40  Iteration 5795/11040 Training loss: 1.2753 0.4733 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40  Iteration 5796/11040 Training loss: 1.2754 0.4840 sec/batch\n",
      "Epoch 22/40  Iteration 5797/11040 Training loss: 1.3540 0.4781 sec/batch\n",
      "Epoch 22/40  Iteration 5798/11040 Training loss: 1.3152 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 5799/11040 Training loss: 1.3118 0.4752 sec/batch\n",
      "Epoch 22/40  Iteration 5800/11040 Training loss: 1.3080 0.4605 sec/batch\n",
      "Epoch 22/40  Iteration 5801/11040 Training loss: 1.3061 0.4738 sec/batch\n",
      "Epoch 22/40  Iteration 5802/11040 Training loss: 1.3031 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 5803/11040 Training loss: 1.2934 0.4589 sec/batch\n",
      "Epoch 22/40  Iteration 5804/11040 Training loss: 1.2906 0.4603 sec/batch\n",
      "Epoch 22/40  Iteration 5805/11040 Training loss: 1.2868 0.4734 sec/batch\n",
      "Epoch 22/40  Iteration 5806/11040 Training loss: 1.2865 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 5807/11040 Training loss: 1.2828 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 5808/11040 Training loss: 1.2821 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5809/11040 Training loss: 1.2792 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 5810/11040 Training loss: 1.2764 0.4770 sec/batch\n",
      "Epoch 22/40  Iteration 5811/11040 Training loss: 1.2758 0.4721 sec/batch\n",
      "Epoch 22/40  Iteration 5812/11040 Training loss: 1.2760 0.4590 sec/batch\n",
      "Epoch 22/40  Iteration 5813/11040 Training loss: 1.2727 0.4749 sec/batch\n",
      "Epoch 22/40  Iteration 5814/11040 Training loss: 1.2705 0.4754 sec/batch\n",
      "Epoch 22/40  Iteration 5815/11040 Training loss: 1.2700 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5816/11040 Training loss: 1.2702 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 5817/11040 Training loss: 1.2713 0.4598 sec/batch\n",
      "Epoch 22/40  Iteration 5818/11040 Training loss: 1.2723 0.4647 sec/batch\n",
      "Epoch 22/40  Iteration 5819/11040 Training loss: 1.2718 0.4687 sec/batch\n",
      "Epoch 22/40  Iteration 5820/11040 Training loss: 1.2721 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 5821/11040 Training loss: 1.2711 0.4741 sec/batch\n",
      "Epoch 22/40  Iteration 5822/11040 Training loss: 1.2703 0.4777 sec/batch\n",
      "Epoch 22/40  Iteration 5823/11040 Training loss: 1.2698 0.4723 sec/batch\n",
      "Epoch 22/40  Iteration 5824/11040 Training loss: 1.2698 0.4746 sec/batch\n",
      "Epoch 22/40  Iteration 5825/11040 Training loss: 1.2699 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5826/11040 Training loss: 1.2697 0.4749 sec/batch\n",
      "Epoch 22/40  Iteration 5827/11040 Training loss: 1.2696 0.4769 sec/batch\n",
      "Epoch 22/40  Iteration 5828/11040 Training loss: 1.2693 0.4731 sec/batch\n",
      "Epoch 22/40  Iteration 5829/11040 Training loss: 1.2685 0.4738 sec/batch\n",
      "Epoch 22/40  Iteration 5830/11040 Training loss: 1.2681 0.4754 sec/batch\n",
      "Epoch 22/40  Iteration 5831/11040 Training loss: 1.2680 0.4730 sec/batch\n",
      "Epoch 22/40  Iteration 5832/11040 Training loss: 1.2685 0.4603 sec/batch\n",
      "Epoch 22/40  Iteration 5833/11040 Training loss: 1.2684 0.4587 sec/batch\n",
      "Epoch 22/40  Iteration 5834/11040 Training loss: 1.2674 0.4593 sec/batch\n",
      "Epoch 22/40  Iteration 5835/11040 Training loss: 1.2668 0.4606 sec/batch\n",
      "Epoch 22/40  Iteration 5836/11040 Training loss: 1.2669 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5837/11040 Training loss: 1.2664 0.4607 sec/batch\n",
      "Epoch 22/40  Iteration 5838/11040 Training loss: 1.2654 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5839/11040 Training loss: 1.2656 0.4619 sec/batch\n",
      "Epoch 22/40  Iteration 5840/11040 Training loss: 1.2648 0.4703 sec/batch\n",
      "Epoch 22/40  Iteration 5841/11040 Training loss: 1.2645 0.4669 sec/batch\n",
      "Epoch 22/40  Iteration 5842/11040 Training loss: 1.2640 0.4694 sec/batch\n",
      "Epoch 22/40  Iteration 5843/11040 Training loss: 1.2633 0.4738 sec/batch\n",
      "Epoch 22/40  Iteration 5844/11040 Training loss: 1.2632 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 5845/11040 Training loss: 1.2628 0.4709 sec/batch\n",
      "Epoch 22/40  Iteration 5846/11040 Training loss: 1.2630 0.4615 sec/batch\n",
      "Epoch 22/40  Iteration 5847/11040 Training loss: 1.2628 0.4743 sec/batch\n",
      "Epoch 22/40  Iteration 5848/11040 Training loss: 1.2627 0.4604 sec/batch\n",
      "Epoch 22/40  Iteration 5849/11040 Training loss: 1.2630 0.4730 sec/batch\n",
      "Epoch 22/40  Iteration 5850/11040 Training loss: 1.2627 0.4749 sec/batch\n",
      "Epoch 22/40  Iteration 5851/11040 Training loss: 1.2627 0.4761 sec/batch\n",
      "Epoch 22/40  Iteration 5852/11040 Training loss: 1.2629 0.4741 sec/batch\n",
      "Epoch 22/40  Iteration 5853/11040 Training loss: 1.2627 0.4591 sec/batch\n",
      "Epoch 22/40  Iteration 5854/11040 Training loss: 1.2626 0.4642 sec/batch\n",
      "Epoch 22/40  Iteration 5855/11040 Training loss: 1.2629 0.4867 sec/batch\n",
      "Epoch 22/40  Iteration 5856/11040 Training loss: 1.2625 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5857/11040 Training loss: 1.2625 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 5858/11040 Training loss: 1.2625 0.4756 sec/batch\n",
      "Epoch 22/40  Iteration 5859/11040 Training loss: 1.2622 0.4743 sec/batch\n",
      "Epoch 22/40  Iteration 5860/11040 Training loss: 1.2621 0.4735 sec/batch\n",
      "Epoch 22/40  Iteration 5861/11040 Training loss: 1.2622 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 5862/11040 Training loss: 1.2620 0.4757 sec/batch\n",
      "Epoch 22/40  Iteration 5863/11040 Training loss: 1.2618 0.4741 sec/batch\n",
      "Epoch 22/40  Iteration 5864/11040 Training loss: 1.2620 0.4760 sec/batch\n",
      "Epoch 22/40  Iteration 5865/11040 Training loss: 1.2616 0.4731 sec/batch\n",
      "Epoch 22/40  Iteration 5866/11040 Training loss: 1.2617 0.4597 sec/batch\n",
      "Epoch 22/40  Iteration 5867/11040 Training loss: 1.2612 0.4739 sec/batch\n",
      "Epoch 22/40  Iteration 5868/11040 Training loss: 1.2608 0.4755 sec/batch\n",
      "Epoch 22/40  Iteration 5869/11040 Training loss: 1.2606 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5870/11040 Training loss: 1.2602 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 5871/11040 Training loss: 1.2599 0.4604 sec/batch\n",
      "Epoch 22/40  Iteration 5872/11040 Training loss: 1.2602 0.4591 sec/batch\n",
      "Epoch 22/40  Iteration 5873/11040 Training loss: 1.2598 0.4744 sec/batch\n",
      "Epoch 22/40  Iteration 5874/11040 Training loss: 1.2594 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 5875/11040 Training loss: 1.2592 0.4749 sec/batch\n",
      "Epoch 22/40  Iteration 5876/11040 Training loss: 1.2593 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 5877/11040 Training loss: 1.2590 0.4748 sec/batch\n",
      "Epoch 22/40  Iteration 5878/11040 Training loss: 1.2592 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 5879/11040 Training loss: 1.2592 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5880/11040 Training loss: 1.2591 0.4584 sec/batch\n",
      "Epoch 22/40  Iteration 5881/11040 Training loss: 1.2589 0.4597 sec/batch\n",
      "Epoch 22/40  Iteration 5882/11040 Training loss: 1.2589 0.4754 sec/batch\n",
      "Epoch 22/40  Iteration 5883/11040 Training loss: 1.2584 0.4733 sec/batch\n",
      "Epoch 22/40  Iteration 5884/11040 Training loss: 1.2575 0.4763 sec/batch\n",
      "Epoch 22/40  Iteration 5885/11040 Training loss: 1.2575 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 5886/11040 Training loss: 1.2577 0.4721 sec/batch\n",
      "Epoch 22/40  Iteration 5887/11040 Training loss: 1.2578 0.4611 sec/batch\n",
      "Epoch 22/40  Iteration 5888/11040 Training loss: 1.2578 0.4755 sec/batch\n",
      "Epoch 22/40  Iteration 5889/11040 Training loss: 1.2576 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5890/11040 Training loss: 1.2580 0.4647 sec/batch\n",
      "Epoch 22/40  Iteration 5891/11040 Training loss: 1.2584 0.4694 sec/batch\n",
      "Epoch 22/40  Iteration 5892/11040 Training loss: 1.2587 0.4608 sec/batch\n",
      "Epoch 22/40  Iteration 5893/11040 Training loss: 1.2591 0.4720 sec/batch\n",
      "Epoch 22/40  Iteration 5894/11040 Training loss: 1.2594 0.4749 sec/batch\n",
      "Epoch 22/40  Iteration 5895/11040 Training loss: 1.2597 0.4770 sec/batch\n",
      "Epoch 22/40  Iteration 5896/11040 Training loss: 1.2595 0.4736 sec/batch\n",
      "Epoch 22/40  Iteration 5897/11040 Training loss: 1.2596 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5898/11040 Training loss: 1.2598 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 5899/11040 Training loss: 1.2594 0.4589 sec/batch\n",
      "Epoch 22/40  Iteration 5900/11040 Training loss: 1.2595 0.4735 sec/batch\n",
      "Epoch 22/40  Iteration 5901/11040 Training loss: 1.2595 0.4604 sec/batch\n",
      "Epoch 22/40  Iteration 5902/11040 Training loss: 1.2594 0.4576 sec/batch\n",
      "Epoch 22/40  Iteration 5903/11040 Training loss: 1.2594 0.4730 sec/batch\n",
      "Epoch 22/40  Iteration 5904/11040 Training loss: 1.2596 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 5905/11040 Training loss: 1.2599 0.4765 sec/batch\n",
      "Epoch 22/40  Iteration 5906/11040 Training loss: 1.2600 0.4755 sec/batch\n",
      "Epoch 22/40  Iteration 5907/11040 Training loss: 1.2600 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5908/11040 Training loss: 1.2603 0.4738 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40  Iteration 5909/11040 Training loss: 1.2604 0.4688 sec/batch\n",
      "Epoch 22/40  Iteration 5910/11040 Training loss: 1.2604 0.4810 sec/batch\n",
      "Epoch 22/40  Iteration 5911/11040 Training loss: 1.2608 0.4746 sec/batch\n",
      "Epoch 22/40  Iteration 5912/11040 Training loss: 1.2611 0.4748 sec/batch\n",
      "Epoch 22/40  Iteration 5913/11040 Training loss: 1.2610 0.4585 sec/batch\n",
      "Epoch 22/40  Iteration 5914/11040 Training loss: 1.2613 0.4597 sec/batch\n",
      "Epoch 22/40  Iteration 5915/11040 Training loss: 1.2615 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 5916/11040 Training loss: 1.2616 0.4749 sec/batch\n",
      "Epoch 22/40  Iteration 5917/11040 Training loss: 1.2617 0.4743 sec/batch\n",
      "Epoch 22/40  Iteration 5918/11040 Training loss: 1.2617 0.4746 sec/batch\n",
      "Epoch 22/40  Iteration 5919/11040 Training loss: 1.2620 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 5920/11040 Training loss: 1.2625 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5921/11040 Training loss: 1.2626 0.4771 sec/batch\n",
      "Epoch 22/40  Iteration 5922/11040 Training loss: 1.2628 0.4722 sec/batch\n",
      "Epoch 22/40  Iteration 5923/11040 Training loss: 1.2628 0.4603 sec/batch\n",
      "Epoch 22/40  Iteration 5924/11040 Training loss: 1.2629 0.4759 sec/batch\n",
      "Epoch 22/40  Iteration 5925/11040 Training loss: 1.2628 0.4725 sec/batch\n",
      "Epoch 22/40  Iteration 5926/11040 Training loss: 1.2630 0.4766 sec/batch\n",
      "Epoch 22/40  Iteration 5927/11040 Training loss: 1.2629 0.4766 sec/batch\n",
      "Epoch 22/40  Iteration 5928/11040 Training loss: 1.2627 0.4722 sec/batch\n",
      "Epoch 22/40  Iteration 5929/11040 Training loss: 1.2628 0.4770 sec/batch\n",
      "Epoch 22/40  Iteration 5930/11040 Training loss: 1.2626 0.4724 sec/batch\n",
      "Epoch 22/40  Iteration 5931/11040 Training loss: 1.2627 0.4661 sec/batch\n",
      "Epoch 22/40  Iteration 5932/11040 Training loss: 1.2629 0.4703 sec/batch\n",
      "Epoch 22/40  Iteration 5933/11040 Training loss: 1.2628 0.4737 sec/batch\n",
      "Epoch 22/40  Iteration 5934/11040 Training loss: 1.2630 0.4733 sec/batch\n",
      "Epoch 22/40  Iteration 5935/11040 Training loss: 1.2632 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 5936/11040 Training loss: 1.2632 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 5937/11040 Training loss: 1.2633 0.4734 sec/batch\n",
      "Epoch 22/40  Iteration 5938/11040 Training loss: 1.2634 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5939/11040 Training loss: 1.2633 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 5940/11040 Training loss: 1.2632 0.4602 sec/batch\n",
      "Epoch 22/40  Iteration 5941/11040 Training loss: 1.2633 0.4594 sec/batch\n",
      "Epoch 22/40  Iteration 5942/11040 Training loss: 1.2632 0.4730 sec/batch\n",
      "Epoch 22/40  Iteration 5943/11040 Training loss: 1.2633 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 5944/11040 Training loss: 1.2632 0.4746 sec/batch\n",
      "Epoch 22/40  Iteration 5945/11040 Training loss: 1.2632 0.4719 sec/batch\n",
      "Epoch 22/40  Iteration 5946/11040 Training loss: 1.2631 0.4757 sec/batch\n",
      "Epoch 22/40  Iteration 5947/11040 Training loss: 1.2631 0.4744 sec/batch\n",
      "Epoch 22/40  Iteration 5948/11040 Training loss: 1.2633 0.4595 sec/batch\n",
      "Epoch 22/40  Iteration 5949/11040 Training loss: 1.2634 0.4591 sec/batch\n",
      "Epoch 22/40  Iteration 5950/11040 Training loss: 1.2635 0.4597 sec/batch\n",
      "Epoch 22/40  Iteration 5951/11040 Training loss: 1.2636 0.4732 sec/batch\n",
      "Epoch 22/40  Iteration 5952/11040 Training loss: 1.2636 0.4617 sec/batch\n",
      "Epoch 22/40  Iteration 5953/11040 Training loss: 1.2635 0.4580 sec/batch\n",
      "Epoch 22/40  Iteration 5954/11040 Training loss: 1.2636 0.4586 sec/batch\n",
      "Epoch 22/40  Iteration 5955/11040 Training loss: 1.2638 0.4743 sec/batch\n",
      "Epoch 22/40  Iteration 5956/11040 Training loss: 1.2639 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 5957/11040 Training loss: 1.2640 0.4741 sec/batch\n",
      "Epoch 22/40  Iteration 5958/11040 Training loss: 1.2640 0.4758 sec/batch\n",
      "Epoch 22/40  Iteration 5959/11040 Training loss: 1.2641 0.4605 sec/batch\n",
      "Epoch 22/40  Iteration 5960/11040 Training loss: 1.2643 0.4719 sec/batch\n",
      "Epoch 22/40  Iteration 5961/11040 Training loss: 1.2644 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 5962/11040 Training loss: 1.2647 0.4755 sec/batch\n",
      "Epoch 22/40  Iteration 5963/11040 Training loss: 1.2647 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 5964/11040 Training loss: 1.2646 0.4753 sec/batch\n",
      "Epoch 22/40  Iteration 5965/11040 Training loss: 1.2645 0.4739 sec/batch\n",
      "Epoch 22/40  Iteration 5966/11040 Training loss: 1.2645 0.4622 sec/batch\n",
      "Epoch 22/40  Iteration 5967/11040 Training loss: 1.2647 0.4722 sec/batch\n",
      "Epoch 22/40  Iteration 5968/11040 Training loss: 1.2648 0.4752 sec/batch\n",
      "Epoch 22/40  Iteration 5969/11040 Training loss: 1.2649 0.4733 sec/batch\n",
      "Epoch 22/40  Iteration 5970/11040 Training loss: 1.2648 0.4609 sec/batch\n",
      "Epoch 22/40  Iteration 5971/11040 Training loss: 1.2649 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 5972/11040 Training loss: 1.2649 0.4577 sec/batch\n",
      "Epoch 22/40  Iteration 5973/11040 Training loss: 1.2650 0.4606 sec/batch\n",
      "Epoch 22/40  Iteration 5974/11040 Training loss: 1.2649 0.4731 sec/batch\n",
      "Epoch 22/40  Iteration 5975/11040 Training loss: 1.2652 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 5976/11040 Training loss: 1.2653 0.4744 sec/batch\n",
      "Epoch 22/40  Iteration 5977/11040 Training loss: 1.2653 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 5978/11040 Training loss: 1.2654 0.4763 sec/batch\n",
      "Epoch 22/40  Iteration 5979/11040 Training loss: 1.2653 0.4800 sec/batch\n",
      "Epoch 22/40  Iteration 5980/11040 Training loss: 1.2655 0.4685 sec/batch\n",
      "Epoch 22/40  Iteration 5981/11040 Training loss: 1.2656 0.4759 sec/batch\n",
      "Epoch 22/40  Iteration 5982/11040 Training loss: 1.2656 0.4731 sec/batch\n",
      "Epoch 22/40  Iteration 5983/11040 Training loss: 1.2659 0.4657 sec/batch\n",
      "Epoch 22/40  Iteration 5984/11040 Training loss: 1.2660 0.4696 sec/batch\n",
      "Epoch 22/40  Iteration 5985/11040 Training loss: 1.2660 0.4598 sec/batch\n",
      "Epoch 22/40  Iteration 5986/11040 Training loss: 1.2659 0.4586 sec/batch\n",
      "Epoch 22/40  Iteration 5987/11040 Training loss: 1.2657 0.4645 sec/batch\n",
      "Epoch 22/40  Iteration 5988/11040 Training loss: 1.2657 0.4695 sec/batch\n",
      "Epoch 22/40  Iteration 5989/11040 Training loss: 1.2657 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 5990/11040 Training loss: 1.2658 0.4589 sec/batch\n",
      "Epoch 22/40  Iteration 5991/11040 Training loss: 1.2661 0.4744 sec/batch\n",
      "Epoch 22/40  Iteration 5992/11040 Training loss: 1.2660 0.4763 sec/batch\n",
      "Epoch 22/40  Iteration 5993/11040 Training loss: 1.2661 0.4720 sec/batch\n",
      "Epoch 22/40  Iteration 5994/11040 Training loss: 1.2662 0.4759 sec/batch\n",
      "Epoch 22/40  Iteration 5995/11040 Training loss: 1.2663 0.4743 sec/batch\n",
      "Epoch 22/40  Iteration 5996/11040 Training loss: 1.2664 0.4765 sec/batch\n",
      "Epoch 22/40  Iteration 5997/11040 Training loss: 1.2665 0.4733 sec/batch\n",
      "Epoch 22/40  Iteration 5998/11040 Training loss: 1.2666 0.4760 sec/batch\n",
      "Epoch 22/40  Iteration 5999/11040 Training loss: 1.2667 0.4725 sec/batch\n",
      "Epoch 22/40  Iteration 6000/11040 Training loss: 1.2669 0.4603 sec/batch\n",
      "Validation loss: 1.26985 Saving checkpoint!\n",
      "Epoch 22/40  Iteration 6001/11040 Training loss: 1.2676 0.4688 sec/batch\n",
      "Epoch 22/40  Iteration 6002/11040 Training loss: 1.2676 0.4753 sec/batch\n",
      "Epoch 22/40  Iteration 6003/11040 Training loss: 1.2677 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 6004/11040 Training loss: 1.2677 0.4764 sec/batch\n",
      "Epoch 22/40  Iteration 6005/11040 Training loss: 1.2676 0.4738 sec/batch\n",
      "Epoch 22/40  Iteration 6006/11040 Training loss: 1.2675 0.4576 sec/batch\n",
      "Epoch 22/40  Iteration 6007/11040 Training loss: 1.2675 0.4743 sec/batch\n",
      "Epoch 22/40  Iteration 6008/11040 Training loss: 1.2675 0.4753 sec/batch\n",
      "Epoch 22/40  Iteration 6009/11040 Training loss: 1.2674 0.4744 sec/batch\n",
      "Epoch 22/40  Iteration 6010/11040 Training loss: 1.2674 0.4758 sec/batch\n",
      "Epoch 22/40  Iteration 6011/11040 Training loss: 1.2673 0.4773 sec/batch\n",
      "Epoch 22/40  Iteration 6012/11040 Training loss: 1.2671 0.4700 sec/batch\n",
      "Epoch 22/40  Iteration 6013/11040 Training loss: 1.2670 0.4769 sec/batch\n",
      "Epoch 22/40  Iteration 6014/11040 Training loss: 1.2669 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 6015/11040 Training loss: 1.2668 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 6016/11040 Training loss: 1.2667 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 6017/11040 Training loss: 1.2666 0.4745 sec/batch\n",
      "Epoch 22/40  Iteration 6018/11040 Training loss: 1.2664 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 6019/11040 Training loss: 1.2663 0.4756 sec/batch\n",
      "Epoch 22/40  Iteration 6020/11040 Training loss: 1.2664 0.4729 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40  Iteration 6021/11040 Training loss: 1.2664 0.4748 sec/batch\n",
      "Epoch 22/40  Iteration 6022/11040 Training loss: 1.2664 0.4600 sec/batch\n",
      "Epoch 22/40  Iteration 6023/11040 Training loss: 1.2662 0.4593 sec/batch\n",
      "Epoch 22/40  Iteration 6024/11040 Training loss: 1.2661 0.4584 sec/batch\n",
      "Epoch 22/40  Iteration 6025/11040 Training loss: 1.2662 0.4606 sec/batch\n",
      "Epoch 22/40  Iteration 6026/11040 Training loss: 1.2663 0.4592 sec/batch\n",
      "Epoch 22/40  Iteration 6027/11040 Training loss: 1.2662 0.4729 sec/batch\n",
      "Epoch 22/40  Iteration 6028/11040 Training loss: 1.2663 0.4751 sec/batch\n",
      "Epoch 22/40  Iteration 6029/11040 Training loss: 1.2662 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 6030/11040 Training loss: 1.2661 0.4760 sec/batch\n",
      "Epoch 22/40  Iteration 6031/11040 Training loss: 1.2662 0.4733 sec/batch\n",
      "Epoch 22/40  Iteration 6032/11040 Training loss: 1.2662 0.4760 sec/batch\n",
      "Epoch 22/40  Iteration 6033/11040 Training loss: 1.2663 0.4741 sec/batch\n",
      "Epoch 22/40  Iteration 6034/11040 Training loss: 1.2663 0.4747 sec/batch\n",
      "Epoch 22/40  Iteration 6035/11040 Training loss: 1.2664 0.4746 sec/batch\n",
      "Epoch 22/40  Iteration 6036/11040 Training loss: 1.2664 0.4762 sec/batch\n",
      "Epoch 22/40  Iteration 6037/11040 Training loss: 1.2665 0.4729 sec/batch\n",
      "Epoch 22/40  Iteration 6038/11040 Training loss: 1.2665 0.4604 sec/batch\n",
      "Epoch 22/40  Iteration 6039/11040 Training loss: 1.2664 0.4764 sec/batch\n",
      "Epoch 22/40  Iteration 6040/11040 Training loss: 1.2664 0.4573 sec/batch\n",
      "Epoch 22/40  Iteration 6041/11040 Training loss: 1.2663 0.4585 sec/batch\n",
      "Epoch 22/40  Iteration 6042/11040 Training loss: 1.2663 0.4592 sec/batch\n",
      "Epoch 22/40  Iteration 6043/11040 Training loss: 1.2663 0.4593 sec/batch\n",
      "Epoch 22/40  Iteration 6044/11040 Training loss: 1.2662 0.4747 sec/batch\n",
      "Epoch 22/40  Iteration 6045/11040 Training loss: 1.2662 0.4742 sec/batch\n",
      "Epoch 22/40  Iteration 6046/11040 Training loss: 1.2663 0.4760 sec/batch\n",
      "Epoch 22/40  Iteration 6047/11040 Training loss: 1.2661 0.4734 sec/batch\n",
      "Epoch 22/40  Iteration 6048/11040 Training loss: 1.2661 0.4769 sec/batch\n",
      "Epoch 22/40  Iteration 6049/11040 Training loss: 1.2661 0.4855 sec/batch\n",
      "Epoch 22/40  Iteration 6050/11040 Training loss: 1.2662 0.4587 sec/batch\n",
      "Epoch 22/40  Iteration 6051/11040 Training loss: 1.2662 0.4581 sec/batch\n",
      "Epoch 22/40  Iteration 6052/11040 Training loss: 1.2662 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 6053/11040 Training loss: 1.2663 0.4772 sec/batch\n",
      "Epoch 22/40  Iteration 6054/11040 Training loss: 1.2663 0.4730 sec/batch\n",
      "Epoch 22/40  Iteration 6055/11040 Training loss: 1.2662 0.4590 sec/batch\n",
      "Epoch 22/40  Iteration 6056/11040 Training loss: 1.2662 0.4594 sec/batch\n",
      "Epoch 22/40  Iteration 6057/11040 Training loss: 1.2663 0.4740 sec/batch\n",
      "Epoch 22/40  Iteration 6058/11040 Training loss: 1.2664 0.4595 sec/batch\n",
      "Epoch 22/40  Iteration 6059/11040 Training loss: 1.2664 0.4598 sec/batch\n",
      "Epoch 22/40  Iteration 6060/11040 Training loss: 1.2665 0.4722 sec/batch\n",
      "Epoch 22/40  Iteration 6061/11040 Training loss: 1.2665 0.4626 sec/batch\n",
      "Epoch 22/40  Iteration 6062/11040 Training loss: 1.2666 0.4723 sec/batch\n",
      "Epoch 22/40  Iteration 6063/11040 Training loss: 1.2667 0.4604 sec/batch\n",
      "Epoch 22/40  Iteration 6064/11040 Training loss: 1.2668 0.4761 sec/batch\n",
      "Epoch 22/40  Iteration 6065/11040 Training loss: 1.2669 0.4739 sec/batch\n",
      "Epoch 22/40  Iteration 6066/11040 Training loss: 1.2670 0.4593 sec/batch\n",
      "Epoch 22/40  Iteration 6067/11040 Training loss: 1.2673 0.4585 sec/batch\n",
      "Epoch 22/40  Iteration 6068/11040 Training loss: 1.2675 0.4593 sec/batch\n",
      "Epoch 22/40  Iteration 6069/11040 Training loss: 1.2676 0.4591 sec/batch\n",
      "Epoch 22/40  Iteration 6070/11040 Training loss: 1.2676 0.4584 sec/batch\n",
      "Epoch 22/40  Iteration 6071/11040 Training loss: 1.2676 0.4750 sec/batch\n",
      "Epoch 22/40  Iteration 6072/11040 Training loss: 1.2677 0.4602 sec/batch\n",
      "Epoch 23/40  Iteration 6073/11040 Training loss: 1.3349 0.4730 sec/batch\n",
      "Epoch 23/40  Iteration 6074/11040 Training loss: 1.2966 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6075/11040 Training loss: 1.2930 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6076/11040 Training loss: 1.2929 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6077/11040 Training loss: 1.2905 0.4593 sec/batch\n",
      "Epoch 23/40  Iteration 6078/11040 Training loss: 1.2872 0.4585 sec/batch\n",
      "Epoch 23/40  Iteration 6079/11040 Training loss: 1.2794 0.4672 sec/batch\n",
      "Epoch 23/40  Iteration 6080/11040 Training loss: 1.2774 0.4829 sec/batch\n",
      "Epoch 23/40  Iteration 6081/11040 Training loss: 1.2737 0.4738 sec/batch\n",
      "Epoch 23/40  Iteration 6082/11040 Training loss: 1.2734 0.4749 sec/batch\n",
      "Epoch 23/40  Iteration 6083/11040 Training loss: 1.2697 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6084/11040 Training loss: 1.2691 0.4764 sec/batch\n",
      "Epoch 23/40  Iteration 6085/11040 Training loss: 1.2666 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6086/11040 Training loss: 1.2646 0.4734 sec/batch\n",
      "Epoch 23/40  Iteration 6087/11040 Training loss: 1.2636 0.4751 sec/batch\n",
      "Epoch 23/40  Iteration 6088/11040 Training loss: 1.2640 0.4729 sec/batch\n",
      "Epoch 23/40  Iteration 6089/11040 Training loss: 1.2612 0.4740 sec/batch\n",
      "Epoch 23/40  Iteration 6090/11040 Training loss: 1.2592 0.4771 sec/batch\n",
      "Epoch 23/40  Iteration 6091/11040 Training loss: 1.2584 0.4594 sec/batch\n",
      "Epoch 23/40  Iteration 6092/11040 Training loss: 1.2581 0.4737 sec/batch\n",
      "Epoch 23/40  Iteration 6093/11040 Training loss: 1.2597 0.4727 sec/batch\n",
      "Epoch 23/40  Iteration 6094/11040 Training loss: 1.2601 0.4661 sec/batch\n",
      "Epoch 23/40  Iteration 6095/11040 Training loss: 1.2599 0.4700 sec/batch\n",
      "Epoch 23/40  Iteration 6096/11040 Training loss: 1.2605 0.4734 sec/batch\n",
      "Epoch 23/40  Iteration 6097/11040 Training loss: 1.2599 0.4748 sec/batch\n",
      "Epoch 23/40  Iteration 6098/11040 Training loss: 1.2593 0.4756 sec/batch\n",
      "Epoch 23/40  Iteration 6099/11040 Training loss: 1.2590 0.4726 sec/batch\n",
      "Epoch 23/40  Iteration 6100/11040 Training loss: 1.2588 0.4802 sec/batch\n",
      "Epoch 23/40  Iteration 6101/11040 Training loss: 1.2591 0.4713 sec/batch\n",
      "Epoch 23/40  Iteration 6102/11040 Training loss: 1.2594 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6103/11040 Training loss: 1.2592 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6104/11040 Training loss: 1.2594 0.4759 sec/batch\n",
      "Epoch 23/40  Iteration 6105/11040 Training loss: 1.2588 0.4735 sec/batch\n",
      "Epoch 23/40  Iteration 6106/11040 Training loss: 1.2586 0.4781 sec/batch\n",
      "Epoch 23/40  Iteration 6107/11040 Training loss: 1.2585 0.4723 sec/batch\n",
      "Epoch 23/40  Iteration 6108/11040 Training loss: 1.2591 0.4744 sec/batch\n",
      "Epoch 23/40  Iteration 6109/11040 Training loss: 1.2588 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6110/11040 Training loss: 1.2581 0.4763 sec/batch\n",
      "Epoch 23/40  Iteration 6111/11040 Training loss: 1.2574 0.4732 sec/batch\n",
      "Epoch 23/40  Iteration 6112/11040 Training loss: 1.2574 0.4593 sec/batch\n",
      "Epoch 23/40  Iteration 6113/11040 Training loss: 1.2570 0.4603 sec/batch\n",
      "Epoch 23/40  Iteration 6114/11040 Training loss: 1.2560 0.4590 sec/batch\n",
      "Epoch 23/40  Iteration 6115/11040 Training loss: 1.2565 0.4731 sec/batch\n",
      "Epoch 23/40  Iteration 6116/11040 Training loss: 1.2558 0.4555 sec/batch\n",
      "Epoch 23/40  Iteration 6117/11040 Training loss: 1.2554 0.4731 sec/batch\n",
      "Epoch 23/40  Iteration 6118/11040 Training loss: 1.2550 0.4605 sec/batch\n",
      "Epoch 23/40  Iteration 6119/11040 Training loss: 1.2544 0.4729 sec/batch\n",
      "Epoch 23/40  Iteration 6120/11040 Training loss: 1.2541 0.4750 sec/batch\n",
      "Epoch 23/40  Iteration 6121/11040 Training loss: 1.2536 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6122/11040 Training loss: 1.2538 0.4742 sec/batch\n",
      "Epoch 23/40  Iteration 6123/11040 Training loss: 1.2536 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6124/11040 Training loss: 1.2534 0.4615 sec/batch\n",
      "Epoch 23/40  Iteration 6125/11040 Training loss: 1.2537 0.4584 sec/batch\n",
      "Epoch 23/40  Iteration 6126/11040 Training loss: 1.2533 0.4584 sec/batch\n",
      "Epoch 23/40  Iteration 6127/11040 Training loss: 1.2532 0.4604 sec/batch\n",
      "Epoch 23/40  Iteration 6128/11040 Training loss: 1.2532 0.4730 sec/batch\n",
      "Epoch 23/40  Iteration 6129/11040 Training loss: 1.2530 0.4761 sec/batch\n",
      "Epoch 23/40  Iteration 6130/11040 Training loss: 1.2530 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6131/11040 Training loss: 1.2531 0.4748 sec/batch\n",
      "Epoch 23/40  Iteration 6132/11040 Training loss: 1.2528 0.4594 sec/batch\n",
      "Epoch 23/40  Iteration 6133/11040 Training loss: 1.2528 0.4593 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40  Iteration 6134/11040 Training loss: 1.2526 0.4701 sec/batch\n",
      "Epoch 23/40  Iteration 6135/11040 Training loss: 1.2526 0.4636 sec/batch\n",
      "Epoch 23/40  Iteration 6136/11040 Training loss: 1.2524 0.4605 sec/batch\n",
      "Epoch 23/40  Iteration 6137/11040 Training loss: 1.2526 0.4732 sec/batch\n",
      "Epoch 23/40  Iteration 6138/11040 Training loss: 1.2524 0.4753 sec/batch\n",
      "Epoch 23/40  Iteration 6139/11040 Training loss: 1.2523 0.4705 sec/batch\n",
      "Epoch 23/40  Iteration 6140/11040 Training loss: 1.2525 0.4742 sec/batch\n",
      "Epoch 23/40  Iteration 6141/11040 Training loss: 1.2524 0.4797 sec/batch\n",
      "Epoch 23/40  Iteration 6142/11040 Training loss: 1.2523 0.4694 sec/batch\n",
      "Epoch 23/40  Iteration 6143/11040 Training loss: 1.2520 0.4589 sec/batch\n",
      "Epoch 23/40  Iteration 6144/11040 Training loss: 1.2514 0.4611 sec/batch\n",
      "Epoch 23/40  Iteration 6145/11040 Training loss: 1.2509 0.4575 sec/batch\n",
      "Epoch 23/40  Iteration 6146/11040 Training loss: 1.2505 0.4581 sec/batch\n",
      "Epoch 23/40  Iteration 6147/11040 Training loss: 1.2504 0.4760 sec/batch\n",
      "Epoch 23/40  Iteration 6148/11040 Training loss: 1.2507 0.4759 sec/batch\n",
      "Epoch 23/40  Iteration 6149/11040 Training loss: 1.2502 0.4727 sec/batch\n",
      "Epoch 23/40  Iteration 6150/11040 Training loss: 1.2498 0.4595 sec/batch\n",
      "Epoch 23/40  Iteration 6151/11040 Training loss: 1.2496 0.4605 sec/batch\n",
      "Epoch 23/40  Iteration 6152/11040 Training loss: 1.2494 0.4730 sec/batch\n",
      "Epoch 23/40  Iteration 6153/11040 Training loss: 1.2493 0.4750 sec/batch\n",
      "Epoch 23/40  Iteration 6154/11040 Training loss: 1.2494 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6155/11040 Training loss: 1.2494 0.4605 sec/batch\n",
      "Epoch 23/40  Iteration 6156/11040 Training loss: 1.2494 0.4586 sec/batch\n",
      "Epoch 23/40  Iteration 6157/11040 Training loss: 1.2491 0.4592 sec/batch\n",
      "Epoch 23/40  Iteration 6158/11040 Training loss: 1.2491 0.4588 sec/batch\n",
      "Epoch 23/40  Iteration 6159/11040 Training loss: 1.2486 0.4593 sec/batch\n",
      "Epoch 23/40  Iteration 6160/11040 Training loss: 1.2477 0.4586 sec/batch\n",
      "Epoch 23/40  Iteration 6161/11040 Training loss: 1.2477 0.4749 sec/batch\n",
      "Epoch 23/40  Iteration 6162/11040 Training loss: 1.2477 0.4648 sec/batch\n",
      "Epoch 23/40  Iteration 6163/11040 Training loss: 1.2480 0.4860 sec/batch\n",
      "Epoch 23/40  Iteration 6164/11040 Training loss: 1.2479 0.4730 sec/batch\n",
      "Epoch 23/40  Iteration 6165/11040 Training loss: 1.2476 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6166/11040 Training loss: 1.2481 0.4719 sec/batch\n",
      "Epoch 23/40  Iteration 6167/11040 Training loss: 1.2485 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6168/11040 Training loss: 1.2488 0.4748 sec/batch\n",
      "Epoch 23/40  Iteration 6169/11040 Training loss: 1.2491 0.4772 sec/batch\n",
      "Epoch 23/40  Iteration 6170/11040 Training loss: 1.2495 0.4720 sec/batch\n",
      "Epoch 23/40  Iteration 6171/11040 Training loss: 1.2496 0.4773 sec/batch\n",
      "Epoch 23/40  Iteration 6172/11040 Training loss: 1.2495 0.4716 sec/batch\n",
      "Epoch 23/40  Iteration 6173/11040 Training loss: 1.2496 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6174/11040 Training loss: 1.2498 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6175/11040 Training loss: 1.2493 0.4750 sec/batch\n",
      "Epoch 23/40  Iteration 6176/11040 Training loss: 1.2496 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6177/11040 Training loss: 1.2495 0.4589 sec/batch\n",
      "Epoch 23/40  Iteration 6178/11040 Training loss: 1.2494 0.4753 sec/batch\n",
      "Epoch 23/40  Iteration 6179/11040 Training loss: 1.2495 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6180/11040 Training loss: 1.2497 0.4770 sec/batch\n",
      "Epoch 23/40  Iteration 6181/11040 Training loss: 1.2500 0.4720 sec/batch\n",
      "Epoch 23/40  Iteration 6182/11040 Training loss: 1.2501 0.4597 sec/batch\n",
      "Epoch 23/40  Iteration 6183/11040 Training loss: 1.2502 0.4674 sec/batch\n",
      "Epoch 23/40  Iteration 6184/11040 Training loss: 1.2504 0.4677 sec/batch\n",
      "Epoch 23/40  Iteration 6185/11040 Training loss: 1.2505 0.4733 sec/batch\n",
      "Epoch 23/40  Iteration 6186/11040 Training loss: 1.2506 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6187/11040 Training loss: 1.2509 0.4783 sec/batch\n",
      "Epoch 23/40  Iteration 6188/11040 Training loss: 1.2512 0.4738 sec/batch\n",
      "Epoch 23/40  Iteration 6189/11040 Training loss: 1.2512 0.4718 sec/batch\n",
      "Epoch 23/40  Iteration 6190/11040 Training loss: 1.2515 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6191/11040 Training loss: 1.2516 0.4801 sec/batch\n",
      "Epoch 23/40  Iteration 6192/11040 Training loss: 1.2517 0.4704 sec/batch\n",
      "Epoch 23/40  Iteration 6193/11040 Training loss: 1.2518 0.4734 sec/batch\n",
      "Epoch 23/40  Iteration 6194/11040 Training loss: 1.2519 0.4590 sec/batch\n",
      "Epoch 23/40  Iteration 6195/11040 Training loss: 1.2523 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6196/11040 Training loss: 1.2527 0.4760 sec/batch\n",
      "Epoch 23/40  Iteration 6197/11040 Training loss: 1.2529 0.4749 sec/batch\n",
      "Epoch 23/40  Iteration 6198/11040 Training loss: 1.2531 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6199/11040 Training loss: 1.2532 0.4801 sec/batch\n",
      "Epoch 23/40  Iteration 6200/11040 Training loss: 1.2532 0.4707 sec/batch\n",
      "Epoch 23/40  Iteration 6201/11040 Training loss: 1.2531 0.4877 sec/batch\n",
      "Epoch 23/40  Iteration 6202/11040 Training loss: 1.2533 0.4755 sec/batch\n",
      "Epoch 23/40  Iteration 6203/11040 Training loss: 1.2533 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6204/11040 Training loss: 1.2531 0.4786 sec/batch\n",
      "Epoch 23/40  Iteration 6205/11040 Training loss: 1.2532 0.4707 sec/batch\n",
      "Epoch 23/40  Iteration 6206/11040 Training loss: 1.2530 0.4603 sec/batch\n",
      "Epoch 23/40  Iteration 6207/11040 Training loss: 1.2532 0.4574 sec/batch\n",
      "Epoch 23/40  Iteration 6208/11040 Training loss: 1.2533 0.4712 sec/batch\n",
      "Epoch 23/40  Iteration 6209/11040 Training loss: 1.2532 0.4795 sec/batch\n",
      "Epoch 23/40  Iteration 6210/11040 Training loss: 1.2534 0.4720 sec/batch\n",
      "Epoch 23/40  Iteration 6211/11040 Training loss: 1.2535 0.4761 sec/batch\n",
      "Epoch 23/40  Iteration 6212/11040 Training loss: 1.2535 0.4784 sec/batch\n",
      "Epoch 23/40  Iteration 6213/11040 Training loss: 1.2536 0.4734 sec/batch\n",
      "Epoch 23/40  Iteration 6214/11040 Training loss: 1.2537 0.4599 sec/batch\n",
      "Epoch 23/40  Iteration 6215/11040 Training loss: 1.2537 0.4736 sec/batch\n",
      "Epoch 23/40  Iteration 6216/11040 Training loss: 1.2536 0.4578 sec/batch\n",
      "Epoch 23/40  Iteration 6217/11040 Training loss: 1.2536 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6218/11040 Training loss: 1.2535 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6219/11040 Training loss: 1.2534 0.4737 sec/batch\n",
      "Epoch 23/40  Iteration 6220/11040 Training loss: 1.2534 0.4609 sec/batch\n",
      "Epoch 23/40  Iteration 6221/11040 Training loss: 1.2532 0.4607 sec/batch\n",
      "Epoch 23/40  Iteration 6222/11040 Training loss: 1.2533 0.4730 sec/batch\n",
      "Epoch 23/40  Iteration 6223/11040 Training loss: 1.2532 0.4762 sec/batch\n",
      "Epoch 23/40  Iteration 6224/11040 Training loss: 1.2534 0.4744 sec/batch\n",
      "Epoch 23/40  Iteration 6225/11040 Training loss: 1.2534 0.4746 sec/batch\n",
      "Epoch 23/40  Iteration 6226/11040 Training loss: 1.2536 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6227/11040 Training loss: 1.2537 0.4754 sec/batch\n",
      "Epoch 23/40  Iteration 6228/11040 Training loss: 1.2537 0.4755 sec/batch\n",
      "Epoch 23/40  Iteration 6229/11040 Training loss: 1.2535 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6230/11040 Training loss: 1.2537 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6231/11040 Training loss: 1.2538 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6232/11040 Training loss: 1.2538 0.4746 sec/batch\n",
      "Epoch 23/40  Iteration 6233/11040 Training loss: 1.2538 0.4747 sec/batch\n",
      "Epoch 23/40  Iteration 6234/11040 Training loss: 1.2538 0.4767 sec/batch\n",
      "Epoch 23/40  Iteration 6235/11040 Training loss: 1.2540 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6236/11040 Training loss: 1.2541 0.4606 sec/batch\n",
      "Epoch 23/40  Iteration 6237/11040 Training loss: 1.2543 0.4759 sec/batch\n",
      "Epoch 23/40  Iteration 6238/11040 Training loss: 1.2545 0.4742 sec/batch\n",
      "Epoch 23/40  Iteration 6239/11040 Training loss: 1.2545 0.4732 sec/batch\n",
      "Epoch 23/40  Iteration 6240/11040 Training loss: 1.2544 0.4607 sec/batch\n",
      "Epoch 23/40  Iteration 6241/11040 Training loss: 1.2544 0.4585 sec/batch\n",
      "Epoch 23/40  Iteration 6242/11040 Training loss: 1.2544 0.4750 sec/batch\n",
      "Epoch 23/40  Iteration 6243/11040 Training loss: 1.2547 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6244/11040 Training loss: 1.2548 0.4742 sec/batch\n",
      "Epoch 23/40  Iteration 6245/11040 Training loss: 1.2548 0.4603 sec/batch\n",
      "Epoch 23/40  Iteration 6246/11040 Training loss: 1.2547 0.4762 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40  Iteration 6247/11040 Training loss: 1.2547 0.4744 sec/batch\n",
      "Epoch 23/40  Iteration 6248/11040 Training loss: 1.2548 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6249/11040 Training loss: 1.2549 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6250/11040 Training loss: 1.2549 0.4712 sec/batch\n",
      "Epoch 23/40  Iteration 6251/11040 Training loss: 1.2551 0.4784 sec/batch\n",
      "Epoch 23/40  Iteration 6252/11040 Training loss: 1.2552 0.4763 sec/batch\n",
      "Epoch 23/40  Iteration 6253/11040 Training loss: 1.2553 0.4737 sec/batch\n",
      "Epoch 23/40  Iteration 6254/11040 Training loss: 1.2552 0.4921 sec/batch\n",
      "Epoch 23/40  Iteration 6255/11040 Training loss: 1.2551 0.4768 sec/batch\n",
      "Epoch 23/40  Iteration 6256/11040 Training loss: 1.2553 0.4738 sec/batch\n",
      "Epoch 23/40  Iteration 6257/11040 Training loss: 1.2554 0.4754 sec/batch\n",
      "Epoch 23/40  Iteration 6258/11040 Training loss: 1.2555 0.4612 sec/batch\n",
      "Epoch 23/40  Iteration 6259/11040 Training loss: 1.2557 0.4733 sec/batch\n",
      "Epoch 23/40  Iteration 6260/11040 Training loss: 1.2558 0.4745 sec/batch\n",
      "Epoch 23/40  Iteration 6261/11040 Training loss: 1.2558 0.4616 sec/batch\n",
      "Epoch 23/40  Iteration 6262/11040 Training loss: 1.2558 0.4732 sec/batch\n",
      "Epoch 23/40  Iteration 6263/11040 Training loss: 1.2557 0.4744 sec/batch\n",
      "Epoch 23/40  Iteration 6264/11040 Training loss: 1.2556 0.4770 sec/batch\n",
      "Epoch 23/40  Iteration 6265/11040 Training loss: 1.2556 0.4738 sec/batch\n",
      "Epoch 23/40  Iteration 6266/11040 Training loss: 1.2557 0.4761 sec/batch\n",
      "Epoch 23/40  Iteration 6267/11040 Training loss: 1.2560 0.4737 sec/batch\n",
      "Epoch 23/40  Iteration 6268/11040 Training loss: 1.2559 0.4767 sec/batch\n",
      "Epoch 23/40  Iteration 6269/11040 Training loss: 1.2560 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6270/11040 Training loss: 1.2560 0.4756 sec/batch\n",
      "Epoch 23/40  Iteration 6271/11040 Training loss: 1.2562 0.4748 sec/batch\n",
      "Epoch 23/40  Iteration 6272/11040 Training loss: 1.2563 0.4774 sec/batch\n",
      "Epoch 23/40  Iteration 6273/11040 Training loss: 1.2564 0.4734 sec/batch\n",
      "Epoch 23/40  Iteration 6274/11040 Training loss: 1.2566 0.4743 sec/batch\n",
      "Epoch 23/40  Iteration 6275/11040 Training loss: 1.2567 0.4760 sec/batch\n",
      "Epoch 23/40  Iteration 6276/11040 Training loss: 1.2568 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6277/11040 Training loss: 1.2568 0.4747 sec/batch\n",
      "Epoch 23/40  Iteration 6278/11040 Training loss: 1.2567 0.4773 sec/batch\n",
      "Epoch 23/40  Iteration 6279/11040 Training loss: 1.2567 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6280/11040 Training loss: 1.2568 0.4901 sec/batch\n",
      "Epoch 23/40  Iteration 6281/11040 Training loss: 1.2566 0.4747 sec/batch\n",
      "Epoch 23/40  Iteration 6282/11040 Training loss: 1.2566 0.4610 sec/batch\n",
      "Epoch 23/40  Iteration 6283/11040 Training loss: 1.2566 0.4746 sec/batch\n",
      "Epoch 23/40  Iteration 6284/11040 Training loss: 1.2566 0.4781 sec/batch\n",
      "Epoch 23/40  Iteration 6285/11040 Training loss: 1.2566 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6286/11040 Training loss: 1.2565 0.4739 sec/batch\n",
      "Epoch 23/40  Iteration 6287/11040 Training loss: 1.2564 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6288/11040 Training loss: 1.2564 0.4739 sec/batch\n",
      "Epoch 23/40  Iteration 6289/11040 Training loss: 1.2562 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6290/11040 Training loss: 1.2561 0.4612 sec/batch\n",
      "Epoch 23/40  Iteration 6291/11040 Training loss: 1.2560 0.4746 sec/batch\n",
      "Epoch 23/40  Iteration 6292/11040 Training loss: 1.2560 0.4910 sec/batch\n",
      "Epoch 23/40  Iteration 6293/11040 Training loss: 1.2559 0.4753 sec/batch\n",
      "Epoch 23/40  Iteration 6294/11040 Training loss: 1.2558 0.4898 sec/batch\n",
      "Epoch 23/40  Iteration 6295/11040 Training loss: 1.2557 0.4759 sec/batch\n",
      "Epoch 23/40  Iteration 6296/11040 Training loss: 1.2558 0.4906 sec/batch\n",
      "Epoch 23/40  Iteration 6297/11040 Training loss: 1.2557 0.4766 sec/batch\n",
      "Epoch 23/40  Iteration 6298/11040 Training loss: 1.2557 0.4747 sec/batch\n",
      "Epoch 23/40  Iteration 6299/11040 Training loss: 1.2555 0.4767 sec/batch\n",
      "Epoch 23/40  Iteration 6300/11040 Training loss: 1.2555 0.4736 sec/batch\n",
      "Epoch 23/40  Iteration 6301/11040 Training loss: 1.2556 0.4777 sec/batch\n",
      "Epoch 23/40  Iteration 6302/11040 Training loss: 1.2556 0.4734 sec/batch\n",
      "Epoch 23/40  Iteration 6303/11040 Training loss: 1.2556 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6304/11040 Training loss: 1.2557 0.4759 sec/batch\n",
      "Epoch 23/40  Iteration 6305/11040 Training loss: 1.2556 0.4884 sec/batch\n",
      "Epoch 23/40  Iteration 6306/11040 Training loss: 1.2555 0.4761 sec/batch\n",
      "Epoch 23/40  Iteration 6307/11040 Training loss: 1.2556 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6308/11040 Training loss: 1.2556 0.4750 sec/batch\n",
      "Epoch 23/40  Iteration 6309/11040 Training loss: 1.2557 0.4761 sec/batch\n",
      "Epoch 23/40  Iteration 6310/11040 Training loss: 1.2557 0.4751 sec/batch\n",
      "Epoch 23/40  Iteration 6311/11040 Training loss: 1.2558 0.4905 sec/batch\n",
      "Epoch 23/40  Iteration 6312/11040 Training loss: 1.2559 0.4828 sec/batch\n",
      "Epoch 23/40  Iteration 6313/11040 Training loss: 1.2560 0.4851 sec/batch\n",
      "Epoch 23/40  Iteration 6314/11040 Training loss: 1.2559 0.4765 sec/batch\n",
      "Epoch 23/40  Iteration 6315/11040 Training loss: 1.2559 0.4733 sec/batch\n",
      "Epoch 23/40  Iteration 6316/11040 Training loss: 1.2559 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6317/11040 Training loss: 1.2558 0.4753 sec/batch\n",
      "Epoch 23/40  Iteration 6318/11040 Training loss: 1.2558 0.4783 sec/batch\n",
      "Epoch 23/40  Iteration 6319/11040 Training loss: 1.2558 0.4741 sec/batch\n",
      "Epoch 23/40  Iteration 6320/11040 Training loss: 1.2557 0.4746 sec/batch\n",
      "Epoch 23/40  Iteration 6321/11040 Training loss: 1.2557 0.4748 sec/batch\n",
      "Epoch 23/40  Iteration 6322/11040 Training loss: 1.2557 0.4899 sec/batch\n",
      "Epoch 23/40  Iteration 6323/11040 Training loss: 1.2556 0.4768 sec/batch\n",
      "Epoch 23/40  Iteration 6324/11040 Training loss: 1.2557 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6325/11040 Training loss: 1.2557 0.4778 sec/batch\n",
      "Epoch 23/40  Iteration 6326/11040 Training loss: 1.2557 0.4750 sec/batch\n",
      "Epoch 23/40  Iteration 6327/11040 Training loss: 1.2557 0.4740 sec/batch\n",
      "Epoch 23/40  Iteration 6328/11040 Training loss: 1.2557 0.4927 sec/batch\n",
      "Epoch 23/40  Iteration 6329/11040 Training loss: 1.2558 0.4752 sec/batch\n",
      "Epoch 23/40  Iteration 6330/11040 Training loss: 1.2559 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6331/11040 Training loss: 1.2559 0.4912 sec/batch\n",
      "Epoch 23/40  Iteration 6332/11040 Training loss: 1.2559 0.4839 sec/batch\n",
      "Epoch 23/40  Iteration 6333/11040 Training loss: 1.2560 0.4806 sec/batch\n",
      "Epoch 23/40  Iteration 6334/11040 Training loss: 1.2561 0.4846 sec/batch\n",
      "Epoch 23/40  Iteration 6335/11040 Training loss: 1.2561 0.4672 sec/batch\n",
      "Epoch 23/40  Iteration 6336/11040 Training loss: 1.2562 0.4774 sec/batch\n",
      "Epoch 23/40  Iteration 6337/11040 Training loss: 1.2562 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6338/11040 Training loss: 1.2563 0.4737 sec/batch\n",
      "Epoch 23/40  Iteration 6339/11040 Training loss: 1.2564 0.4766 sec/batch\n",
      "Epoch 23/40  Iteration 6340/11040 Training loss: 1.2565 0.4890 sec/batch\n",
      "Epoch 23/40  Iteration 6341/11040 Training loss: 1.2566 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6342/11040 Training loss: 1.2566 0.4765 sec/batch\n",
      "Epoch 23/40  Iteration 6343/11040 Training loss: 1.2570 0.4913 sec/batch\n",
      "Epoch 23/40  Iteration 6344/11040 Training loss: 1.2572 0.4757 sec/batch\n",
      "Epoch 23/40  Iteration 6345/11040 Training loss: 1.2573 0.4762 sec/batch\n",
      "Epoch 23/40  Iteration 6346/11040 Training loss: 1.2574 0.4889 sec/batch\n",
      "Epoch 23/40  Iteration 6347/11040 Training loss: 1.2574 0.4758 sec/batch\n",
      "Epoch 23/40  Iteration 6348/11040 Training loss: 1.2575 0.4750 sec/batch\n",
      "Epoch 24/40  Iteration 6349/11040 Training loss: 1.3362 0.4907 sec/batch\n",
      "Epoch 24/40  Iteration 6350/11040 Training loss: 1.2978 0.4777 sec/batch\n",
      "Epoch 24/40  Iteration 6351/11040 Training loss: 1.2923 0.4742 sec/batch\n",
      "Epoch 24/40  Iteration 6352/11040 Training loss: 1.2907 0.4900 sec/batch\n",
      "Epoch 24/40  Iteration 6353/11040 Training loss: 1.2879 0.4746 sec/batch\n",
      "Epoch 24/40  Iteration 6354/11040 Training loss: 1.2831 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6355/11040 Training loss: 1.2736 0.4918 sec/batch\n",
      "Epoch 24/40  Iteration 6356/11040 Training loss: 1.2727 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6357/11040 Training loss: 1.2685 0.4762 sec/batch\n",
      "Epoch 24/40  Iteration 6358/11040 Training loss: 1.2680 0.4928 sec/batch\n",
      "Epoch 24/40  Iteration 6359/11040 Training loss: 1.2647 0.4736 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40  Iteration 6360/11040 Training loss: 1.2642 0.4755 sec/batch\n",
      "Epoch 24/40  Iteration 6361/11040 Training loss: 1.2606 0.4742 sec/batch\n",
      "Epoch 24/40  Iteration 6362/11040 Training loss: 1.2581 0.4761 sec/batch\n",
      "Epoch 24/40  Iteration 6363/11040 Training loss: 1.2571 0.4772 sec/batch\n",
      "Epoch 24/40  Iteration 6364/11040 Training loss: 1.2569 0.4889 sec/batch\n",
      "Epoch 24/40  Iteration 6365/11040 Training loss: 1.2545 0.4773 sec/batch\n",
      "Epoch 24/40  Iteration 6366/11040 Training loss: 1.2522 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6367/11040 Training loss: 1.2517 0.4643 sec/batch\n",
      "Epoch 24/40  Iteration 6368/11040 Training loss: 1.2517 0.4879 sec/batch\n",
      "Epoch 24/40  Iteration 6369/11040 Training loss: 1.2535 0.4738 sec/batch\n",
      "Epoch 24/40  Iteration 6370/11040 Training loss: 1.2541 0.4756 sec/batch\n",
      "Epoch 24/40  Iteration 6371/11040 Training loss: 1.2538 0.4916 sec/batch\n",
      "Epoch 24/40  Iteration 6372/11040 Training loss: 1.2539 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6373/11040 Training loss: 1.2531 0.4766 sec/batch\n",
      "Epoch 24/40  Iteration 6374/11040 Training loss: 1.2528 0.4909 sec/batch\n",
      "Epoch 24/40  Iteration 6375/11040 Training loss: 1.2522 0.4898 sec/batch\n",
      "Epoch 24/40  Iteration 6376/11040 Training loss: 1.2518 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6377/11040 Training loss: 1.2520 0.4763 sec/batch\n",
      "Epoch 24/40  Iteration 6378/11040 Training loss: 1.2522 0.4925 sec/batch\n",
      "Epoch 24/40  Iteration 6379/11040 Training loss: 1.2520 0.4730 sec/batch\n",
      "Epoch 24/40  Iteration 6380/11040 Training loss: 1.2517 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6381/11040 Training loss: 1.2509 0.4925 sec/batch\n",
      "Epoch 24/40  Iteration 6382/11040 Training loss: 1.2506 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6383/11040 Training loss: 1.2503 0.4854 sec/batch\n",
      "Epoch 24/40  Iteration 6384/11040 Training loss: 1.2509 0.4793 sec/batch\n",
      "Epoch 24/40  Iteration 6385/11040 Training loss: 1.2509 0.4857 sec/batch\n",
      "Epoch 24/40  Iteration 6386/11040 Training loss: 1.2501 0.4907 sec/batch\n",
      "Epoch 24/40  Iteration 6387/11040 Training loss: 1.2495 0.4879 sec/batch\n",
      "Epoch 24/40  Iteration 6388/11040 Training loss: 1.2495 0.4750 sec/batch\n",
      "Epoch 24/40  Iteration 6389/11040 Training loss: 1.2490 0.4738 sec/batch\n",
      "Epoch 24/40  Iteration 6390/11040 Training loss: 1.2480 0.4710 sec/batch\n",
      "Epoch 24/40  Iteration 6391/11040 Training loss: 1.2483 0.4661 sec/batch\n",
      "Epoch 24/40  Iteration 6392/11040 Training loss: 1.2474 0.4749 sec/batch\n",
      "Epoch 24/40  Iteration 6393/11040 Training loss: 1.2469 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6394/11040 Training loss: 1.2465 0.4898 sec/batch\n",
      "Epoch 24/40  Iteration 6395/11040 Training loss: 1.2457 0.4768 sec/batch\n",
      "Epoch 24/40  Iteration 6396/11040 Training loss: 1.2454 0.4811 sec/batch\n",
      "Epoch 24/40  Iteration 6397/11040 Training loss: 1.2448 0.4716 sec/batch\n",
      "Epoch 24/40  Iteration 6398/11040 Training loss: 1.2453 0.4732 sec/batch\n",
      "Epoch 24/40  Iteration 6399/11040 Training loss: 1.2450 0.4760 sec/batch\n",
      "Epoch 24/40  Iteration 6400/11040 Training loss: 1.2450 0.4773 sec/batch\n",
      "Epoch 24/40  Iteration 6401/11040 Training loss: 1.2452 0.4744 sec/batch\n",
      "Epoch 24/40  Iteration 6402/11040 Training loss: 1.2448 0.4757 sec/batch\n",
      "Epoch 24/40  Iteration 6403/11040 Training loss: 1.2448 0.4906 sec/batch\n",
      "Epoch 24/40  Iteration 6404/11040 Training loss: 1.2447 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6405/11040 Training loss: 1.2445 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6406/11040 Training loss: 1.2443 0.4769 sec/batch\n",
      "Epoch 24/40  Iteration 6407/11040 Training loss: 1.2446 0.4729 sec/batch\n",
      "Epoch 24/40  Iteration 6408/11040 Training loss: 1.2443 0.4774 sec/batch\n",
      "Epoch 24/40  Iteration 6409/11040 Training loss: 1.2442 0.4885 sec/batch\n",
      "Epoch 24/40  Iteration 6410/11040 Training loss: 1.2440 0.4770 sec/batch\n",
      "Epoch 24/40  Iteration 6411/11040 Training loss: 1.2440 0.4750 sec/batch\n",
      "Epoch 24/40  Iteration 6412/11040 Training loss: 1.2439 0.4916 sec/batch\n",
      "Epoch 24/40  Iteration 6413/11040 Training loss: 1.2441 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6414/11040 Training loss: 1.2438 0.4761 sec/batch\n",
      "Epoch 24/40  Iteration 6415/11040 Training loss: 1.2437 0.4767 sec/batch\n",
      "Epoch 24/40  Iteration 6416/11040 Training loss: 1.2438 0.4905 sec/batch\n",
      "Epoch 24/40  Iteration 6417/11040 Training loss: 1.2437 0.4764 sec/batch\n",
      "Epoch 24/40  Iteration 6418/11040 Training loss: 1.2436 0.4765 sec/batch\n",
      "Epoch 24/40  Iteration 6419/11040 Training loss: 1.2432 0.4891 sec/batch\n",
      "Epoch 24/40  Iteration 6420/11040 Training loss: 1.2427 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6421/11040 Training loss: 1.2423 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6422/11040 Training loss: 1.2418 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6423/11040 Training loss: 1.2417 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6424/11040 Training loss: 1.2421 0.4762 sec/batch\n",
      "Epoch 24/40  Iteration 6425/11040 Training loss: 1.2415 0.4771 sec/batch\n",
      "Epoch 24/40  Iteration 6426/11040 Training loss: 1.2411 0.4739 sec/batch\n",
      "Epoch 24/40  Iteration 6427/11040 Training loss: 1.2410 0.4793 sec/batch\n",
      "Epoch 24/40  Iteration 6428/11040 Training loss: 1.2410 0.4871 sec/batch\n",
      "Epoch 24/40  Iteration 6429/11040 Training loss: 1.2408 0.4760 sec/batch\n",
      "Epoch 24/40  Iteration 6430/11040 Training loss: 1.2410 0.4767 sec/batch\n",
      "Epoch 24/40  Iteration 6431/11040 Training loss: 1.2411 0.4926 sec/batch\n",
      "Epoch 24/40  Iteration 6432/11040 Training loss: 1.2410 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6433/11040 Training loss: 1.2407 0.4772 sec/batch\n",
      "Epoch 24/40  Iteration 6434/11040 Training loss: 1.2407 0.4755 sec/batch\n",
      "Epoch 24/40  Iteration 6435/11040 Training loss: 1.2403 0.4737 sec/batch\n",
      "Epoch 24/40  Iteration 6436/11040 Training loss: 1.2393 0.4768 sec/batch\n",
      "Epoch 24/40  Iteration 6437/11040 Training loss: 1.2393 0.4892 sec/batch\n",
      "Epoch 24/40  Iteration 6438/11040 Training loss: 1.2394 0.4760 sec/batch\n",
      "Epoch 24/40  Iteration 6439/11040 Training loss: 1.2395 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6440/11040 Training loss: 1.2394 0.4763 sec/batch\n",
      "Epoch 24/40  Iteration 6441/11040 Training loss: 1.2393 0.4894 sec/batch\n",
      "Epoch 24/40  Iteration 6442/11040 Training loss: 1.2396 0.4766 sec/batch\n",
      "Epoch 24/40  Iteration 6443/11040 Training loss: 1.2401 0.4769 sec/batch\n",
      "Epoch 24/40  Iteration 6444/11040 Training loss: 1.2404 0.4892 sec/batch\n",
      "Epoch 24/40  Iteration 6445/11040 Training loss: 1.2407 0.4761 sec/batch\n",
      "Epoch 24/40  Iteration 6446/11040 Training loss: 1.2411 0.4757 sec/batch\n",
      "Epoch 24/40  Iteration 6447/11040 Training loss: 1.2414 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6448/11040 Training loss: 1.2412 0.4754 sec/batch\n",
      "Epoch 24/40  Iteration 6449/11040 Training loss: 1.2413 0.4921 sec/batch\n",
      "Epoch 24/40  Iteration 6450/11040 Training loss: 1.2415 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6451/11040 Training loss: 1.2411 0.4749 sec/batch\n",
      "Epoch 24/40  Iteration 6452/11040 Training loss: 1.2413 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6453/11040 Training loss: 1.2411 0.4905 sec/batch\n",
      "Epoch 24/40  Iteration 6454/11040 Training loss: 1.2410 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6455/11040 Training loss: 1.2410 0.4740 sec/batch\n",
      "Epoch 24/40  Iteration 6456/11040 Training loss: 1.2413 0.4602 sec/batch\n",
      "Epoch 24/40  Iteration 6457/11040 Training loss: 1.2415 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6458/11040 Training loss: 1.2417 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6459/11040 Training loss: 1.2416 0.4755 sec/batch\n",
      "Epoch 24/40  Iteration 6460/11040 Training loss: 1.2419 0.4742 sec/batch\n",
      "Epoch 24/40  Iteration 6461/11040 Training loss: 1.2421 0.4608 sec/batch\n",
      "Epoch 24/40  Iteration 6462/11040 Training loss: 1.2421 0.4608 sec/batch\n",
      "Epoch 24/40  Iteration 6463/11040 Training loss: 1.2424 0.4729 sec/batch\n",
      "Epoch 24/40  Iteration 6464/11040 Training loss: 1.2428 0.4739 sec/batch\n",
      "Epoch 24/40  Iteration 6465/11040 Training loss: 1.2427 0.4782 sec/batch\n",
      "Epoch 24/40  Iteration 6466/11040 Training loss: 1.2429 0.4716 sec/batch\n",
      "Epoch 24/40  Iteration 6467/11040 Training loss: 1.2431 0.4744 sec/batch\n",
      "Epoch 24/40  Iteration 6468/11040 Training loss: 1.2431 0.4603 sec/batch\n",
      "Epoch 24/40  Iteration 6469/11040 Training loss: 1.2432 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6470/11040 Training loss: 1.2432 0.4747 sec/batch\n",
      "Epoch 24/40  Iteration 6471/11040 Training loss: 1.2435 0.4751 sec/batch\n",
      "Epoch 24/40  Iteration 6472/11040 Training loss: 1.2440 0.4745 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40  Iteration 6473/11040 Training loss: 1.2442 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6474/11040 Training loss: 1.2445 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6475/11040 Training loss: 1.2446 0.4731 sec/batch\n",
      "Epoch 24/40  Iteration 6476/11040 Training loss: 1.2446 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6477/11040 Training loss: 1.2445 0.4595 sec/batch\n",
      "Epoch 24/40  Iteration 6478/11040 Training loss: 1.2447 0.4595 sec/batch\n",
      "Epoch 24/40  Iteration 6479/11040 Training loss: 1.2447 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6480/11040 Training loss: 1.2445 0.4612 sec/batch\n",
      "Epoch 24/40  Iteration 6481/11040 Training loss: 1.2445 0.4741 sec/batch\n",
      "Epoch 24/40  Iteration 6482/11040 Training loss: 1.2444 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6483/11040 Training loss: 1.2446 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6484/11040 Training loss: 1.2447 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6485/11040 Training loss: 1.2446 0.4741 sec/batch\n",
      "Epoch 24/40  Iteration 6486/11040 Training loss: 1.2448 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6487/11040 Training loss: 1.2450 0.4600 sec/batch\n",
      "Epoch 24/40  Iteration 6488/11040 Training loss: 1.2451 0.4751 sec/batch\n",
      "Epoch 24/40  Iteration 6489/11040 Training loss: 1.2450 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6490/11040 Training loss: 1.2452 0.4764 sec/batch\n",
      "Epoch 24/40  Iteration 6491/11040 Training loss: 1.2452 0.4733 sec/batch\n",
      "Epoch 24/40  Iteration 6492/11040 Training loss: 1.2451 0.4608 sec/batch\n",
      "Epoch 24/40  Iteration 6493/11040 Training loss: 1.2451 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6494/11040 Training loss: 1.2451 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6495/11040 Training loss: 1.2450 0.4756 sec/batch\n",
      "Epoch 24/40  Iteration 6496/11040 Training loss: 1.2450 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6497/11040 Training loss: 1.2449 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6498/11040 Training loss: 1.2449 0.4776 sec/batch\n",
      "Epoch 24/40  Iteration 6499/11040 Training loss: 1.2448 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6500/11040 Training loss: 1.2450 0.4732 sec/batch\n",
      "Validation loss: 1.26348 Saving checkpoint!\n",
      "Epoch 24/40  Iteration 6501/11040 Training loss: 1.2460 0.4687 sec/batch\n",
      "Epoch 24/40  Iteration 6502/11040 Training loss: 1.2461 0.4576 sec/batch\n",
      "Epoch 24/40  Iteration 6503/11040 Training loss: 1.2462 0.4732 sec/batch\n",
      "Epoch 24/40  Iteration 6504/11040 Training loss: 1.2462 0.4742 sec/batch\n",
      "Epoch 24/40  Iteration 6505/11040 Training loss: 1.2461 0.4762 sec/batch\n",
      "Epoch 24/40  Iteration 6506/11040 Training loss: 1.2462 0.4742 sec/batch\n",
      "Epoch 24/40  Iteration 6507/11040 Training loss: 1.2464 0.4594 sec/batch\n",
      "Epoch 24/40  Iteration 6508/11040 Training loss: 1.2464 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6509/11040 Training loss: 1.2465 0.4740 sec/batch\n",
      "Epoch 24/40  Iteration 6510/11040 Training loss: 1.2465 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6511/11040 Training loss: 1.2466 0.4751 sec/batch\n",
      "Epoch 24/40  Iteration 6512/11040 Training loss: 1.2468 0.4740 sec/batch\n",
      "Epoch 24/40  Iteration 6513/11040 Training loss: 1.2470 0.4617 sec/batch\n",
      "Epoch 24/40  Iteration 6514/11040 Training loss: 1.2473 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6515/11040 Training loss: 1.2473 0.4767 sec/batch\n",
      "Epoch 24/40  Iteration 6516/11040 Training loss: 1.2472 0.4720 sec/batch\n",
      "Epoch 24/40  Iteration 6517/11040 Training loss: 1.2471 0.4762 sec/batch\n",
      "Epoch 24/40  Iteration 6518/11040 Training loss: 1.2471 0.4910 sec/batch\n",
      "Epoch 24/40  Iteration 6519/11040 Training loss: 1.2473 0.4766 sec/batch\n",
      "Epoch 24/40  Iteration 6520/11040 Training loss: 1.2474 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6521/11040 Training loss: 1.2475 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6522/11040 Training loss: 1.2474 0.4761 sec/batch\n",
      "Epoch 24/40  Iteration 6523/11040 Training loss: 1.2474 0.4747 sec/batch\n",
      "Epoch 24/40  Iteration 6524/11040 Training loss: 1.2475 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6525/11040 Training loss: 1.2475 0.4764 sec/batch\n",
      "Epoch 24/40  Iteration 6526/11040 Training loss: 1.2475 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6527/11040 Training loss: 1.2477 0.4744 sec/batch\n",
      "Epoch 24/40  Iteration 6528/11040 Training loss: 1.2478 0.4762 sec/batch\n",
      "Epoch 24/40  Iteration 6529/11040 Training loss: 1.2478 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6530/11040 Training loss: 1.2479 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6531/11040 Training loss: 1.2478 0.4754 sec/batch\n",
      "Epoch 24/40  Iteration 6532/11040 Training loss: 1.2479 0.4744 sec/batch\n",
      "Epoch 24/40  Iteration 6533/11040 Training loss: 1.2481 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6534/11040 Training loss: 1.2482 0.4756 sec/batch\n",
      "Epoch 24/40  Iteration 6535/11040 Training loss: 1.2485 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6536/11040 Training loss: 1.2485 0.4812 sec/batch\n",
      "Epoch 24/40  Iteration 6537/11040 Training loss: 1.2486 0.4855 sec/batch\n",
      "Epoch 24/40  Iteration 6538/11040 Training loss: 1.2486 0.4747 sec/batch\n",
      "Epoch 24/40  Iteration 6539/11040 Training loss: 1.2485 0.4591 sec/batch\n",
      "Epoch 24/40  Iteration 6540/11040 Training loss: 1.2485 0.4741 sec/batch\n",
      "Epoch 24/40  Iteration 6541/11040 Training loss: 1.2485 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6542/11040 Training loss: 1.2486 0.4623 sec/batch\n",
      "Epoch 24/40  Iteration 6543/11040 Training loss: 1.2488 0.4740 sec/batch\n",
      "Epoch 24/40  Iteration 6544/11040 Training loss: 1.2487 0.4736 sec/batch\n",
      "Epoch 24/40  Iteration 6545/11040 Training loss: 1.2488 0.4600 sec/batch\n",
      "Epoch 24/40  Iteration 6546/11040 Training loss: 1.2488 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6547/11040 Training loss: 1.2490 0.4751 sec/batch\n",
      "Epoch 24/40  Iteration 6548/11040 Training loss: 1.2491 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6549/11040 Training loss: 1.2493 0.4769 sec/batch\n",
      "Epoch 24/40  Iteration 6550/11040 Training loss: 1.2494 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6551/11040 Training loss: 1.2495 0.4741 sec/batch\n",
      "Epoch 24/40  Iteration 6552/11040 Training loss: 1.2496 0.4757 sec/batch\n",
      "Epoch 24/40  Iteration 6553/11040 Training loss: 1.2496 0.4696 sec/batch\n",
      "Epoch 24/40  Iteration 6554/11040 Training loss: 1.2495 0.4801 sec/batch\n",
      "Epoch 24/40  Iteration 6555/11040 Training loss: 1.2496 0.4719 sec/batch\n",
      "Epoch 24/40  Iteration 6556/11040 Training loss: 1.2496 0.4615 sec/batch\n",
      "Epoch 24/40  Iteration 6557/11040 Training loss: 1.2495 0.4756 sec/batch\n",
      "Epoch 24/40  Iteration 6558/11040 Training loss: 1.2495 0.4760 sec/batch\n",
      "Epoch 24/40  Iteration 6559/11040 Training loss: 1.2495 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6560/11040 Training loss: 1.2495 0.4831 sec/batch\n",
      "Epoch 24/40  Iteration 6561/11040 Training loss: 1.2495 0.4744 sec/batch\n",
      "Epoch 24/40  Iteration 6562/11040 Training loss: 1.2494 0.4813 sec/batch\n",
      "Epoch 24/40  Iteration 6563/11040 Training loss: 1.2493 0.4763 sec/batch\n",
      "Epoch 24/40  Iteration 6564/11040 Training loss: 1.2492 0.4755 sec/batch\n",
      "Epoch 24/40  Iteration 6565/11040 Training loss: 1.2491 0.4721 sec/batch\n",
      "Epoch 24/40  Iteration 6566/11040 Training loss: 1.2490 0.4803 sec/batch\n",
      "Epoch 24/40  Iteration 6567/11040 Training loss: 1.2489 0.4729 sec/batch\n",
      "Epoch 24/40  Iteration 6568/11040 Training loss: 1.2488 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6569/11040 Training loss: 1.2487 0.4765 sec/batch\n",
      "Epoch 24/40  Iteration 6570/11040 Training loss: 1.2486 0.4724 sec/batch\n",
      "Epoch 24/40  Iteration 6571/11040 Training loss: 1.2485 0.4763 sec/batch\n",
      "Epoch 24/40  Iteration 6572/11040 Training loss: 1.2486 0.4747 sec/batch\n",
      "Epoch 24/40  Iteration 6573/11040 Training loss: 1.2485 0.4748 sec/batch\n",
      "Epoch 24/40  Iteration 6574/11040 Training loss: 1.2485 0.4771 sec/batch\n",
      "Epoch 24/40  Iteration 6575/11040 Training loss: 1.2483 0.4746 sec/batch\n",
      "Epoch 24/40  Iteration 6576/11040 Training loss: 1.2482 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6577/11040 Training loss: 1.2483 0.4743 sec/batch\n",
      "Epoch 24/40  Iteration 6578/11040 Training loss: 1.2483 0.4754 sec/batch\n",
      "Epoch 24/40  Iteration 6579/11040 Training loss: 1.2483 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6580/11040 Training loss: 1.2484 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6581/11040 Training loss: 1.2483 0.4739 sec/batch\n",
      "Epoch 24/40  Iteration 6582/11040 Training loss: 1.2482 0.4777 sec/batch\n",
      "Epoch 24/40  Iteration 6583/11040 Training loss: 1.2483 0.4735 sec/batch\n",
      "Epoch 24/40  Iteration 6584/11040 Training loss: 1.2484 0.4759 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40  Iteration 6585/11040 Training loss: 1.2484 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6586/11040 Training loss: 1.2484 0.4643 sec/batch\n",
      "Epoch 24/40  Iteration 6587/11040 Training loss: 1.2485 0.4703 sec/batch\n",
      "Epoch 24/40  Iteration 6588/11040 Training loss: 1.2486 0.4631 sec/batch\n",
      "Epoch 24/40  Iteration 6589/11040 Training loss: 1.2487 0.4733 sec/batch\n",
      "Epoch 24/40  Iteration 6590/11040 Training loss: 1.2486 0.4734 sec/batch\n",
      "Epoch 24/40  Iteration 6591/11040 Training loss: 1.2486 0.4762 sec/batch\n",
      "Epoch 24/40  Iteration 6592/11040 Training loss: 1.2486 0.4737 sec/batch\n",
      "Epoch 24/40  Iteration 6593/11040 Training loss: 1.2485 0.4759 sec/batch\n",
      "Epoch 24/40  Iteration 6594/11040 Training loss: 1.2486 0.4789 sec/batch\n",
      "Epoch 24/40  Iteration 6595/11040 Training loss: 1.2486 0.4717 sec/batch\n",
      "Epoch 24/40  Iteration 6596/11040 Training loss: 1.2485 0.4774 sec/batch\n",
      "Epoch 24/40  Iteration 6597/11040 Training loss: 1.2485 0.4732 sec/batch\n",
      "Epoch 24/40  Iteration 6598/11040 Training loss: 1.2485 0.4814 sec/batch\n",
      "Epoch 24/40  Iteration 6599/11040 Training loss: 1.2484 0.4845 sec/batch\n",
      "Epoch 24/40  Iteration 6600/11040 Training loss: 1.2484 0.4745 sec/batch\n",
      "Epoch 24/40  Iteration 6601/11040 Training loss: 1.2484 0.4774 sec/batch\n",
      "Epoch 24/40  Iteration 6602/11040 Training loss: 1.2484 0.4733 sec/batch\n",
      "Epoch 24/40  Iteration 6603/11040 Training loss: 1.2483 0.4744 sec/batch\n",
      "Epoch 24/40  Iteration 6604/11040 Training loss: 1.2484 0.4934 sec/batch\n",
      "Epoch 24/40  Iteration 6605/11040 Training loss: 1.2485 0.4914 sec/batch\n",
      "Epoch 24/40  Iteration 6606/11040 Training loss: 1.2486 0.4728 sec/batch\n",
      "Epoch 24/40  Iteration 6607/11040 Training loss: 1.2485 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6608/11040 Training loss: 1.2486 0.4753 sec/batch\n",
      "Epoch 24/40  Iteration 6609/11040 Training loss: 1.2486 0.4752 sec/batch\n",
      "Epoch 24/40  Iteration 6610/11040 Training loss: 1.2488 0.4931 sec/batch\n",
      "Epoch 24/40  Iteration 6611/11040 Training loss: 1.2489 0.4750 sec/batch\n",
      "Epoch 24/40  Iteration 6612/11040 Training loss: 1.2490 0.4876 sec/batch\n",
      "Epoch 24/40  Iteration 6613/11040 Training loss: 1.2490 0.4726 sec/batch\n",
      "Epoch 24/40  Iteration 6614/11040 Training loss: 1.2491 0.4740 sec/batch\n",
      "Epoch 24/40  Iteration 6615/11040 Training loss: 1.2493 0.4904 sec/batch\n",
      "Epoch 24/40  Iteration 6616/11040 Training loss: 1.2493 0.4863 sec/batch\n",
      "Epoch 24/40  Iteration 6617/11040 Training loss: 1.2494 0.4758 sec/batch\n",
      "Epoch 24/40  Iteration 6618/11040 Training loss: 1.2495 0.4896 sec/batch\n",
      "Epoch 24/40  Iteration 6619/11040 Training loss: 1.2498 0.4829 sec/batch\n",
      "Epoch 24/40  Iteration 6620/11040 Training loss: 1.2500 0.4857 sec/batch\n",
      "Epoch 24/40  Iteration 6621/11040 Training loss: 1.2501 0.4886 sec/batch\n",
      "Epoch 24/40  Iteration 6622/11040 Training loss: 1.2502 0.4730 sec/batch\n",
      "Epoch 24/40  Iteration 6623/11040 Training loss: 1.2502 0.4782 sec/batch\n",
      "Epoch 24/40  Iteration 6624/11040 Training loss: 1.2503 0.4904 sec/batch\n",
      "Epoch 25/40  Iteration 6625/11040 Training loss: 1.3216 0.4870 sec/batch\n",
      "Epoch 25/40  Iteration 6626/11040 Training loss: 1.2842 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6627/11040 Training loss: 1.2815 0.4933 sec/batch\n",
      "Epoch 25/40  Iteration 6628/11040 Training loss: 1.2799 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6629/11040 Training loss: 1.2759 0.4890 sec/batch\n",
      "Epoch 25/40  Iteration 6630/11040 Training loss: 1.2721 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6631/11040 Training loss: 1.2646 0.4763 sec/batch\n",
      "Epoch 25/40  Iteration 6632/11040 Training loss: 1.2621 0.4751 sec/batch\n",
      "Epoch 25/40  Iteration 6633/11040 Training loss: 1.2581 0.4751 sec/batch\n",
      "Epoch 25/40  Iteration 6634/11040 Training loss: 1.2576 0.4921 sec/batch\n",
      "Epoch 25/40  Iteration 6635/11040 Training loss: 1.2546 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6636/11040 Training loss: 1.2538 0.4768 sec/batch\n",
      "Epoch 25/40  Iteration 6637/11040 Training loss: 1.2511 0.4898 sec/batch\n",
      "Epoch 25/40  Iteration 6638/11040 Training loss: 1.2488 0.4758 sec/batch\n",
      "Epoch 25/40  Iteration 6639/11040 Training loss: 1.2486 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6640/11040 Training loss: 1.2492 0.4760 sec/batch\n",
      "Epoch 25/40  Iteration 6641/11040 Training loss: 1.2467 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6642/11040 Training loss: 1.2446 0.4828 sec/batch\n",
      "Epoch 25/40  Iteration 6643/11040 Training loss: 1.2438 0.4844 sec/batch\n",
      "Epoch 25/40  Iteration 6644/11040 Training loss: 1.2438 0.4766 sec/batch\n",
      "Epoch 25/40  Iteration 6645/11040 Training loss: 1.2450 0.4769 sec/batch\n",
      "Epoch 25/40  Iteration 6646/11040 Training loss: 1.2453 0.4729 sec/batch\n",
      "Epoch 25/40  Iteration 6647/11040 Training loss: 1.2452 0.4751 sec/batch\n",
      "Epoch 25/40  Iteration 6648/11040 Training loss: 1.2457 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6649/11040 Training loss: 1.2447 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6650/11040 Training loss: 1.2439 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6651/11040 Training loss: 1.2436 0.4909 sec/batch\n",
      "Epoch 25/40  Iteration 6652/11040 Training loss: 1.2434 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6653/11040 Training loss: 1.2431 0.4782 sec/batch\n",
      "Epoch 25/40  Iteration 6654/11040 Training loss: 1.2432 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6655/11040 Training loss: 1.2431 0.4737 sec/batch\n",
      "Epoch 25/40  Iteration 6656/11040 Training loss: 1.2427 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6657/11040 Training loss: 1.2419 0.4750 sec/batch\n",
      "Epoch 25/40  Iteration 6658/11040 Training loss: 1.2416 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6659/11040 Training loss: 1.2416 0.4775 sec/batch\n",
      "Epoch 25/40  Iteration 6660/11040 Training loss: 1.2422 0.4725 sec/batch\n",
      "Epoch 25/40  Iteration 6661/11040 Training loss: 1.2422 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6662/11040 Training loss: 1.2413 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6663/11040 Training loss: 1.2406 0.4750 sec/batch\n",
      "Epoch 25/40  Iteration 6664/11040 Training loss: 1.2407 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6665/11040 Training loss: 1.2402 0.4603 sec/batch\n",
      "Epoch 25/40  Iteration 6666/11040 Training loss: 1.2393 0.4735 sec/batch\n",
      "Epoch 25/40  Iteration 6667/11040 Training loss: 1.2396 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6668/11040 Training loss: 1.2388 0.4766 sec/batch\n",
      "Epoch 25/40  Iteration 6669/11040 Training loss: 1.2383 0.4726 sec/batch\n",
      "Epoch 25/40  Iteration 6670/11040 Training loss: 1.2381 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6671/11040 Training loss: 1.2374 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6672/11040 Training loss: 1.2371 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6673/11040 Training loss: 1.2366 0.4596 sec/batch\n",
      "Epoch 25/40  Iteration 6674/11040 Training loss: 1.2371 0.4758 sec/batch\n",
      "Epoch 25/40  Iteration 6675/11040 Training loss: 1.2368 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6676/11040 Training loss: 1.2365 0.4903 sec/batch\n",
      "Epoch 25/40  Iteration 6677/11040 Training loss: 1.2367 0.4628 sec/batch\n",
      "Epoch 25/40  Iteration 6678/11040 Training loss: 1.2362 0.4734 sec/batch\n",
      "Epoch 25/40  Iteration 6679/11040 Training loss: 1.2363 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6680/11040 Training loss: 1.2363 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6681/11040 Training loss: 1.2361 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6682/11040 Training loss: 1.2360 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6683/11040 Training loss: 1.2362 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6684/11040 Training loss: 1.2358 0.4604 sec/batch\n",
      "Epoch 25/40  Iteration 6685/11040 Training loss: 1.2357 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6686/11040 Training loss: 1.2356 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6687/11040 Training loss: 1.2356 0.4607 sec/batch\n",
      "Epoch 25/40  Iteration 6688/11040 Training loss: 1.2358 0.4741 sec/batch\n",
      "Epoch 25/40  Iteration 6689/11040 Training loss: 1.2361 0.4762 sec/batch\n",
      "Epoch 25/40  Iteration 6690/11040 Training loss: 1.2358 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6691/11040 Training loss: 1.2357 0.4751 sec/batch\n",
      "Epoch 25/40  Iteration 6692/11040 Training loss: 1.2360 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6693/11040 Training loss: 1.2358 0.4780 sec/batch\n",
      "Epoch 25/40  Iteration 6694/11040 Training loss: 1.2359 0.4731 sec/batch\n",
      "Epoch 25/40  Iteration 6695/11040 Training loss: 1.2356 0.4728 sec/batch\n",
      "Epoch 25/40  Iteration 6696/11040 Training loss: 1.2352 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6697/11040 Training loss: 1.2347 0.4634 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40  Iteration 6698/11040 Training loss: 1.2343 0.4729 sec/batch\n",
      "Epoch 25/40  Iteration 6699/11040 Training loss: 1.2343 0.4749 sec/batch\n",
      "Epoch 25/40  Iteration 6700/11040 Training loss: 1.2347 0.4641 sec/batch\n",
      "Epoch 25/40  Iteration 6701/11040 Training loss: 1.2342 0.4694 sec/batch\n",
      "Epoch 25/40  Iteration 6702/11040 Training loss: 1.2338 0.4742 sec/batch\n",
      "Epoch 25/40  Iteration 6703/11040 Training loss: 1.2338 0.4775 sec/batch\n",
      "Epoch 25/40  Iteration 6704/11040 Training loss: 1.2338 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6705/11040 Training loss: 1.2337 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6706/11040 Training loss: 1.2338 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6707/11040 Training loss: 1.2338 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6708/11040 Training loss: 1.2337 0.4733 sec/batch\n",
      "Epoch 25/40  Iteration 6709/11040 Training loss: 1.2335 0.4766 sec/batch\n",
      "Epoch 25/40  Iteration 6710/11040 Training loss: 1.2334 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6711/11040 Training loss: 1.2328 0.4588 sec/batch\n",
      "Epoch 25/40  Iteration 6712/11040 Training loss: 1.2318 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6713/11040 Training loss: 1.2318 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6714/11040 Training loss: 1.2318 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6715/11040 Training loss: 1.2319 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6716/11040 Training loss: 1.2318 0.4729 sec/batch\n",
      "Epoch 25/40  Iteration 6717/11040 Training loss: 1.2316 0.4788 sec/batch\n",
      "Epoch 25/40  Iteration 6718/11040 Training loss: 1.2320 0.4880 sec/batch\n",
      "Epoch 25/40  Iteration 6719/11040 Training loss: 1.2324 0.4767 sec/batch\n",
      "Epoch 25/40  Iteration 6720/11040 Training loss: 1.2326 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6721/11040 Training loss: 1.2331 0.4733 sec/batch\n",
      "Epoch 25/40  Iteration 6722/11040 Training loss: 1.2335 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6723/11040 Training loss: 1.2337 0.4742 sec/batch\n",
      "Epoch 25/40  Iteration 6724/11040 Training loss: 1.2335 0.4609 sec/batch\n",
      "Epoch 25/40  Iteration 6725/11040 Training loss: 1.2336 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6726/11040 Training loss: 1.2337 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6727/11040 Training loss: 1.2333 0.4606 sec/batch\n",
      "Epoch 25/40  Iteration 6728/11040 Training loss: 1.2336 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6729/11040 Training loss: 1.2334 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6730/11040 Training loss: 1.2332 0.4741 sec/batch\n",
      "Epoch 25/40  Iteration 6731/11040 Training loss: 1.2332 0.4762 sec/batch\n",
      "Epoch 25/40  Iteration 6732/11040 Training loss: 1.2333 0.4742 sec/batch\n",
      "Epoch 25/40  Iteration 6733/11040 Training loss: 1.2336 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6734/11040 Training loss: 1.2337 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6735/11040 Training loss: 1.2337 0.4752 sec/batch\n",
      "Epoch 25/40  Iteration 6736/11040 Training loss: 1.2339 0.4597 sec/batch\n",
      "Epoch 25/40  Iteration 6737/11040 Training loss: 1.2341 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6738/11040 Training loss: 1.2341 0.4815 sec/batch\n",
      "Epoch 25/40  Iteration 6739/11040 Training loss: 1.2344 0.4692 sec/batch\n",
      "Epoch 25/40  Iteration 6740/11040 Training loss: 1.2347 0.4734 sec/batch\n",
      "Epoch 25/40  Iteration 6741/11040 Training loss: 1.2345 0.4741 sec/batch\n",
      "Epoch 25/40  Iteration 6742/11040 Training loss: 1.2348 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6743/11040 Training loss: 1.2349 0.4736 sec/batch\n",
      "Epoch 25/40  Iteration 6744/11040 Training loss: 1.2350 0.4773 sec/batch\n",
      "Epoch 25/40  Iteration 6745/11040 Training loss: 1.2350 0.4749 sec/batch\n",
      "Epoch 25/40  Iteration 6746/11040 Training loss: 1.2351 0.4749 sec/batch\n",
      "Epoch 25/40  Iteration 6747/11040 Training loss: 1.2355 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6748/11040 Training loss: 1.2360 0.4734 sec/batch\n",
      "Epoch 25/40  Iteration 6749/11040 Training loss: 1.2361 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6750/11040 Training loss: 1.2363 0.4738 sec/batch\n",
      "Epoch 25/40  Iteration 6751/11040 Training loss: 1.2364 0.4765 sec/batch\n",
      "Epoch 25/40  Iteration 6752/11040 Training loss: 1.2365 0.4915 sec/batch\n",
      "Epoch 25/40  Iteration 6753/11040 Training loss: 1.2363 0.4735 sec/batch\n",
      "Epoch 25/40  Iteration 6754/11040 Training loss: 1.2365 0.4598 sec/batch\n",
      "Epoch 25/40  Iteration 6755/11040 Training loss: 1.2364 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6756/11040 Training loss: 1.2363 0.4733 sec/batch\n",
      "Epoch 25/40  Iteration 6757/11040 Training loss: 1.2363 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6758/11040 Training loss: 1.2363 0.4602 sec/batch\n",
      "Epoch 25/40  Iteration 6759/11040 Training loss: 1.2363 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6760/11040 Training loss: 1.2364 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6761/11040 Training loss: 1.2363 0.4735 sec/batch\n",
      "Epoch 25/40  Iteration 6762/11040 Training loss: 1.2364 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6763/11040 Training loss: 1.2366 0.4760 sec/batch\n",
      "Epoch 25/40  Iteration 6764/11040 Training loss: 1.2367 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6765/11040 Training loss: 1.2367 0.4595 sec/batch\n",
      "Epoch 25/40  Iteration 6766/11040 Training loss: 1.2368 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6767/11040 Training loss: 1.2369 0.4758 sec/batch\n",
      "Epoch 25/40  Iteration 6768/11040 Training loss: 1.2368 0.4795 sec/batch\n",
      "Epoch 25/40  Iteration 6769/11040 Training loss: 1.2369 0.4695 sec/batch\n",
      "Epoch 25/40  Iteration 6770/11040 Training loss: 1.2369 0.4598 sec/batch\n",
      "Epoch 25/40  Iteration 6771/11040 Training loss: 1.2369 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6772/11040 Training loss: 1.2369 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6773/11040 Training loss: 1.2368 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6774/11040 Training loss: 1.2368 0.4762 sec/batch\n",
      "Epoch 25/40  Iteration 6775/11040 Training loss: 1.2367 0.4589 sec/batch\n",
      "Epoch 25/40  Iteration 6776/11040 Training loss: 1.2370 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6777/11040 Training loss: 1.2370 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6778/11040 Training loss: 1.2373 0.4621 sec/batch\n",
      "Epoch 25/40  Iteration 6779/11040 Training loss: 1.2373 0.4735 sec/batch\n",
      "Epoch 25/40  Iteration 6780/11040 Training loss: 1.2373 0.4736 sec/batch\n",
      "Epoch 25/40  Iteration 6781/11040 Training loss: 1.2371 0.4767 sec/batch\n",
      "Epoch 25/40  Iteration 6782/11040 Training loss: 1.2372 0.4765 sec/batch\n",
      "Epoch 25/40  Iteration 6783/11040 Training loss: 1.2373 0.4737 sec/batch\n",
      "Epoch 25/40  Iteration 6784/11040 Training loss: 1.2373 0.4604 sec/batch\n",
      "Epoch 25/40  Iteration 6785/11040 Training loss: 1.2374 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6786/11040 Training loss: 1.2374 0.4729 sec/batch\n",
      "Epoch 25/40  Iteration 6787/11040 Training loss: 1.2375 0.4764 sec/batch\n",
      "Epoch 25/40  Iteration 6788/11040 Training loss: 1.2377 0.4572 sec/batch\n",
      "Epoch 25/40  Iteration 6789/11040 Training loss: 1.2379 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6790/11040 Training loss: 1.2381 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6791/11040 Training loss: 1.2381 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6792/11040 Training loss: 1.2380 0.4615 sec/batch\n",
      "Epoch 25/40  Iteration 6793/11040 Training loss: 1.2380 0.4739 sec/batch\n",
      "Epoch 25/40  Iteration 6794/11040 Training loss: 1.2380 0.4594 sec/batch\n",
      "Epoch 25/40  Iteration 6795/11040 Training loss: 1.2382 0.4624 sec/batch\n",
      "Epoch 25/40  Iteration 6796/11040 Training loss: 1.2383 0.4719 sec/batch\n",
      "Epoch 25/40  Iteration 6797/11040 Training loss: 1.2383 0.4734 sec/batch\n",
      "Epoch 25/40  Iteration 6798/11040 Training loss: 1.2383 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6799/11040 Training loss: 1.2383 0.4758 sec/batch\n",
      "Epoch 25/40  Iteration 6800/11040 Training loss: 1.2383 0.4595 sec/batch\n",
      "Epoch 25/40  Iteration 6801/11040 Training loss: 1.2384 0.4739 sec/batch\n",
      "Epoch 25/40  Iteration 6802/11040 Training loss: 1.2384 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6803/11040 Training loss: 1.2387 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6804/11040 Training loss: 1.2389 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6805/11040 Training loss: 1.2388 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6806/11040 Training loss: 1.2389 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6807/11040 Training loss: 1.2388 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6808/11040 Training loss: 1.2390 0.4752 sec/batch\n",
      "Epoch 25/40  Iteration 6809/11040 Training loss: 1.2391 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6810/11040 Training loss: 1.2392 0.4759 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40  Iteration 6811/11040 Training loss: 1.2394 0.4588 sec/batch\n",
      "Epoch 25/40  Iteration 6812/11040 Training loss: 1.2395 0.4912 sec/batch\n",
      "Epoch 25/40  Iteration 6813/11040 Training loss: 1.2395 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6814/11040 Training loss: 1.2394 0.4642 sec/batch\n",
      "Epoch 25/40  Iteration 6815/11040 Training loss: 1.2393 0.4718 sec/batch\n",
      "Epoch 25/40  Iteration 6816/11040 Training loss: 1.2393 0.4747 sec/batch\n",
      "Epoch 25/40  Iteration 6817/11040 Training loss: 1.2394 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6818/11040 Training loss: 1.2394 0.4791 sec/batch\n",
      "Epoch 25/40  Iteration 6819/11040 Training loss: 1.2396 0.4716 sec/batch\n",
      "Epoch 25/40  Iteration 6820/11040 Training loss: 1.2395 0.4739 sec/batch\n",
      "Epoch 25/40  Iteration 6821/11040 Training loss: 1.2396 0.4601 sec/batch\n",
      "Epoch 25/40  Iteration 6822/11040 Training loss: 1.2396 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6823/11040 Training loss: 1.2397 0.4763 sec/batch\n",
      "Epoch 25/40  Iteration 6824/11040 Training loss: 1.2399 0.4769 sec/batch\n",
      "Epoch 25/40  Iteration 6825/11040 Training loss: 1.2401 0.4714 sec/batch\n",
      "Epoch 25/40  Iteration 6826/11040 Training loss: 1.2402 0.4763 sec/batch\n",
      "Epoch 25/40  Iteration 6827/11040 Training loss: 1.2404 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6828/11040 Training loss: 1.2405 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6829/11040 Training loss: 1.2405 0.4757 sec/batch\n",
      "Epoch 25/40  Iteration 6830/11040 Training loss: 1.2404 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6831/11040 Training loss: 1.2405 0.4742 sec/batch\n",
      "Epoch 25/40  Iteration 6832/11040 Training loss: 1.2404 0.4765 sec/batch\n",
      "Epoch 25/40  Iteration 6833/11040 Training loss: 1.2403 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6834/11040 Training loss: 1.2402 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6835/11040 Training loss: 1.2402 0.4738 sec/batch\n",
      "Epoch 25/40  Iteration 6836/11040 Training loss: 1.2403 0.4602 sec/batch\n",
      "Epoch 25/40  Iteration 6837/11040 Training loss: 1.2402 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6838/11040 Training loss: 1.2402 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6839/11040 Training loss: 1.2401 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6840/11040 Training loss: 1.2400 0.4764 sec/batch\n",
      "Epoch 25/40  Iteration 6841/11040 Training loss: 1.2399 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6842/11040 Training loss: 1.2398 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6843/11040 Training loss: 1.2397 0.4745 sec/batch\n",
      "Epoch 25/40  Iteration 6844/11040 Training loss: 1.2397 0.4752 sec/batch\n",
      "Epoch 25/40  Iteration 6845/11040 Training loss: 1.2396 0.4597 sec/batch\n",
      "Epoch 25/40  Iteration 6846/11040 Training loss: 1.2394 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6847/11040 Training loss: 1.2393 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6848/11040 Training loss: 1.2394 0.4750 sec/batch\n",
      "Epoch 25/40  Iteration 6849/11040 Training loss: 1.2393 0.4587 sec/batch\n",
      "Epoch 25/40  Iteration 6850/11040 Training loss: 1.2393 0.4636 sec/batch\n",
      "Epoch 25/40  Iteration 6851/11040 Training loss: 1.2392 0.4710 sec/batch\n",
      "Epoch 25/40  Iteration 6852/11040 Training loss: 1.2391 0.4755 sec/batch\n",
      "Epoch 25/40  Iteration 6853/11040 Training loss: 1.2391 0.4754 sec/batch\n",
      "Epoch 25/40  Iteration 6854/11040 Training loss: 1.2392 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6855/11040 Training loss: 1.2392 0.4901 sec/batch\n",
      "Epoch 25/40  Iteration 6856/11040 Training loss: 1.2393 0.4769 sec/batch\n",
      "Epoch 25/40  Iteration 6857/11040 Training loss: 1.2392 0.4725 sec/batch\n",
      "Epoch 25/40  Iteration 6858/11040 Training loss: 1.2391 0.4743 sec/batch\n",
      "Epoch 25/40  Iteration 6859/11040 Training loss: 1.2392 0.4600 sec/batch\n",
      "Epoch 25/40  Iteration 6860/11040 Training loss: 1.2393 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6861/11040 Training loss: 1.2393 0.4813 sec/batch\n",
      "Epoch 25/40  Iteration 6862/11040 Training loss: 1.2394 0.4789 sec/batch\n",
      "Epoch 25/40  Iteration 6863/11040 Training loss: 1.2395 0.4761 sec/batch\n",
      "Epoch 25/40  Iteration 6864/11040 Training loss: 1.2396 0.4746 sec/batch\n",
      "Epoch 25/40  Iteration 6865/11040 Training loss: 1.2397 0.4750 sec/batch\n",
      "Epoch 25/40  Iteration 6866/11040 Training loss: 1.2397 0.4783 sec/batch\n",
      "Epoch 25/40  Iteration 6867/11040 Training loss: 1.2396 0.4730 sec/batch\n",
      "Epoch 25/40  Iteration 6868/11040 Training loss: 1.2396 0.4740 sec/batch\n",
      "Epoch 25/40  Iteration 6869/11040 Training loss: 1.2396 0.4756 sec/batch\n",
      "Epoch 25/40  Iteration 6870/11040 Training loss: 1.2396 0.4758 sec/batch\n",
      "Epoch 25/40  Iteration 6871/11040 Training loss: 1.2396 0.4779 sec/batch\n",
      "Epoch 25/40  Iteration 6872/11040 Training loss: 1.2395 0.4691 sec/batch\n",
      "Epoch 25/40  Iteration 6873/11040 Training loss: 1.2395 0.4617 sec/batch\n",
      "Epoch 25/40  Iteration 6874/11040 Training loss: 1.2395 0.4715 sec/batch\n",
      "Epoch 25/40  Iteration 6875/11040 Training loss: 1.2394 0.4763 sec/batch\n",
      "Epoch 25/40  Iteration 6876/11040 Training loss: 1.2394 0.4742 sec/batch\n",
      "Epoch 25/40  Iteration 6877/11040 Training loss: 1.2394 0.4748 sec/batch\n",
      "Epoch 25/40  Iteration 6878/11040 Training loss: 1.2395 0.4762 sec/batch\n",
      "Epoch 25/40  Iteration 6879/11040 Training loss: 1.2395 0.4590 sec/batch\n",
      "Epoch 25/40  Iteration 6880/11040 Training loss: 1.2396 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6881/11040 Training loss: 1.2396 0.4773 sec/batch\n",
      "Epoch 25/40  Iteration 6882/11040 Training loss: 1.2397 0.4727 sec/batch\n",
      "Epoch 25/40  Iteration 6883/11040 Training loss: 1.2396 0.4721 sec/batch\n",
      "Epoch 25/40  Iteration 6884/11040 Training loss: 1.2397 0.4765 sec/batch\n",
      "Epoch 25/40  Iteration 6885/11040 Training loss: 1.2397 0.4736 sec/batch\n",
      "Epoch 25/40  Iteration 6886/11040 Training loss: 1.2399 0.4779 sec/batch\n",
      "Epoch 25/40  Iteration 6887/11040 Training loss: 1.2400 0.4713 sec/batch\n",
      "Epoch 25/40  Iteration 6888/11040 Training loss: 1.2400 0.4762 sec/batch\n",
      "Epoch 25/40  Iteration 6889/11040 Training loss: 1.2400 0.4759 sec/batch\n",
      "Epoch 25/40  Iteration 6890/11040 Training loss: 1.2401 0.4738 sec/batch\n",
      "Epoch 25/40  Iteration 6891/11040 Training loss: 1.2403 0.4779 sec/batch\n",
      "Epoch 25/40  Iteration 6892/11040 Training loss: 1.2404 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6893/11040 Training loss: 1.2404 0.4722 sec/batch\n",
      "Epoch 25/40  Iteration 6894/11040 Training loss: 1.2405 0.4606 sec/batch\n",
      "Epoch 25/40  Iteration 6895/11040 Training loss: 1.2409 0.4730 sec/batch\n",
      "Epoch 25/40  Iteration 6896/11040 Training loss: 1.2411 0.4744 sec/batch\n",
      "Epoch 25/40  Iteration 6897/11040 Training loss: 1.2412 0.4753 sec/batch\n",
      "Epoch 25/40  Iteration 6898/11040 Training loss: 1.2412 0.4606 sec/batch\n",
      "Epoch 25/40  Iteration 6899/11040 Training loss: 1.2413 0.4762 sec/batch\n",
      "Epoch 25/40  Iteration 6900/11040 Training loss: 1.2414 0.4734 sec/batch\n",
      "Epoch 26/40  Iteration 6901/11040 Training loss: 1.3276 0.4749 sec/batch\n",
      "Epoch 26/40  Iteration 6902/11040 Training loss: 1.2877 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 6903/11040 Training loss: 1.2828 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 6904/11040 Training loss: 1.2776 0.4731 sec/batch\n",
      "Epoch 26/40  Iteration 6905/11040 Training loss: 1.2751 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 6906/11040 Training loss: 1.2711 0.4585 sec/batch\n",
      "Epoch 26/40  Iteration 6907/11040 Training loss: 1.2608 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 6908/11040 Training loss: 1.2581 0.4729 sec/batch\n",
      "Epoch 26/40  Iteration 6909/11040 Training loss: 1.2528 0.4710 sec/batch\n",
      "Epoch 26/40  Iteration 6910/11040 Training loss: 1.2521 0.4784 sec/batch\n",
      "Epoch 26/40  Iteration 6911/11040 Training loss: 1.2492 0.4765 sec/batch\n",
      "Epoch 26/40  Iteration 6912/11040 Training loss: 1.2475 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 6913/11040 Training loss: 1.2439 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 6914/11040 Training loss: 1.2415 0.4761 sec/batch\n",
      "Epoch 26/40  Iteration 6915/11040 Training loss: 1.2406 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 6916/11040 Training loss: 1.2408 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 6917/11040 Training loss: 1.2374 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 6918/11040 Training loss: 1.2354 0.4906 sec/batch\n",
      "Epoch 26/40  Iteration 6919/11040 Training loss: 1.2347 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 6920/11040 Training loss: 1.2351 0.4732 sec/batch\n",
      "Epoch 26/40  Iteration 6921/11040 Training loss: 1.2365 0.4618 sec/batch\n",
      "Epoch 26/40  Iteration 6922/11040 Training loss: 1.2373 0.4717 sec/batch\n",
      "Epoch 26/40  Iteration 6923/11040 Training loss: 1.2367 0.4766 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40  Iteration 6924/11040 Training loss: 1.2374 0.4740 sec/batch\n",
      "Epoch 26/40  Iteration 6925/11040 Training loss: 1.2363 0.4773 sec/batch\n",
      "Epoch 26/40  Iteration 6926/11040 Training loss: 1.2359 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 6927/11040 Training loss: 1.2355 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 6928/11040 Training loss: 1.2351 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 6929/11040 Training loss: 1.2348 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 6930/11040 Training loss: 1.2351 0.4754 sec/batch\n",
      "Epoch 26/40  Iteration 6931/11040 Training loss: 1.2349 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 6932/11040 Training loss: 1.2346 0.4753 sec/batch\n",
      "Epoch 26/40  Iteration 6933/11040 Training loss: 1.2340 0.4752 sec/batch\n",
      "Epoch 26/40  Iteration 6934/11040 Training loss: 1.2336 0.4751 sec/batch\n",
      "Epoch 26/40  Iteration 6935/11040 Training loss: 1.2337 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 6936/11040 Training loss: 1.2343 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 6937/11040 Training loss: 1.2344 0.4748 sec/batch\n",
      "Epoch 26/40  Iteration 6938/11040 Training loss: 1.2338 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 6939/11040 Training loss: 1.2332 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 6940/11040 Training loss: 1.2332 0.4768 sec/batch\n",
      "Epoch 26/40  Iteration 6941/11040 Training loss: 1.2327 0.4737 sec/batch\n",
      "Epoch 26/40  Iteration 6942/11040 Training loss: 1.2317 0.4627 sec/batch\n",
      "Epoch 26/40  Iteration 6943/11040 Training loss: 1.2319 0.4724 sec/batch\n",
      "Epoch 26/40  Iteration 6944/11040 Training loss: 1.2310 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 6945/11040 Training loss: 1.2306 0.4774 sec/batch\n",
      "Epoch 26/40  Iteration 6946/11040 Training loss: 1.2302 0.4737 sec/batch\n",
      "Epoch 26/40  Iteration 6947/11040 Training loss: 1.2295 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 6948/11040 Training loss: 1.2295 0.4909 sec/batch\n",
      "Epoch 26/40  Iteration 6949/11040 Training loss: 1.2290 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 6950/11040 Training loss: 1.2293 0.4589 sec/batch\n",
      "Epoch 26/40  Iteration 6951/11040 Training loss: 1.2290 0.4750 sec/batch\n",
      "Epoch 26/40  Iteration 6952/11040 Training loss: 1.2288 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 6953/11040 Training loss: 1.2291 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 6954/11040 Training loss: 1.2288 0.4747 sec/batch\n",
      "Epoch 26/40  Iteration 6955/11040 Training loss: 1.2287 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 6956/11040 Training loss: 1.2289 0.4759 sec/batch\n",
      "Epoch 26/40  Iteration 6957/11040 Training loss: 1.2286 0.4735 sec/batch\n",
      "Epoch 26/40  Iteration 6958/11040 Training loss: 1.2286 0.4752 sec/batch\n",
      "Epoch 26/40  Iteration 6959/11040 Training loss: 1.2289 0.4773 sec/batch\n",
      "Epoch 26/40  Iteration 6960/11040 Training loss: 1.2285 0.4736 sec/batch\n",
      "Epoch 26/40  Iteration 6961/11040 Training loss: 1.2284 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 6962/11040 Training loss: 1.2283 0.4606 sec/batch\n",
      "Epoch 26/40  Iteration 6963/11040 Training loss: 1.2282 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 6964/11040 Training loss: 1.2282 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 6965/11040 Training loss: 1.2284 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 6966/11040 Training loss: 1.2281 0.4755 sec/batch\n",
      "Epoch 26/40  Iteration 6967/11040 Training loss: 1.2279 0.4586 sec/batch\n",
      "Epoch 26/40  Iteration 6968/11040 Training loss: 1.2280 0.4753 sec/batch\n",
      "Epoch 26/40  Iteration 6969/11040 Training loss: 1.2277 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 6970/11040 Training loss: 1.2278 0.4754 sec/batch\n",
      "Epoch 26/40  Iteration 6971/11040 Training loss: 1.2275 0.4738 sec/batch\n",
      "Epoch 26/40  Iteration 6972/11040 Training loss: 1.2271 0.4773 sec/batch\n",
      "Epoch 26/40  Iteration 6973/11040 Training loss: 1.2267 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 6974/11040 Training loss: 1.2263 0.4758 sec/batch\n",
      "Epoch 26/40  Iteration 6975/11040 Training loss: 1.2263 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 6976/11040 Training loss: 1.2266 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 6977/11040 Training loss: 1.2262 0.4758 sec/batch\n",
      "Epoch 26/40  Iteration 6978/11040 Training loss: 1.2257 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 6979/11040 Training loss: 1.2255 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 6980/11040 Training loss: 1.2256 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 6981/11040 Training loss: 1.2254 0.4767 sec/batch\n",
      "Epoch 26/40  Iteration 6982/11040 Training loss: 1.2256 0.4771 sec/batch\n",
      "Epoch 26/40  Iteration 6983/11040 Training loss: 1.2255 0.4753 sec/batch\n",
      "Epoch 26/40  Iteration 6984/11040 Training loss: 1.2255 0.4740 sec/batch\n",
      "Epoch 26/40  Iteration 6985/11040 Training loss: 1.2253 0.4750 sec/batch\n",
      "Epoch 26/40  Iteration 6986/11040 Training loss: 1.2252 0.4749 sec/batch\n",
      "Epoch 26/40  Iteration 6987/11040 Training loss: 1.2246 0.4758 sec/batch\n",
      "Epoch 26/40  Iteration 6988/11040 Training loss: 1.2237 0.4749 sec/batch\n",
      "Epoch 26/40  Iteration 6989/11040 Training loss: 1.2238 0.4748 sec/batch\n",
      "Epoch 26/40  Iteration 6990/11040 Training loss: 1.2239 0.4751 sec/batch\n",
      "Epoch 26/40  Iteration 6991/11040 Training loss: 1.2240 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 6992/11040 Training loss: 1.2239 0.4754 sec/batch\n",
      "Epoch 26/40  Iteration 6993/11040 Training loss: 1.2237 0.4736 sec/batch\n",
      "Epoch 26/40  Iteration 6994/11040 Training loss: 1.2241 0.4768 sec/batch\n",
      "Epoch 26/40  Iteration 6995/11040 Training loss: 1.2246 0.4901 sec/batch\n",
      "Epoch 26/40  Iteration 6996/11040 Training loss: 1.2248 0.4879 sec/batch\n",
      "Epoch 26/40  Iteration 6997/11040 Training loss: 1.2253 0.4788 sec/batch\n",
      "Epoch 26/40  Iteration 6998/11040 Training loss: 1.2257 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 6999/11040 Training loss: 1.2258 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7000/11040 Training loss: 1.2255 0.4899 sec/batch\n",
      "Validation loss: 1.25607 Saving checkpoint!\n",
      "Epoch 26/40  Iteration 7001/11040 Training loss: 1.2270 0.4689 sec/batch\n",
      "Epoch 26/40  Iteration 7002/11040 Training loss: 1.2273 0.4875 sec/batch\n",
      "Epoch 26/40  Iteration 7003/11040 Training loss: 1.2269 0.4732 sec/batch\n",
      "Epoch 26/40  Iteration 7004/11040 Training loss: 1.2271 0.4775 sec/batch\n",
      "Epoch 26/40  Iteration 7005/11040 Training loss: 1.2269 0.4726 sec/batch\n",
      "Epoch 26/40  Iteration 7006/11040 Training loss: 1.2267 0.4787 sec/batch\n",
      "Epoch 26/40  Iteration 7007/11040 Training loss: 1.2267 0.4705 sec/batch\n",
      "Epoch 26/40  Iteration 7008/11040 Training loss: 1.2268 0.4739 sec/batch\n",
      "Epoch 26/40  Iteration 7009/11040 Training loss: 1.2271 0.4742 sec/batch\n",
      "Epoch 26/40  Iteration 7010/11040 Training loss: 1.2272 0.4731 sec/batch\n",
      "Epoch 26/40  Iteration 7011/11040 Training loss: 1.2271 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7012/11040 Training loss: 1.2273 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 7013/11040 Training loss: 1.2274 0.4605 sec/batch\n",
      "Epoch 26/40  Iteration 7014/11040 Training loss: 1.2275 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 7015/11040 Training loss: 1.2278 0.4734 sec/batch\n",
      "Epoch 26/40  Iteration 7016/11040 Training loss: 1.2281 0.4731 sec/batch\n",
      "Epoch 26/40  Iteration 7017/11040 Training loss: 1.2280 0.4775 sec/batch\n",
      "Epoch 26/40  Iteration 7018/11040 Training loss: 1.2283 0.4809 sec/batch\n",
      "Epoch 26/40  Iteration 7019/11040 Training loss: 1.2283 0.4852 sec/batch\n",
      "Epoch 26/40  Iteration 7020/11040 Training loss: 1.2284 0.4740 sec/batch\n",
      "Epoch 26/40  Iteration 7021/11040 Training loss: 1.2283 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7022/11040 Training loss: 1.2284 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7023/11040 Training loss: 1.2287 0.4740 sec/batch\n",
      "Epoch 26/40  Iteration 7024/11040 Training loss: 1.2292 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 7025/11040 Training loss: 1.2293 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 7026/11040 Training loss: 1.2296 0.4800 sec/batch\n",
      "Epoch 26/40  Iteration 7027/11040 Training loss: 1.2296 0.4704 sec/batch\n",
      "Epoch 26/40  Iteration 7028/11040 Training loss: 1.2296 0.4740 sec/batch\n",
      "Epoch 26/40  Iteration 7029/11040 Training loss: 1.2294 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7030/11040 Training loss: 1.2297 0.4637 sec/batch\n",
      "Epoch 26/40  Iteration 7031/11040 Training loss: 1.2296 0.4717 sec/batch\n",
      "Epoch 26/40  Iteration 7032/11040 Training loss: 1.2294 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 7033/11040 Training loss: 1.2295 0.4767 sec/batch\n",
      "Epoch 26/40  Iteration 7034/11040 Training loss: 1.2293 0.4589 sec/batch\n",
      "Epoch 26/40  Iteration 7035/11040 Training loss: 1.2295 0.4744 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40  Iteration 7036/11040 Training loss: 1.2296 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7037/11040 Training loss: 1.2294 0.4779 sec/batch\n",
      "Epoch 26/40  Iteration 7038/11040 Training loss: 1.2295 0.4730 sec/batch\n",
      "Epoch 26/40  Iteration 7039/11040 Training loss: 1.2297 0.4758 sec/batch\n",
      "Epoch 26/40  Iteration 7040/11040 Training loss: 1.2298 0.4912 sec/batch\n",
      "Epoch 26/40  Iteration 7041/11040 Training loss: 1.2299 0.4765 sec/batch\n",
      "Epoch 26/40  Iteration 7042/11040 Training loss: 1.2300 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 7043/11040 Training loss: 1.2300 0.4753 sec/batch\n",
      "Epoch 26/40  Iteration 7044/11040 Training loss: 1.2299 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7045/11040 Training loss: 1.2299 0.4771 sec/batch\n",
      "Epoch 26/40  Iteration 7046/11040 Training loss: 1.2299 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 7047/11040 Training loss: 1.2300 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7048/11040 Training loss: 1.2299 0.4616 sec/batch\n",
      "Epoch 26/40  Iteration 7049/11040 Training loss: 1.2298 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 7050/11040 Training loss: 1.2298 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 7051/11040 Training loss: 1.2298 0.4771 sec/batch\n",
      "Epoch 26/40  Iteration 7052/11040 Training loss: 1.2300 0.4732 sec/batch\n",
      "Epoch 26/40  Iteration 7053/11040 Training loss: 1.2301 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 7054/11040 Training loss: 1.2302 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7055/11040 Training loss: 1.2303 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 7056/11040 Training loss: 1.2303 0.4747 sec/batch\n",
      "Epoch 26/40  Iteration 7057/11040 Training loss: 1.2301 0.4755 sec/batch\n",
      "Epoch 26/40  Iteration 7058/11040 Training loss: 1.2303 0.4735 sec/batch\n",
      "Epoch 26/40  Iteration 7059/11040 Training loss: 1.2304 0.4772 sec/batch\n",
      "Epoch 26/40  Iteration 7060/11040 Training loss: 1.2305 0.4759 sec/batch\n",
      "Epoch 26/40  Iteration 7061/11040 Training loss: 1.2305 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 7062/11040 Training loss: 1.2305 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7063/11040 Training loss: 1.2306 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7064/11040 Training loss: 1.2307 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 7065/11040 Training loss: 1.2309 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 7066/11040 Training loss: 1.2312 0.4751 sec/batch\n",
      "Epoch 26/40  Iteration 7067/11040 Training loss: 1.2312 0.4621 sec/batch\n",
      "Epoch 26/40  Iteration 7068/11040 Training loss: 1.2311 0.4735 sec/batch\n",
      "Epoch 26/40  Iteration 7069/11040 Training loss: 1.2310 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 7070/11040 Training loss: 1.2310 0.4616 sec/batch\n",
      "Epoch 26/40  Iteration 7071/11040 Training loss: 1.2312 0.4737 sec/batch\n",
      "Epoch 26/40  Iteration 7072/11040 Training loss: 1.2313 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 7073/11040 Training loss: 1.2313 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7074/11040 Training loss: 1.2313 0.4580 sec/batch\n",
      "Epoch 26/40  Iteration 7075/11040 Training loss: 1.2313 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 7076/11040 Training loss: 1.2314 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7077/11040 Training loss: 1.2315 0.4732 sec/batch\n",
      "Epoch 26/40  Iteration 7078/11040 Training loss: 1.2314 0.4750 sec/batch\n",
      "Epoch 26/40  Iteration 7079/11040 Training loss: 1.2316 0.4606 sec/batch\n",
      "Epoch 26/40  Iteration 7080/11040 Training loss: 1.2317 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7081/11040 Training loss: 1.2316 0.4769 sec/batch\n",
      "Epoch 26/40  Iteration 7082/11040 Training loss: 1.2317 0.4722 sec/batch\n",
      "Epoch 26/40  Iteration 7083/11040 Training loss: 1.2316 0.4596 sec/batch\n",
      "Epoch 26/40  Iteration 7084/11040 Training loss: 1.2317 0.4595 sec/batch\n",
      "Epoch 26/40  Iteration 7085/11040 Training loss: 1.2318 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 7086/11040 Training loss: 1.2319 0.4769 sec/batch\n",
      "Epoch 26/40  Iteration 7087/11040 Training loss: 1.2321 0.4723 sec/batch\n",
      "Epoch 26/40  Iteration 7088/11040 Training loss: 1.2322 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7089/11040 Training loss: 1.2323 0.4771 sec/batch\n",
      "Epoch 26/40  Iteration 7090/11040 Training loss: 1.2323 0.4839 sec/batch\n",
      "Epoch 26/40  Iteration 7091/11040 Training loss: 1.2322 0.4811 sec/batch\n",
      "Epoch 26/40  Iteration 7092/11040 Training loss: 1.2322 0.4737 sec/batch\n",
      "Epoch 26/40  Iteration 7093/11040 Training loss: 1.2322 0.4749 sec/batch\n",
      "Epoch 26/40  Iteration 7094/11040 Training loss: 1.2322 0.4848 sec/batch\n",
      "Epoch 26/40  Iteration 7095/11040 Training loss: 1.2325 0.4620 sec/batch\n",
      "Epoch 26/40  Iteration 7096/11040 Training loss: 1.2324 0.4723 sec/batch\n",
      "Epoch 26/40  Iteration 7097/11040 Training loss: 1.2325 0.4607 sec/batch\n",
      "Epoch 26/40  Iteration 7098/11040 Training loss: 1.2326 0.4604 sec/batch\n",
      "Epoch 26/40  Iteration 7099/11040 Training loss: 1.2327 0.4732 sec/batch\n",
      "Epoch 26/40  Iteration 7100/11040 Training loss: 1.2328 0.4759 sec/batch\n",
      "Epoch 26/40  Iteration 7101/11040 Training loss: 1.2330 0.4754 sec/batch\n",
      "Epoch 26/40  Iteration 7102/11040 Training loss: 1.2332 0.4595 sec/batch\n",
      "Epoch 26/40  Iteration 7103/11040 Training loss: 1.2333 0.4578 sec/batch\n",
      "Epoch 26/40  Iteration 7104/11040 Training loss: 1.2334 0.4602 sec/batch\n",
      "Epoch 26/40  Iteration 7105/11040 Training loss: 1.2333 0.4732 sec/batch\n",
      "Epoch 26/40  Iteration 7106/11040 Training loss: 1.2333 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 7107/11040 Training loss: 1.2334 0.4758 sec/batch\n",
      "Epoch 26/40  Iteration 7108/11040 Training loss: 1.2334 0.4735 sec/batch\n",
      "Epoch 26/40  Iteration 7109/11040 Training loss: 1.2332 0.4588 sec/batch\n",
      "Epoch 26/40  Iteration 7110/11040 Training loss: 1.2332 0.4739 sec/batch\n",
      "Epoch 26/40  Iteration 7111/11040 Training loss: 1.2332 0.4754 sec/batch\n",
      "Epoch 26/40  Iteration 7112/11040 Training loss: 1.2332 0.4755 sec/batch\n",
      "Epoch 26/40  Iteration 7113/11040 Training loss: 1.2332 0.4751 sec/batch\n",
      "Epoch 26/40  Iteration 7114/11040 Training loss: 1.2331 0.4895 sec/batch\n",
      "Epoch 26/40  Iteration 7115/11040 Training loss: 1.2330 0.4751 sec/batch\n",
      "Epoch 26/40  Iteration 7116/11040 Training loss: 1.2329 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 7117/11040 Training loss: 1.2328 0.4620 sec/batch\n",
      "Epoch 26/40  Iteration 7118/11040 Training loss: 1.2327 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 7119/11040 Training loss: 1.2326 0.4746 sec/batch\n",
      "Epoch 26/40  Iteration 7120/11040 Training loss: 1.2326 0.4747 sec/batch\n",
      "Epoch 26/40  Iteration 7121/11040 Training loss: 1.2324 0.4735 sec/batch\n",
      "Epoch 26/40  Iteration 7122/11040 Training loss: 1.2323 0.4602 sec/batch\n",
      "Epoch 26/40  Iteration 7123/11040 Training loss: 1.2322 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7124/11040 Training loss: 1.2323 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7125/11040 Training loss: 1.2322 0.4761 sec/batch\n",
      "Epoch 26/40  Iteration 7126/11040 Training loss: 1.2322 0.4766 sec/batch\n",
      "Epoch 26/40  Iteration 7127/11040 Training loss: 1.2320 0.4740 sec/batch\n",
      "Epoch 26/40  Iteration 7128/11040 Training loss: 1.2319 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7129/11040 Training loss: 1.2320 0.4753 sec/batch\n",
      "Epoch 26/40  Iteration 7130/11040 Training loss: 1.2321 0.4755 sec/batch\n",
      "Epoch 26/40  Iteration 7131/11040 Training loss: 1.2321 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7132/11040 Training loss: 1.2322 0.4762 sec/batch\n",
      "Epoch 26/40  Iteration 7133/11040 Training loss: 1.2322 0.4745 sec/batch\n",
      "Epoch 26/40  Iteration 7134/11040 Training loss: 1.2321 0.4688 sec/batch\n",
      "Epoch 26/40  Iteration 7135/11040 Training loss: 1.2322 0.4815 sec/batch\n",
      "Epoch 26/40  Iteration 7136/11040 Training loss: 1.2323 0.4741 sec/batch\n",
      "Epoch 26/40  Iteration 7137/11040 Training loss: 1.2323 0.4605 sec/batch\n",
      "Epoch 26/40  Iteration 7138/11040 Training loss: 1.2323 0.4752 sec/batch\n",
      "Epoch 26/40  Iteration 7139/11040 Training loss: 1.2324 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7140/11040 Training loss: 1.2325 0.4770 sec/batch\n",
      "Epoch 26/40  Iteration 7141/11040 Training loss: 1.2325 0.4774 sec/batch\n",
      "Epoch 26/40  Iteration 7142/11040 Training loss: 1.2326 0.4723 sec/batch\n",
      "Epoch 26/40  Iteration 7143/11040 Training loss: 1.2326 0.4905 sec/batch\n",
      "Epoch 26/40  Iteration 7144/11040 Training loss: 1.2326 0.4850 sec/batch\n",
      "Epoch 26/40  Iteration 7145/11040 Training loss: 1.2325 0.4815 sec/batch\n",
      "Epoch 26/40  Iteration 7146/11040 Training loss: 1.2325 0.4759 sec/batch\n",
      "Epoch 26/40  Iteration 7147/11040 Training loss: 1.2325 0.4764 sec/batch\n",
      "Epoch 26/40  Iteration 7148/11040 Training loss: 1.2325 0.4729 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40  Iteration 7149/11040 Training loss: 1.2325 0.4760 sec/batch\n",
      "Epoch 26/40  Iteration 7150/11040 Training loss: 1.2325 0.4736 sec/batch\n",
      "Epoch 26/40  Iteration 7151/11040 Training loss: 1.2324 0.4735 sec/batch\n",
      "Epoch 26/40  Iteration 7152/11040 Training loss: 1.2324 0.4769 sec/batch\n",
      "Epoch 26/40  Iteration 7153/11040 Training loss: 1.2324 0.4769 sec/batch\n",
      "Epoch 26/40  Iteration 7154/11040 Training loss: 1.2325 0.4706 sec/batch\n",
      "Epoch 26/40  Iteration 7155/11040 Training loss: 1.2324 0.4836 sec/batch\n",
      "Epoch 26/40  Iteration 7156/11040 Training loss: 1.2325 0.4849 sec/batch\n",
      "Epoch 26/40  Iteration 7157/11040 Training loss: 1.2326 0.4758 sec/batch\n",
      "Epoch 26/40  Iteration 7158/11040 Training loss: 1.2327 0.4759 sec/batch\n",
      "Epoch 26/40  Iteration 7159/11040 Training loss: 1.2327 0.4775 sec/batch\n",
      "Epoch 26/40  Iteration 7160/11040 Training loss: 1.2327 0.4742 sec/batch\n",
      "Epoch 26/40  Iteration 7161/11040 Training loss: 1.2327 0.4874 sec/batch\n",
      "Epoch 26/40  Iteration 7162/11040 Training loss: 1.2329 0.4767 sec/batch\n",
      "Epoch 26/40  Iteration 7163/11040 Training loss: 1.2330 0.4742 sec/batch\n",
      "Epoch 26/40  Iteration 7164/11040 Training loss: 1.2331 0.4766 sec/batch\n",
      "Epoch 26/40  Iteration 7165/11040 Training loss: 1.2331 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7166/11040 Training loss: 1.2332 0.4756 sec/batch\n",
      "Epoch 26/40  Iteration 7167/11040 Training loss: 1.2333 0.4943 sec/batch\n",
      "Epoch 26/40  Iteration 7168/11040 Training loss: 1.2334 0.4886 sec/batch\n",
      "Epoch 26/40  Iteration 7169/11040 Training loss: 1.2335 0.4736 sec/batch\n",
      "Epoch 26/40  Iteration 7170/11040 Training loss: 1.2336 0.4759 sec/batch\n",
      "Epoch 26/40  Iteration 7171/11040 Training loss: 1.2339 0.4743 sec/batch\n",
      "Epoch 26/40  Iteration 7172/11040 Training loss: 1.2341 0.4772 sec/batch\n",
      "Epoch 26/40  Iteration 7173/11040 Training loss: 1.2342 0.4744 sec/batch\n",
      "Epoch 26/40  Iteration 7174/11040 Training loss: 1.2343 0.4761 sec/batch\n",
      "Epoch 26/40  Iteration 7175/11040 Training loss: 1.2343 0.4733 sec/batch\n",
      "Epoch 26/40  Iteration 7176/11040 Training loss: 1.2344 0.4757 sec/batch\n",
      "Epoch 27/40  Iteration 7177/11040 Training loss: 1.3091 0.4761 sec/batch\n",
      "Epoch 27/40  Iteration 7178/11040 Training loss: 1.2713 0.4735 sec/batch\n",
      "Epoch 27/40  Iteration 7179/11040 Training loss: 1.2684 0.4779 sec/batch\n",
      "Epoch 27/40  Iteration 7180/11040 Training loss: 1.2665 0.4881 sec/batch\n",
      "Epoch 27/40  Iteration 7181/11040 Training loss: 1.2628 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7182/11040 Training loss: 1.2588 0.4779 sec/batch\n",
      "Epoch 27/40  Iteration 7183/11040 Training loss: 1.2494 0.4755 sec/batch\n",
      "Epoch 27/40  Iteration 7184/11040 Training loss: 1.2473 0.4601 sec/batch\n",
      "Epoch 27/40  Iteration 7185/11040 Training loss: 1.2440 0.4731 sec/batch\n",
      "Epoch 27/40  Iteration 7186/11040 Training loss: 1.2436 0.4763 sec/batch\n",
      "Epoch 27/40  Iteration 7187/11040 Training loss: 1.2402 0.4907 sec/batch\n",
      "Epoch 27/40  Iteration 7188/11040 Training loss: 1.2385 0.4752 sec/batch\n",
      "Epoch 27/40  Iteration 7189/11040 Training loss: 1.2360 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7190/11040 Training loss: 1.2337 0.4782 sec/batch\n",
      "Epoch 27/40  Iteration 7191/11040 Training loss: 1.2332 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7192/11040 Training loss: 1.2336 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7193/11040 Training loss: 1.2310 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7194/11040 Training loss: 1.2292 0.4760 sec/batch\n",
      "Epoch 27/40  Iteration 7195/11040 Training loss: 1.2284 0.4744 sec/batch\n",
      "Epoch 27/40  Iteration 7196/11040 Training loss: 1.2291 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7197/11040 Training loss: 1.2306 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7198/11040 Training loss: 1.2311 0.4770 sec/batch\n",
      "Epoch 27/40  Iteration 7199/11040 Training loss: 1.2308 0.4757 sec/batch\n",
      "Epoch 27/40  Iteration 7200/11040 Training loss: 1.2312 0.4734 sec/batch\n",
      "Epoch 27/40  Iteration 7201/11040 Training loss: 1.2307 0.4935 sec/batch\n",
      "Epoch 27/40  Iteration 7202/11040 Training loss: 1.2299 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7203/11040 Training loss: 1.2297 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7204/11040 Training loss: 1.2295 0.4748 sec/batch\n",
      "Epoch 27/40  Iteration 7205/11040 Training loss: 1.2295 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7206/11040 Training loss: 1.2296 0.4758 sec/batch\n",
      "Epoch 27/40  Iteration 7207/11040 Training loss: 1.2295 0.4751 sec/batch\n",
      "Epoch 27/40  Iteration 7208/11040 Training loss: 1.2291 0.4898 sec/batch\n",
      "Epoch 27/40  Iteration 7209/11040 Training loss: 1.2280 0.4755 sec/batch\n",
      "Epoch 27/40  Iteration 7210/11040 Training loss: 1.2276 0.4761 sec/batch\n",
      "Epoch 27/40  Iteration 7211/11040 Training loss: 1.2276 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7212/11040 Training loss: 1.2281 0.4768 sec/batch\n",
      "Epoch 27/40  Iteration 7213/11040 Training loss: 1.2280 0.4922 sec/batch\n",
      "Epoch 27/40  Iteration 7214/11040 Training loss: 1.2273 0.4740 sec/batch\n",
      "Epoch 27/40  Iteration 7215/11040 Training loss: 1.2266 0.4759 sec/batch\n",
      "Epoch 27/40  Iteration 7216/11040 Training loss: 1.2265 0.4772 sec/batch\n",
      "Epoch 27/40  Iteration 7217/11040 Training loss: 1.2262 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7218/11040 Training loss: 1.2250 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7219/11040 Training loss: 1.2254 0.4755 sec/batch\n",
      "Epoch 27/40  Iteration 7220/11040 Training loss: 1.2242 0.4782 sec/batch\n",
      "Epoch 27/40  Iteration 7221/11040 Training loss: 1.2236 0.4739 sec/batch\n",
      "Epoch 27/40  Iteration 7222/11040 Training loss: 1.2229 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7223/11040 Training loss: 1.2221 0.4734 sec/batch\n",
      "Epoch 27/40  Iteration 7224/11040 Training loss: 1.2218 0.4740 sec/batch\n",
      "Epoch 27/40  Iteration 7225/11040 Training loss: 1.2215 0.4625 sec/batch\n",
      "Epoch 27/40  Iteration 7226/11040 Training loss: 1.2218 0.4727 sec/batch\n",
      "Epoch 27/40  Iteration 7227/11040 Training loss: 1.2214 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7228/11040 Training loss: 1.2213 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7229/11040 Training loss: 1.2214 0.4749 sec/batch\n",
      "Epoch 27/40  Iteration 7230/11040 Training loss: 1.2210 0.4741 sec/batch\n",
      "Epoch 27/40  Iteration 7231/11040 Training loss: 1.2210 0.4773 sec/batch\n",
      "Epoch 27/40  Iteration 7232/11040 Training loss: 1.2210 0.4737 sec/batch\n",
      "Epoch 27/40  Iteration 7233/11040 Training loss: 1.2208 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7234/11040 Training loss: 1.2206 0.4607 sec/batch\n",
      "Epoch 27/40  Iteration 7235/11040 Training loss: 1.2208 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7236/11040 Training loss: 1.2203 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7237/11040 Training loss: 1.2205 0.4764 sec/batch\n",
      "Epoch 27/40  Iteration 7238/11040 Training loss: 1.2204 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7239/11040 Training loss: 1.2206 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7240/11040 Training loss: 1.2205 0.4759 sec/batch\n",
      "Epoch 27/40  Iteration 7241/11040 Training loss: 1.2208 0.4757 sec/batch\n",
      "Epoch 27/40  Iteration 7242/11040 Training loss: 1.2206 0.4764 sec/batch\n",
      "Epoch 27/40  Iteration 7243/11040 Training loss: 1.2205 0.4748 sec/batch\n",
      "Epoch 27/40  Iteration 7244/11040 Training loss: 1.2207 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7245/11040 Training loss: 1.2204 0.4770 sec/batch\n",
      "Epoch 27/40  Iteration 7246/11040 Training loss: 1.2205 0.4739 sec/batch\n",
      "Epoch 27/40  Iteration 7247/11040 Training loss: 1.2203 0.4768 sec/batch\n",
      "Epoch 27/40  Iteration 7248/11040 Training loss: 1.2198 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7249/11040 Training loss: 1.2194 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7250/11040 Training loss: 1.2189 0.4597 sec/batch\n",
      "Epoch 27/40  Iteration 7251/11040 Training loss: 1.2188 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7252/11040 Training loss: 1.2190 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7253/11040 Training loss: 1.2184 0.4774 sec/batch\n",
      "Epoch 27/40  Iteration 7254/11040 Training loss: 1.2180 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7255/11040 Training loss: 1.2178 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7256/11040 Training loss: 1.2178 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7257/11040 Training loss: 1.2175 0.4757 sec/batch\n",
      "Epoch 27/40  Iteration 7258/11040 Training loss: 1.2176 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7259/11040 Training loss: 1.2175 0.4759 sec/batch\n",
      "Epoch 27/40  Iteration 7260/11040 Training loss: 1.2175 0.4737 sec/batch\n",
      "Epoch 27/40  Iteration 7261/11040 Training loss: 1.2172 0.4748 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40  Iteration 7262/11040 Training loss: 1.2171 0.4758 sec/batch\n",
      "Epoch 27/40  Iteration 7263/11040 Training loss: 1.2166 0.4766 sec/batch\n",
      "Epoch 27/40  Iteration 7264/11040 Training loss: 1.2157 0.4738 sec/batch\n",
      "Epoch 27/40  Iteration 7265/11040 Training loss: 1.2157 0.4766 sec/batch\n",
      "Epoch 27/40  Iteration 7266/11040 Training loss: 1.2157 0.4756 sec/batch\n",
      "Epoch 27/40  Iteration 7267/11040 Training loss: 1.2158 0.4735 sec/batch\n",
      "Epoch 27/40  Iteration 7268/11040 Training loss: 1.2157 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7269/11040 Training loss: 1.2156 0.4903 sec/batch\n",
      "Epoch 27/40  Iteration 7270/11040 Training loss: 1.2158 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7271/11040 Training loss: 1.2164 0.4752 sec/batch\n",
      "Epoch 27/40  Iteration 7272/11040 Training loss: 1.2166 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7273/11040 Training loss: 1.2171 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7274/11040 Training loss: 1.2176 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7275/11040 Training loss: 1.2177 0.4766 sec/batch\n",
      "Epoch 27/40  Iteration 7276/11040 Training loss: 1.2175 0.4768 sec/batch\n",
      "Epoch 27/40  Iteration 7277/11040 Training loss: 1.2177 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7278/11040 Training loss: 1.2179 0.4815 sec/batch\n",
      "Epoch 27/40  Iteration 7279/11040 Training loss: 1.2175 0.4839 sec/batch\n",
      "Epoch 27/40  Iteration 7280/11040 Training loss: 1.2177 0.4748 sec/batch\n",
      "Epoch 27/40  Iteration 7281/11040 Training loss: 1.2175 0.4759 sec/batch\n",
      "Epoch 27/40  Iteration 7282/11040 Training loss: 1.2173 0.4768 sec/batch\n",
      "Epoch 27/40  Iteration 7283/11040 Training loss: 1.2173 0.4740 sec/batch\n",
      "Epoch 27/40  Iteration 7284/11040 Training loss: 1.2175 0.4757 sec/batch\n",
      "Epoch 27/40  Iteration 7285/11040 Training loss: 1.2179 0.4750 sec/batch\n",
      "Epoch 27/40  Iteration 7286/11040 Training loss: 1.2179 0.4770 sec/batch\n",
      "Epoch 27/40  Iteration 7287/11040 Training loss: 1.2180 0.4840 sec/batch\n",
      "Epoch 27/40  Iteration 7288/11040 Training loss: 1.2183 0.4812 sec/batch\n",
      "Epoch 27/40  Iteration 7289/11040 Training loss: 1.2183 0.4751 sec/batch\n",
      "Epoch 27/40  Iteration 7290/11040 Training loss: 1.2183 0.4777 sec/batch\n",
      "Epoch 27/40  Iteration 7291/11040 Training loss: 1.2186 0.4884 sec/batch\n",
      "Epoch 27/40  Iteration 7292/11040 Training loss: 1.2188 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7293/11040 Training loss: 1.2186 0.4769 sec/batch\n",
      "Epoch 27/40  Iteration 7294/11040 Training loss: 1.2189 0.4892 sec/batch\n",
      "Epoch 27/40  Iteration 7295/11040 Training loss: 1.2190 0.4768 sec/batch\n",
      "Epoch 27/40  Iteration 7296/11040 Training loss: 1.2191 0.4765 sec/batch\n",
      "Epoch 27/40  Iteration 7297/11040 Training loss: 1.2191 0.4902 sec/batch\n",
      "Epoch 27/40  Iteration 7298/11040 Training loss: 1.2192 0.4760 sec/batch\n",
      "Epoch 27/40  Iteration 7299/11040 Training loss: 1.2195 0.4756 sec/batch\n",
      "Epoch 27/40  Iteration 7300/11040 Training loss: 1.2201 0.4749 sec/batch\n",
      "Epoch 27/40  Iteration 7301/11040 Training loss: 1.2201 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7302/11040 Training loss: 1.2205 0.4760 sec/batch\n",
      "Epoch 27/40  Iteration 7303/11040 Training loss: 1.2205 0.4770 sec/batch\n",
      "Epoch 27/40  Iteration 7304/11040 Training loss: 1.2206 0.4739 sec/batch\n",
      "Epoch 27/40  Iteration 7305/11040 Training loss: 1.2204 0.4669 sec/batch\n",
      "Epoch 27/40  Iteration 7306/11040 Training loss: 1.2207 0.4842 sec/batch\n",
      "Epoch 27/40  Iteration 7307/11040 Training loss: 1.2206 0.4771 sec/batch\n",
      "Epoch 27/40  Iteration 7308/11040 Training loss: 1.2205 0.4902 sec/batch\n",
      "Epoch 27/40  Iteration 7309/11040 Training loss: 1.2205 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7310/11040 Training loss: 1.2204 0.4756 sec/batch\n",
      "Epoch 27/40  Iteration 7311/11040 Training loss: 1.2205 0.4903 sec/batch\n",
      "Epoch 27/40  Iteration 7312/11040 Training loss: 1.2206 0.4783 sec/batch\n",
      "Epoch 27/40  Iteration 7313/11040 Training loss: 1.2205 0.4896 sec/batch\n",
      "Epoch 27/40  Iteration 7314/11040 Training loss: 1.2206 0.4883 sec/batch\n",
      "Epoch 27/40  Iteration 7315/11040 Training loss: 1.2208 0.4710 sec/batch\n",
      "Epoch 27/40  Iteration 7316/11040 Training loss: 1.2209 0.4922 sec/batch\n",
      "Epoch 27/40  Iteration 7317/11040 Training loss: 1.2209 0.4736 sec/batch\n",
      "Epoch 27/40  Iteration 7318/11040 Training loss: 1.2211 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7319/11040 Training loss: 1.2211 0.4751 sec/batch\n",
      "Epoch 27/40  Iteration 7320/11040 Training loss: 1.2209 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7321/11040 Training loss: 1.2210 0.4758 sec/batch\n",
      "Epoch 27/40  Iteration 7322/11040 Training loss: 1.2209 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7323/11040 Training loss: 1.2209 0.4756 sec/batch\n",
      "Epoch 27/40  Iteration 7324/11040 Training loss: 1.2209 0.4762 sec/batch\n",
      "Epoch 27/40  Iteration 7325/11040 Training loss: 1.2208 0.4927 sec/batch\n",
      "Epoch 27/40  Iteration 7326/11040 Training loss: 1.2209 0.4738 sec/batch\n",
      "Epoch 27/40  Iteration 7327/11040 Training loss: 1.2209 0.4616 sec/batch\n",
      "Epoch 27/40  Iteration 7328/11040 Training loss: 1.2211 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7329/11040 Training loss: 1.2211 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7330/11040 Training loss: 1.2212 0.4592 sec/batch\n",
      "Epoch 27/40  Iteration 7331/11040 Training loss: 1.2213 0.4750 sec/batch\n",
      "Epoch 27/40  Iteration 7332/11040 Training loss: 1.2213 0.4737 sec/batch\n",
      "Epoch 27/40  Iteration 7333/11040 Training loss: 1.2212 0.4761 sec/batch\n",
      "Epoch 27/40  Iteration 7334/11040 Training loss: 1.2213 0.4757 sec/batch\n",
      "Epoch 27/40  Iteration 7335/11040 Training loss: 1.2214 0.4732 sec/batch\n",
      "Epoch 27/40  Iteration 7336/11040 Training loss: 1.2214 0.4595 sec/batch\n",
      "Epoch 27/40  Iteration 7337/11040 Training loss: 1.2215 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7338/11040 Training loss: 1.2215 0.4718 sec/batch\n",
      "Epoch 27/40  Iteration 7339/11040 Training loss: 1.2217 0.4818 sec/batch\n",
      "Epoch 27/40  Iteration 7340/11040 Training loss: 1.2218 0.4712 sec/batch\n",
      "Epoch 27/40  Iteration 7341/11040 Training loss: 1.2220 0.4621 sec/batch\n",
      "Epoch 27/40  Iteration 7342/11040 Training loss: 1.2223 0.4731 sec/batch\n",
      "Epoch 27/40  Iteration 7343/11040 Training loss: 1.2223 0.4866 sec/batch\n",
      "Epoch 27/40  Iteration 7344/11040 Training loss: 1.2223 0.4647 sec/batch\n",
      "Epoch 27/40  Iteration 7345/11040 Training loss: 1.2224 0.4741 sec/batch\n",
      "Epoch 27/40  Iteration 7346/11040 Training loss: 1.2224 0.4769 sec/batch\n",
      "Epoch 27/40  Iteration 7347/11040 Training loss: 1.2227 0.4724 sec/batch\n",
      "Epoch 27/40  Iteration 7348/11040 Training loss: 1.2228 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7349/11040 Training loss: 1.2228 0.4614 sec/batch\n",
      "Epoch 27/40  Iteration 7350/11040 Training loss: 1.2228 0.4732 sec/batch\n",
      "Epoch 27/40  Iteration 7351/11040 Training loss: 1.2229 0.4748 sec/batch\n",
      "Epoch 27/40  Iteration 7352/11040 Training loss: 1.2229 0.4772 sec/batch\n",
      "Epoch 27/40  Iteration 7353/11040 Training loss: 1.2230 0.4724 sec/batch\n",
      "Epoch 27/40  Iteration 7354/11040 Training loss: 1.2229 0.4594 sec/batch\n",
      "Epoch 27/40  Iteration 7355/11040 Training loss: 1.2232 0.4605 sec/batch\n",
      "Epoch 27/40  Iteration 7356/11040 Training loss: 1.2233 0.4738 sec/batch\n",
      "Epoch 27/40  Iteration 7357/11040 Training loss: 1.2233 0.4744 sec/batch\n",
      "Epoch 27/40  Iteration 7358/11040 Training loss: 1.2233 0.4758 sec/batch\n",
      "Epoch 27/40  Iteration 7359/11040 Training loss: 1.2233 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7360/11040 Training loss: 1.2233 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7361/11040 Training loss: 1.2234 0.4756 sec/batch\n",
      "Epoch 27/40  Iteration 7362/11040 Training loss: 1.2234 0.4675 sec/batch\n",
      "Epoch 27/40  Iteration 7363/11040 Training loss: 1.2236 0.4825 sec/batch\n",
      "Epoch 27/40  Iteration 7364/11040 Training loss: 1.2236 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7365/11040 Training loss: 1.2236 0.4759 sec/batch\n",
      "Epoch 27/40  Iteration 7366/11040 Training loss: 1.2236 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7367/11040 Training loss: 1.2235 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7368/11040 Training loss: 1.2235 0.4751 sec/batch\n",
      "Epoch 27/40  Iteration 7369/11040 Training loss: 1.2234 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7370/11040 Training loss: 1.2235 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7371/11040 Training loss: 1.2237 0.4619 sec/batch\n",
      "Epoch 27/40  Iteration 7372/11040 Training loss: 1.2236 0.4734 sec/batch\n",
      "Epoch 27/40  Iteration 7373/11040 Training loss: 1.2238 0.4741 sec/batch\n",
      "Epoch 27/40  Iteration 7374/11040 Training loss: 1.2238 0.4752 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40  Iteration 7375/11040 Training loss: 1.2239 0.4752 sec/batch\n",
      "Epoch 27/40  Iteration 7376/11040 Training loss: 1.2240 0.4762 sec/batch\n",
      "Epoch 27/40  Iteration 7377/11040 Training loss: 1.2241 0.4741 sec/batch\n",
      "Epoch 27/40  Iteration 7378/11040 Training loss: 1.2243 0.4732 sec/batch\n",
      "Epoch 27/40  Iteration 7379/11040 Training loss: 1.2244 0.4748 sec/batch\n",
      "Epoch 27/40  Iteration 7380/11040 Training loss: 1.2245 0.4773 sec/batch\n",
      "Epoch 27/40  Iteration 7381/11040 Training loss: 1.2245 0.4576 sec/batch\n",
      "Epoch 27/40  Iteration 7382/11040 Training loss: 1.2244 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7383/11040 Training loss: 1.2245 0.4751 sec/batch\n",
      "Epoch 27/40  Iteration 7384/11040 Training loss: 1.2245 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7385/11040 Training loss: 1.2244 0.4607 sec/batch\n",
      "Epoch 27/40  Iteration 7386/11040 Training loss: 1.2243 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7387/11040 Training loss: 1.2243 0.4738 sec/batch\n",
      "Epoch 27/40  Iteration 7388/11040 Training loss: 1.2244 0.4766 sec/batch\n",
      "Epoch 27/40  Iteration 7389/11040 Training loss: 1.2244 0.4739 sec/batch\n",
      "Epoch 27/40  Iteration 7390/11040 Training loss: 1.2244 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7391/11040 Training loss: 1.2243 0.4733 sec/batch\n",
      "Epoch 27/40  Iteration 7392/11040 Training loss: 1.2242 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7393/11040 Training loss: 1.2240 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7394/11040 Training loss: 1.2238 0.4607 sec/batch\n",
      "Epoch 27/40  Iteration 7395/11040 Training loss: 1.2237 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7396/11040 Training loss: 1.2236 0.4743 sec/batch\n",
      "Epoch 27/40  Iteration 7397/11040 Training loss: 1.2235 0.4764 sec/batch\n",
      "Epoch 27/40  Iteration 7398/11040 Training loss: 1.2233 0.4731 sec/batch\n",
      "Epoch 27/40  Iteration 7399/11040 Training loss: 1.2232 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7400/11040 Training loss: 1.2233 0.4674 sec/batch\n",
      "Epoch 27/40  Iteration 7401/11040 Training loss: 1.2233 0.4852 sec/batch\n",
      "Epoch 27/40  Iteration 7402/11040 Training loss: 1.2232 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7403/11040 Training loss: 1.2230 0.4738 sec/batch\n",
      "Epoch 27/40  Iteration 7404/11040 Training loss: 1.2230 0.4607 sec/batch\n",
      "Epoch 27/40  Iteration 7405/11040 Training loss: 1.2230 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7406/11040 Training loss: 1.2231 0.4758 sec/batch\n",
      "Epoch 27/40  Iteration 7407/11040 Training loss: 1.2231 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7408/11040 Training loss: 1.2232 0.4738 sec/batch\n",
      "Epoch 27/40  Iteration 7409/11040 Training loss: 1.2231 0.4755 sec/batch\n",
      "Epoch 27/40  Iteration 7410/11040 Training loss: 1.2230 0.4766 sec/batch\n",
      "Epoch 27/40  Iteration 7411/11040 Training loss: 1.2232 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7412/11040 Training loss: 1.2232 0.4894 sec/batch\n",
      "Epoch 27/40  Iteration 7413/11040 Training loss: 1.2232 0.4769 sec/batch\n",
      "Epoch 27/40  Iteration 7414/11040 Training loss: 1.2233 0.4892 sec/batch\n",
      "Epoch 27/40  Iteration 7415/11040 Training loss: 1.2234 0.4750 sec/batch\n",
      "Epoch 27/40  Iteration 7416/11040 Training loss: 1.2235 0.4762 sec/batch\n",
      "Epoch 27/40  Iteration 7417/11040 Training loss: 1.2235 0.4753 sec/batch\n",
      "Epoch 27/40  Iteration 7418/11040 Training loss: 1.2236 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7419/11040 Training loss: 1.2235 0.4756 sec/batch\n",
      "Epoch 27/40  Iteration 7420/11040 Training loss: 1.2235 0.4744 sec/batch\n",
      "Epoch 27/40  Iteration 7421/11040 Training loss: 1.2234 0.4764 sec/batch\n",
      "Epoch 27/40  Iteration 7422/11040 Training loss: 1.2234 0.4745 sec/batch\n",
      "Epoch 27/40  Iteration 7423/11040 Training loss: 1.2234 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7424/11040 Training loss: 1.2234 0.4763 sec/batch\n",
      "Epoch 27/40  Iteration 7425/11040 Training loss: 1.2234 0.4736 sec/batch\n",
      "Epoch 27/40  Iteration 7426/11040 Training loss: 1.2234 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7427/11040 Training loss: 1.2233 0.4776 sec/batch\n",
      "Epoch 27/40  Iteration 7428/11040 Training loss: 1.2233 0.4742 sec/batch\n",
      "Epoch 27/40  Iteration 7429/11040 Training loss: 1.2233 0.4699 sec/batch\n",
      "Epoch 27/40  Iteration 7430/11040 Training loss: 1.2233 0.4961 sec/batch\n",
      "Epoch 27/40  Iteration 7431/11040 Training loss: 1.2234 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7432/11040 Training loss: 1.2235 0.4787 sec/batch\n",
      "Epoch 27/40  Iteration 7433/11040 Training loss: 1.2235 0.4727 sec/batch\n",
      "Epoch 27/40  Iteration 7434/11040 Training loss: 1.2236 0.4755 sec/batch\n",
      "Epoch 27/40  Iteration 7435/11040 Training loss: 1.2235 0.4740 sec/batch\n",
      "Epoch 27/40  Iteration 7436/11040 Training loss: 1.2236 0.4767 sec/batch\n",
      "Epoch 27/40  Iteration 7437/11040 Training loss: 1.2237 0.4754 sec/batch\n",
      "Epoch 27/40  Iteration 7438/11040 Training loss: 1.2238 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7439/11040 Training loss: 1.2239 0.4901 sec/batch\n",
      "Epoch 27/40  Iteration 7440/11040 Training loss: 1.2240 0.4773 sec/batch\n",
      "Epoch 27/40  Iteration 7441/11040 Training loss: 1.2240 0.4891 sec/batch\n",
      "Epoch 27/40  Iteration 7442/11040 Training loss: 1.2242 0.4747 sec/batch\n",
      "Epoch 27/40  Iteration 7443/11040 Training loss: 1.2244 0.4729 sec/batch\n",
      "Epoch 27/40  Iteration 7444/11040 Training loss: 1.2245 0.4746 sec/batch\n",
      "Epoch 27/40  Iteration 7445/11040 Training loss: 1.2246 0.4759 sec/batch\n",
      "Epoch 27/40  Iteration 7446/11040 Training loss: 1.2247 0.4767 sec/batch\n",
      "Epoch 27/40  Iteration 7447/11040 Training loss: 1.2250 0.4778 sec/batch\n",
      "Epoch 27/40  Iteration 7448/11040 Training loss: 1.2253 0.4916 sec/batch\n",
      "Epoch 27/40  Iteration 7449/11040 Training loss: 1.2254 0.4744 sec/batch\n",
      "Epoch 27/40  Iteration 7450/11040 Training loss: 1.2255 0.4744 sec/batch\n",
      "Epoch 27/40  Iteration 7451/11040 Training loss: 1.2255 0.4778 sec/batch\n",
      "Epoch 27/40  Iteration 7452/11040 Training loss: 1.2256 0.4896 sec/batch\n",
      "Epoch 28/40  Iteration 7453/11040 Training loss: 1.3050 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7454/11040 Training loss: 1.2708 0.4755 sec/batch\n",
      "Epoch 28/40  Iteration 7455/11040 Training loss: 1.2637 0.4911 sec/batch\n",
      "Epoch 28/40  Iteration 7456/11040 Training loss: 1.2630 0.4771 sec/batch\n",
      "Epoch 28/40  Iteration 7457/11040 Training loss: 1.2585 0.4903 sec/batch\n",
      "Epoch 28/40  Iteration 7458/11040 Training loss: 1.2536 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7459/11040 Training loss: 1.2454 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7460/11040 Training loss: 1.2427 0.4778 sec/batch\n",
      "Epoch 28/40  Iteration 7461/11040 Training loss: 1.2390 0.4883 sec/batch\n",
      "Epoch 28/40  Iteration 7462/11040 Training loss: 1.2380 0.4916 sec/batch\n",
      "Epoch 28/40  Iteration 7463/11040 Training loss: 1.2345 0.4751 sec/batch\n",
      "Epoch 28/40  Iteration 7464/11040 Training loss: 1.2333 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7465/11040 Training loss: 1.2305 0.4786 sec/batch\n",
      "Epoch 28/40  Iteration 7466/11040 Training loss: 1.2287 0.4909 sec/batch\n",
      "Epoch 28/40  Iteration 7467/11040 Training loss: 1.2274 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7468/11040 Training loss: 1.2272 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7469/11040 Training loss: 1.2243 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7470/11040 Training loss: 1.2221 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7471/11040 Training loss: 1.2211 0.4923 sec/batch\n",
      "Epoch 28/40  Iteration 7472/11040 Training loss: 1.2210 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7473/11040 Training loss: 1.2230 0.4885 sec/batch\n",
      "Epoch 28/40  Iteration 7474/11040 Training loss: 1.2235 0.4717 sec/batch\n",
      "Epoch 28/40  Iteration 7475/11040 Training loss: 1.2228 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7476/11040 Training loss: 1.2233 0.4915 sec/batch\n",
      "Epoch 28/40  Iteration 7477/11040 Training loss: 1.2222 0.4903 sec/batch\n",
      "Epoch 28/40  Iteration 7478/11040 Training loss: 1.2217 0.4767 sec/batch\n",
      "Epoch 28/40  Iteration 7479/11040 Training loss: 1.2210 0.4755 sec/batch\n",
      "Epoch 28/40  Iteration 7480/11040 Training loss: 1.2208 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7481/11040 Training loss: 1.2209 0.4905 sec/batch\n",
      "Epoch 28/40  Iteration 7482/11040 Training loss: 1.2210 0.4916 sec/batch\n",
      "Epoch 28/40  Iteration 7483/11040 Training loss: 1.2207 0.4744 sec/batch\n",
      "Epoch 28/40  Iteration 7484/11040 Training loss: 1.2202 0.4760 sec/batch\n",
      "Epoch 28/40  Iteration 7485/11040 Training loss: 1.2194 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7486/11040 Training loss: 1.2191 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7487/11040 Training loss: 1.2190 0.4915 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40  Iteration 7488/11040 Training loss: 1.2194 0.4902 sec/batch\n",
      "Epoch 28/40  Iteration 7489/11040 Training loss: 1.2194 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7490/11040 Training loss: 1.2187 0.4745 sec/batch\n",
      "Epoch 28/40  Iteration 7491/11040 Training loss: 1.2181 0.4928 sec/batch\n",
      "Epoch 28/40  Iteration 7492/11040 Training loss: 1.2180 0.4761 sec/batch\n",
      "Epoch 28/40  Iteration 7493/11040 Training loss: 1.2176 0.4698 sec/batch\n",
      "Epoch 28/40  Iteration 7494/11040 Training loss: 1.2164 0.4643 sec/batch\n",
      "Epoch 28/40  Iteration 7495/11040 Training loss: 1.2165 0.4746 sec/batch\n",
      "Epoch 28/40  Iteration 7496/11040 Training loss: 1.2156 0.4770 sec/batch\n",
      "Epoch 28/40  Iteration 7497/11040 Training loss: 1.2151 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7498/11040 Training loss: 1.2143 0.4906 sec/batch\n",
      "Epoch 28/40  Iteration 7499/11040 Training loss: 1.2136 0.4750 sec/batch\n",
      "Epoch 28/40  Iteration 7500/11040 Training loss: 1.2135 0.4752 sec/batch\n",
      "Validation loss: 1.24845 Saving checkpoint!\n",
      "Epoch 28/40  Iteration 7501/11040 Training loss: 1.2161 0.4689 sec/batch\n",
      "Epoch 28/40  Iteration 7502/11040 Training loss: 1.2166 0.4577 sec/batch\n",
      "Epoch 28/40  Iteration 7503/11040 Training loss: 1.2163 0.4722 sec/batch\n",
      "Epoch 28/40  Iteration 7504/11040 Training loss: 1.2161 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7505/11040 Training loss: 1.2163 0.4751 sec/batch\n",
      "Epoch 28/40  Iteration 7506/11040 Training loss: 1.2159 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7507/11040 Training loss: 1.2158 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7508/11040 Training loss: 1.2159 0.4603 sec/batch\n",
      "Epoch 28/40  Iteration 7509/11040 Training loss: 1.2157 0.4749 sec/batch\n",
      "Epoch 28/40  Iteration 7510/11040 Training loss: 1.2155 0.4594 sec/batch\n",
      "Epoch 28/40  Iteration 7511/11040 Training loss: 1.2158 0.4586 sec/batch\n",
      "Epoch 28/40  Iteration 7512/11040 Training loss: 1.2153 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7513/11040 Training loss: 1.2152 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7514/11040 Training loss: 1.2151 0.4836 sec/batch\n",
      "Epoch 28/40  Iteration 7515/11040 Training loss: 1.2152 0.4618 sec/batch\n",
      "Epoch 28/40  Iteration 7516/11040 Training loss: 1.2151 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7517/11040 Training loss: 1.2153 0.4589 sec/batch\n",
      "Epoch 28/40  Iteration 7518/11040 Training loss: 1.2150 0.4761 sec/batch\n",
      "Epoch 28/40  Iteration 7519/11040 Training loss: 1.2150 0.4758 sec/batch\n",
      "Epoch 28/40  Iteration 7520/11040 Training loss: 1.2151 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7521/11040 Training loss: 1.2148 0.4756 sec/batch\n",
      "Epoch 28/40  Iteration 7522/11040 Training loss: 1.2147 0.4726 sec/batch\n",
      "Epoch 28/40  Iteration 7523/11040 Training loss: 1.2143 0.4646 sec/batch\n",
      "Epoch 28/40  Iteration 7524/11040 Training loss: 1.2138 0.4706 sec/batch\n",
      "Epoch 28/40  Iteration 7525/11040 Training loss: 1.2133 0.4744 sec/batch\n",
      "Epoch 28/40  Iteration 7526/11040 Training loss: 1.2127 0.4750 sec/batch\n",
      "Epoch 28/40  Iteration 7527/11040 Training loss: 1.2126 0.4751 sec/batch\n",
      "Epoch 28/40  Iteration 7528/11040 Training loss: 1.2129 0.4594 sec/batch\n",
      "Epoch 28/40  Iteration 7529/11040 Training loss: 1.2124 0.4580 sec/batch\n",
      "Epoch 28/40  Iteration 7530/11040 Training loss: 1.2119 0.4740 sec/batch\n",
      "Epoch 28/40  Iteration 7531/11040 Training loss: 1.2118 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7532/11040 Training loss: 1.2118 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7533/11040 Training loss: 1.2116 0.4727 sec/batch\n",
      "Epoch 28/40  Iteration 7534/11040 Training loss: 1.2118 0.4601 sec/batch\n",
      "Epoch 28/40  Iteration 7535/11040 Training loss: 1.2118 0.4606 sec/batch\n",
      "Epoch 28/40  Iteration 7536/11040 Training loss: 1.2116 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7537/11040 Training loss: 1.2113 0.4740 sec/batch\n",
      "Epoch 28/40  Iteration 7538/11040 Training loss: 1.2114 0.4745 sec/batch\n",
      "Epoch 28/40  Iteration 7539/11040 Training loss: 1.2107 0.4740 sec/batch\n",
      "Epoch 28/40  Iteration 7540/11040 Training loss: 1.2099 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7541/11040 Training loss: 1.2098 0.4774 sec/batch\n",
      "Epoch 28/40  Iteration 7542/11040 Training loss: 1.2098 0.4750 sec/batch\n",
      "Epoch 28/40  Iteration 7543/11040 Training loss: 1.2098 0.4731 sec/batch\n",
      "Epoch 28/40  Iteration 7544/11040 Training loss: 1.2097 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7545/11040 Training loss: 1.2095 0.4598 sec/batch\n",
      "Epoch 28/40  Iteration 7546/11040 Training loss: 1.2098 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7547/11040 Training loss: 1.2103 0.4730 sec/batch\n",
      "Epoch 28/40  Iteration 7548/11040 Training loss: 1.2105 0.4751 sec/batch\n",
      "Epoch 28/40  Iteration 7549/11040 Training loss: 1.2108 0.4808 sec/batch\n",
      "Epoch 28/40  Iteration 7550/11040 Training loss: 1.2112 0.4797 sec/batch\n",
      "Epoch 28/40  Iteration 7551/11040 Training loss: 1.2114 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7552/11040 Training loss: 1.2112 0.4739 sec/batch\n",
      "Epoch 28/40  Iteration 7553/11040 Training loss: 1.2113 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7554/11040 Training loss: 1.2115 0.4751 sec/batch\n",
      "Epoch 28/40  Iteration 7555/11040 Training loss: 1.2111 0.4599 sec/batch\n",
      "Epoch 28/40  Iteration 7556/11040 Training loss: 1.2114 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7557/11040 Training loss: 1.2112 0.4734 sec/batch\n",
      "Epoch 28/40  Iteration 7558/11040 Training loss: 1.2111 0.4742 sec/batch\n",
      "Epoch 28/40  Iteration 7559/11040 Training loss: 1.2110 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7560/11040 Training loss: 1.2111 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7561/11040 Training loss: 1.2113 0.4758 sec/batch\n",
      "Epoch 28/40  Iteration 7562/11040 Training loss: 1.2113 0.4733 sec/batch\n",
      "Epoch 28/40  Iteration 7563/11040 Training loss: 1.2112 0.4742 sec/batch\n",
      "Epoch 28/40  Iteration 7564/11040 Training loss: 1.2115 0.4617 sec/batch\n",
      "Epoch 28/40  Iteration 7565/11040 Training loss: 1.2114 0.4735 sec/batch\n",
      "Epoch 28/40  Iteration 7566/11040 Training loss: 1.2115 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7567/11040 Training loss: 1.2117 0.4706 sec/batch\n",
      "Epoch 28/40  Iteration 7568/11040 Training loss: 1.2120 0.4805 sec/batch\n",
      "Epoch 28/40  Iteration 7569/11040 Training loss: 1.2119 0.4733 sec/batch\n",
      "Epoch 28/40  Iteration 7570/11040 Training loss: 1.2121 0.4766 sec/batch\n",
      "Epoch 28/40  Iteration 7571/11040 Training loss: 1.2122 0.4897 sec/batch\n",
      "Epoch 28/40  Iteration 7572/11040 Training loss: 1.2123 0.4716 sec/batch\n",
      "Epoch 28/40  Iteration 7573/11040 Training loss: 1.2122 0.4758 sec/batch\n",
      "Epoch 28/40  Iteration 7574/11040 Training loss: 1.2123 0.4744 sec/batch\n",
      "Epoch 28/40  Iteration 7575/11040 Training loss: 1.2126 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7576/11040 Training loss: 1.2131 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7577/11040 Training loss: 1.2132 0.4757 sec/batch\n",
      "Epoch 28/40  Iteration 7578/11040 Training loss: 1.2134 0.4745 sec/batch\n",
      "Epoch 28/40  Iteration 7579/11040 Training loss: 1.2135 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7580/11040 Training loss: 1.2135 0.4746 sec/batch\n",
      "Epoch 28/40  Iteration 7581/11040 Training loss: 1.2133 0.4755 sec/batch\n",
      "Epoch 28/40  Iteration 7582/11040 Training loss: 1.2135 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7583/11040 Training loss: 1.2134 0.4725 sec/batch\n",
      "Epoch 28/40  Iteration 7584/11040 Training loss: 1.2132 0.4784 sec/batch\n",
      "Epoch 28/40  Iteration 7585/11040 Training loss: 1.2132 0.4744 sec/batch\n",
      "Epoch 28/40  Iteration 7586/11040 Training loss: 1.2131 0.4733 sec/batch\n",
      "Epoch 28/40  Iteration 7587/11040 Training loss: 1.2133 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7588/11040 Training loss: 1.2132 0.4773 sec/batch\n",
      "Epoch 28/40  Iteration 7589/11040 Training loss: 1.2130 0.4888 sec/batch\n",
      "Epoch 28/40  Iteration 7590/11040 Training loss: 1.2133 0.4858 sec/batch\n",
      "Epoch 28/40  Iteration 7591/11040 Training loss: 1.2135 0.4609 sec/batch\n",
      "Epoch 28/40  Iteration 7592/11040 Training loss: 1.2135 0.4737 sec/batch\n",
      "Epoch 28/40  Iteration 7593/11040 Training loss: 1.2136 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7594/11040 Training loss: 1.2138 0.4756 sec/batch\n",
      "Epoch 28/40  Iteration 7595/11040 Training loss: 1.2137 0.4763 sec/batch\n",
      "Epoch 28/40  Iteration 7596/11040 Training loss: 1.2136 0.4899 sec/batch\n",
      "Epoch 28/40  Iteration 7597/11040 Training loss: 1.2137 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7598/11040 Training loss: 1.2137 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7599/11040 Training loss: 1.2137 0.4757 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40  Iteration 7600/11040 Training loss: 1.2136 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7601/11040 Training loss: 1.2136 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7602/11040 Training loss: 1.2136 0.4768 sec/batch\n",
      "Epoch 28/40  Iteration 7603/11040 Training loss: 1.2136 0.4825 sec/batch\n",
      "Epoch 28/40  Iteration 7604/11040 Training loss: 1.2138 0.4823 sec/batch\n",
      "Epoch 28/40  Iteration 7605/11040 Training loss: 1.2139 0.4734 sec/batch\n",
      "Epoch 28/40  Iteration 7606/11040 Training loss: 1.2139 0.4600 sec/batch\n",
      "Epoch 28/40  Iteration 7607/11040 Training loss: 1.2140 0.4784 sec/batch\n",
      "Epoch 28/40  Iteration 7608/11040 Training loss: 1.2140 0.4704 sec/batch\n",
      "Epoch 28/40  Iteration 7609/11040 Training loss: 1.2139 0.4649 sec/batch\n",
      "Epoch 28/40  Iteration 7610/11040 Training loss: 1.2141 0.4703 sec/batch\n",
      "Epoch 28/40  Iteration 7611/11040 Training loss: 1.2143 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7612/11040 Training loss: 1.2143 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7613/11040 Training loss: 1.2144 0.4748 sec/batch\n",
      "Epoch 28/40  Iteration 7614/11040 Training loss: 1.2143 0.4749 sec/batch\n",
      "Epoch 28/40  Iteration 7615/11040 Training loss: 1.2144 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7616/11040 Training loss: 1.2146 0.4748 sec/batch\n",
      "Epoch 28/40  Iteration 7617/11040 Training loss: 1.2148 0.4757 sec/batch\n",
      "Epoch 28/40  Iteration 7618/11040 Training loss: 1.2151 0.4737 sec/batch\n",
      "Epoch 28/40  Iteration 7619/11040 Training loss: 1.2151 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7620/11040 Training loss: 1.2151 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7621/11040 Training loss: 1.2151 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7622/11040 Training loss: 1.2150 0.4599 sec/batch\n",
      "Epoch 28/40  Iteration 7623/11040 Training loss: 1.2154 0.4735 sec/batch\n",
      "Epoch 28/40  Iteration 7624/11040 Training loss: 1.2155 0.4756 sec/batch\n",
      "Epoch 28/40  Iteration 7625/11040 Training loss: 1.2155 0.4599 sec/batch\n",
      "Epoch 28/40  Iteration 7626/11040 Training loss: 1.2155 0.4744 sec/batch\n",
      "Epoch 28/40  Iteration 7627/11040 Training loss: 1.2156 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7628/11040 Training loss: 1.2157 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7629/11040 Training loss: 1.2158 0.4603 sec/batch\n",
      "Epoch 28/40  Iteration 7630/11040 Training loss: 1.2158 0.4726 sec/batch\n",
      "Epoch 28/40  Iteration 7631/11040 Training loss: 1.2160 0.4765 sec/batch\n",
      "Epoch 28/40  Iteration 7632/11040 Training loss: 1.2161 0.4634 sec/batch\n",
      "Epoch 28/40  Iteration 7633/11040 Training loss: 1.2161 0.4718 sec/batch\n",
      "Epoch 28/40  Iteration 7634/11040 Training loss: 1.2162 0.4727 sec/batch\n",
      "Epoch 28/40  Iteration 7635/11040 Training loss: 1.2160 0.4854 sec/batch\n",
      "Epoch 28/40  Iteration 7636/11040 Training loss: 1.2161 0.4843 sec/batch\n",
      "Epoch 28/40  Iteration 7637/11040 Training loss: 1.2162 0.4712 sec/batch\n",
      "Epoch 28/40  Iteration 7638/11040 Training loss: 1.2163 0.4755 sec/batch\n",
      "Epoch 28/40  Iteration 7639/11040 Training loss: 1.2165 0.4760 sec/batch\n",
      "Epoch 28/40  Iteration 7640/11040 Training loss: 1.2167 0.4904 sec/batch\n",
      "Epoch 28/40  Iteration 7641/11040 Training loss: 1.2167 0.4716 sec/batch\n",
      "Epoch 28/40  Iteration 7642/11040 Training loss: 1.2167 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7643/11040 Training loss: 1.2165 0.4603 sec/batch\n",
      "Epoch 28/40  Iteration 7644/11040 Training loss: 1.2165 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7645/11040 Training loss: 1.2165 0.4739 sec/batch\n",
      "Epoch 28/40  Iteration 7646/11040 Training loss: 1.2166 0.4763 sec/batch\n",
      "Epoch 28/40  Iteration 7647/11040 Training loss: 1.2167 0.4756 sec/batch\n",
      "Epoch 28/40  Iteration 7648/11040 Training loss: 1.2166 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7649/11040 Training loss: 1.2167 0.4756 sec/batch\n",
      "Epoch 28/40  Iteration 7650/11040 Training loss: 1.2167 0.4770 sec/batch\n",
      "Epoch 28/40  Iteration 7651/11040 Training loss: 1.2169 0.4896 sec/batch\n",
      "Epoch 28/40  Iteration 7652/11040 Training loss: 1.2170 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7653/11040 Training loss: 1.2172 0.4627 sec/batch\n",
      "Epoch 28/40  Iteration 7654/11040 Training loss: 1.2174 0.4732 sec/batch\n",
      "Epoch 28/40  Iteration 7655/11040 Training loss: 1.2175 0.4732 sec/batch\n",
      "Epoch 28/40  Iteration 7656/11040 Training loss: 1.2176 0.4761 sec/batch\n",
      "Epoch 28/40  Iteration 7657/11040 Training loss: 1.2176 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7658/11040 Training loss: 1.2177 0.4746 sec/batch\n",
      "Epoch 28/40  Iteration 7659/11040 Training loss: 1.2177 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7660/11040 Training loss: 1.2177 0.4747 sec/batch\n",
      "Epoch 28/40  Iteration 7661/11040 Training loss: 1.2176 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7662/11040 Training loss: 1.2175 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7663/11040 Training loss: 1.2175 0.4745 sec/batch\n",
      "Epoch 28/40  Iteration 7664/11040 Training loss: 1.2175 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7665/11040 Training loss: 1.2175 0.4745 sec/batch\n",
      "Epoch 28/40  Iteration 7666/11040 Training loss: 1.2175 0.4742 sec/batch\n",
      "Epoch 28/40  Iteration 7667/11040 Training loss: 1.2174 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7668/11040 Training loss: 1.2172 0.4758 sec/batch\n",
      "Epoch 28/40  Iteration 7669/11040 Training loss: 1.2171 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7670/11040 Training loss: 1.2170 0.4754 sec/batch\n",
      "Epoch 28/40  Iteration 7671/11040 Training loss: 1.2169 0.4745 sec/batch\n",
      "Epoch 28/40  Iteration 7672/11040 Training loss: 1.2169 0.4740 sec/batch\n",
      "Epoch 28/40  Iteration 7673/11040 Training loss: 1.2168 0.4765 sec/batch\n",
      "Epoch 28/40  Iteration 7674/11040 Training loss: 1.2166 0.4742 sec/batch\n",
      "Epoch 28/40  Iteration 7675/11040 Training loss: 1.2165 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7676/11040 Training loss: 1.2166 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7677/11040 Training loss: 1.2166 0.4758 sec/batch\n",
      "Epoch 28/40  Iteration 7678/11040 Training loss: 1.2166 0.4737 sec/batch\n",
      "Epoch 28/40  Iteration 7679/11040 Training loss: 1.2164 0.4749 sec/batch\n",
      "Epoch 28/40  Iteration 7680/11040 Training loss: 1.2164 0.4615 sec/batch\n",
      "Epoch 28/40  Iteration 7681/11040 Training loss: 1.2165 0.4742 sec/batch\n",
      "Epoch 28/40  Iteration 7682/11040 Training loss: 1.2166 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7683/11040 Training loss: 1.2166 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7684/11040 Training loss: 1.2166 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7685/11040 Training loss: 1.2166 0.4896 sec/batch\n",
      "Epoch 28/40  Iteration 7686/11040 Training loss: 1.2165 0.4759 sec/batch\n",
      "Epoch 28/40  Iteration 7687/11040 Training loss: 1.2166 0.4920 sec/batch\n",
      "Epoch 28/40  Iteration 7688/11040 Training loss: 1.2166 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7689/11040 Training loss: 1.2167 0.4753 sec/batch\n",
      "Epoch 28/40  Iteration 7690/11040 Training loss: 1.2167 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7691/11040 Training loss: 1.2169 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7692/11040 Training loss: 1.2169 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7693/11040 Training loss: 1.2170 0.4744 sec/batch\n",
      "Epoch 28/40  Iteration 7694/11040 Training loss: 1.2170 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7695/11040 Training loss: 1.2170 0.4740 sec/batch\n",
      "Epoch 28/40  Iteration 7696/11040 Training loss: 1.2170 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7697/11040 Training loss: 1.2170 0.4936 sec/batch\n",
      "Epoch 28/40  Iteration 7698/11040 Training loss: 1.2170 0.4733 sec/batch\n",
      "Epoch 28/40  Iteration 7699/11040 Training loss: 1.2170 0.4763 sec/batch\n",
      "Epoch 28/40  Iteration 7700/11040 Training loss: 1.2169 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7701/11040 Training loss: 1.2169 0.4748 sec/batch\n",
      "Epoch 28/40  Iteration 7702/11040 Training loss: 1.2170 0.4762 sec/batch\n",
      "Epoch 28/40  Iteration 7703/11040 Training loss: 1.2169 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7704/11040 Training loss: 1.2169 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7705/11040 Training loss: 1.2169 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7706/11040 Training loss: 1.2169 0.4773 sec/batch\n",
      "Epoch 28/40  Iteration 7707/11040 Training loss: 1.2169 0.4873 sec/batch\n",
      "Epoch 28/40  Iteration 7708/11040 Training loss: 1.2170 0.4772 sec/batch\n",
      "Epoch 28/40  Iteration 7709/11040 Training loss: 1.2171 0.4764 sec/batch\n",
      "Epoch 28/40  Iteration 7710/11040 Training loss: 1.2172 0.4743 sec/batch\n",
      "Epoch 28/40  Iteration 7711/11040 Training loss: 1.2172 0.4773 sec/batch\n",
      "Epoch 28/40  Iteration 7712/11040 Training loss: 1.2172 0.4743 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40  Iteration 7713/11040 Training loss: 1.2173 0.4752 sec/batch\n",
      "Epoch 28/40  Iteration 7714/11040 Training loss: 1.2175 0.4750 sec/batch\n",
      "Epoch 28/40  Iteration 7715/11040 Training loss: 1.2175 0.4904 sec/batch\n",
      "Epoch 28/40  Iteration 7716/11040 Training loss: 1.2176 0.4766 sec/batch\n",
      "Epoch 28/40  Iteration 7717/11040 Training loss: 1.2176 0.4746 sec/batch\n",
      "Epoch 28/40  Iteration 7718/11040 Training loss: 1.2177 0.4749 sec/batch\n",
      "Epoch 28/40  Iteration 7719/11040 Training loss: 1.2179 0.4770 sec/batch\n",
      "Epoch 28/40  Iteration 7720/11040 Training loss: 1.2180 0.4662 sec/batch\n",
      "Epoch 28/40  Iteration 7721/11040 Training loss: 1.2181 0.4699 sec/batch\n",
      "Epoch 28/40  Iteration 7722/11040 Training loss: 1.2182 0.4741 sec/batch\n",
      "Epoch 28/40  Iteration 7723/11040 Training loss: 1.2185 0.4761 sec/batch\n",
      "Epoch 28/40  Iteration 7724/11040 Training loss: 1.2188 0.4763 sec/batch\n",
      "Epoch 28/40  Iteration 7725/11040 Training loss: 1.2189 0.4740 sec/batch\n",
      "Epoch 28/40  Iteration 7726/11040 Training loss: 1.2190 0.4771 sec/batch\n",
      "Epoch 28/40  Iteration 7727/11040 Training loss: 1.2190 0.4737 sec/batch\n",
      "Epoch 28/40  Iteration 7728/11040 Training loss: 1.2191 0.4777 sec/batch\n",
      "Epoch 29/40  Iteration 7729/11040 Training loss: 1.2881 0.4734 sec/batch\n",
      "Epoch 29/40  Iteration 7730/11040 Training loss: 1.2553 0.4775 sec/batch\n",
      "Epoch 29/40  Iteration 7731/11040 Training loss: 1.2515 0.4899 sec/batch\n",
      "Epoch 29/40  Iteration 7732/11040 Training loss: 1.2500 0.4737 sec/batch\n",
      "Epoch 29/40  Iteration 7733/11040 Training loss: 1.2468 0.4758 sec/batch\n",
      "Epoch 29/40  Iteration 7734/11040 Training loss: 1.2437 0.4751 sec/batch\n",
      "Epoch 29/40  Iteration 7735/11040 Training loss: 1.2353 0.4767 sec/batch\n",
      "Epoch 29/40  Iteration 7736/11040 Training loss: 1.2334 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7737/11040 Training loss: 1.2286 0.4902 sec/batch\n",
      "Epoch 29/40  Iteration 7738/11040 Training loss: 1.2278 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7739/11040 Training loss: 1.2235 0.4786 sec/batch\n",
      "Epoch 29/40  Iteration 7740/11040 Training loss: 1.2226 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7741/11040 Training loss: 1.2191 0.4903 sec/batch\n",
      "Epoch 29/40  Iteration 7742/11040 Training loss: 1.2170 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7743/11040 Training loss: 1.2163 0.4761 sec/batch\n",
      "Epoch 29/40  Iteration 7744/11040 Training loss: 1.2162 0.4926 sec/batch\n",
      "Epoch 29/40  Iteration 7745/11040 Training loss: 1.2139 0.4807 sec/batch\n",
      "Epoch 29/40  Iteration 7746/11040 Training loss: 1.2126 0.4835 sec/batch\n",
      "Epoch 29/40  Iteration 7747/11040 Training loss: 1.2120 0.4897 sec/batch\n",
      "Epoch 29/40  Iteration 7748/11040 Training loss: 1.2121 0.4774 sec/batch\n",
      "Epoch 29/40  Iteration 7749/11040 Training loss: 1.2137 0.4919 sec/batch\n",
      "Epoch 29/40  Iteration 7750/11040 Training loss: 1.2144 0.4740 sec/batch\n",
      "Epoch 29/40  Iteration 7751/11040 Training loss: 1.2137 0.4592 sec/batch\n",
      "Epoch 29/40  Iteration 7752/11040 Training loss: 1.2139 0.4742 sec/batch\n",
      "Epoch 29/40  Iteration 7753/11040 Training loss: 1.2127 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7754/11040 Training loss: 1.2122 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7755/11040 Training loss: 1.2118 0.4748 sec/batch\n",
      "Epoch 29/40  Iteration 7756/11040 Training loss: 1.2119 0.4738 sec/batch\n",
      "Epoch 29/40  Iteration 7757/11040 Training loss: 1.2122 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7758/11040 Training loss: 1.2124 0.4769 sec/batch\n",
      "Epoch 29/40  Iteration 7759/11040 Training loss: 1.2122 0.4725 sec/batch\n",
      "Epoch 29/40  Iteration 7760/11040 Training loss: 1.2116 0.4601 sec/batch\n",
      "Epoch 29/40  Iteration 7761/11040 Training loss: 1.2107 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7762/11040 Training loss: 1.2106 0.4910 sec/batch\n",
      "Epoch 29/40  Iteration 7763/11040 Training loss: 1.2108 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7764/11040 Training loss: 1.2113 0.4619 sec/batch\n",
      "Epoch 29/40  Iteration 7765/11040 Training loss: 1.2112 0.4734 sec/batch\n",
      "Epoch 29/40  Iteration 7766/11040 Training loss: 1.2106 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7767/11040 Training loss: 1.2096 0.4586 sec/batch\n",
      "Epoch 29/40  Iteration 7768/11040 Training loss: 1.2098 0.4752 sec/batch\n",
      "Epoch 29/40  Iteration 7769/11040 Training loss: 1.2096 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7770/11040 Training loss: 1.2086 0.4740 sec/batch\n",
      "Epoch 29/40  Iteration 7771/11040 Training loss: 1.2088 0.4610 sec/batch\n",
      "Epoch 29/40  Iteration 7772/11040 Training loss: 1.2081 0.4769 sec/batch\n",
      "Epoch 29/40  Iteration 7773/11040 Training loss: 1.2074 0.4718 sec/batch\n",
      "Epoch 29/40  Iteration 7774/11040 Training loss: 1.2069 0.4662 sec/batch\n",
      "Epoch 29/40  Iteration 7775/11040 Training loss: 1.2062 0.4687 sec/batch\n",
      "Epoch 29/40  Iteration 7776/11040 Training loss: 1.2062 0.4691 sec/batch\n",
      "Epoch 29/40  Iteration 7777/11040 Training loss: 1.2058 0.4806 sec/batch\n",
      "Epoch 29/40  Iteration 7778/11040 Training loss: 1.2064 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7779/11040 Training loss: 1.2061 0.4752 sec/batch\n",
      "Epoch 29/40  Iteration 7780/11040 Training loss: 1.2060 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7781/11040 Training loss: 1.2063 0.4603 sec/batch\n",
      "Epoch 29/40  Iteration 7782/11040 Training loss: 1.2061 0.4742 sec/batch\n",
      "Epoch 29/40  Iteration 7783/11040 Training loss: 1.2060 0.4614 sec/batch\n",
      "Epoch 29/40  Iteration 7784/11040 Training loss: 1.2058 0.4734 sec/batch\n",
      "Epoch 29/40  Iteration 7785/11040 Training loss: 1.2057 0.4742 sec/batch\n",
      "Epoch 29/40  Iteration 7786/11040 Training loss: 1.2055 0.4762 sec/batch\n",
      "Epoch 29/40  Iteration 7787/11040 Training loss: 1.2057 0.4756 sec/batch\n",
      "Epoch 29/40  Iteration 7788/11040 Training loss: 1.2053 0.4731 sec/batch\n",
      "Epoch 29/40  Iteration 7789/11040 Training loss: 1.2053 0.4776 sec/batch\n",
      "Epoch 29/40  Iteration 7790/11040 Training loss: 1.2052 0.4724 sec/batch\n",
      "Epoch 29/40  Iteration 7791/11040 Training loss: 1.2054 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7792/11040 Training loss: 1.2055 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7793/11040 Training loss: 1.2058 0.4722 sec/batch\n",
      "Epoch 29/40  Iteration 7794/11040 Training loss: 1.2056 0.4794 sec/batch\n",
      "Epoch 29/40  Iteration 7795/11040 Training loss: 1.2056 0.4709 sec/batch\n",
      "Epoch 29/40  Iteration 7796/11040 Training loss: 1.2057 0.4770 sec/batch\n",
      "Epoch 29/40  Iteration 7797/11040 Training loss: 1.2056 0.4749 sec/batch\n",
      "Epoch 29/40  Iteration 7798/11040 Training loss: 1.2055 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7799/11040 Training loss: 1.2051 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7800/11040 Training loss: 1.2046 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7801/11040 Training loss: 1.2042 0.4761 sec/batch\n",
      "Epoch 29/40  Iteration 7802/11040 Training loss: 1.2038 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7803/11040 Training loss: 1.2036 0.4752 sec/batch\n",
      "Epoch 29/40  Iteration 7804/11040 Training loss: 1.2040 0.4740 sec/batch\n",
      "Epoch 29/40  Iteration 7805/11040 Training loss: 1.2034 0.4773 sec/batch\n",
      "Epoch 29/40  Iteration 7806/11040 Training loss: 1.2031 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7807/11040 Training loss: 1.2030 0.4889 sec/batch\n",
      "Epoch 29/40  Iteration 7808/11040 Training loss: 1.2029 0.4753 sec/batch\n",
      "Epoch 29/40  Iteration 7809/11040 Training loss: 1.2026 0.4768 sec/batch\n",
      "Epoch 29/40  Iteration 7810/11040 Training loss: 1.2028 0.4734 sec/batch\n",
      "Epoch 29/40  Iteration 7811/11040 Training loss: 1.2028 0.4763 sec/batch\n",
      "Epoch 29/40  Iteration 7812/11040 Training loss: 1.2026 0.4736 sec/batch\n",
      "Epoch 29/40  Iteration 7813/11040 Training loss: 1.2025 0.4766 sec/batch\n",
      "Epoch 29/40  Iteration 7814/11040 Training loss: 1.2024 0.4634 sec/batch\n",
      "Epoch 29/40  Iteration 7815/11040 Training loss: 1.2018 0.4632 sec/batch\n",
      "Epoch 29/40  Iteration 7816/11040 Training loss: 1.2008 0.4788 sec/batch\n",
      "Epoch 29/40  Iteration 7817/11040 Training loss: 1.2008 0.4808 sec/batch\n",
      "Epoch 29/40  Iteration 7818/11040 Training loss: 1.2010 0.4811 sec/batch\n",
      "Epoch 29/40  Iteration 7819/11040 Training loss: 1.2011 0.4845 sec/batch\n",
      "Epoch 29/40  Iteration 7820/11040 Training loss: 1.2010 0.4736 sec/batch\n",
      "Epoch 29/40  Iteration 7821/11040 Training loss: 1.2007 0.4762 sec/batch\n",
      "Epoch 29/40  Iteration 7822/11040 Training loss: 1.2011 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7823/11040 Training loss: 1.2016 0.4761 sec/batch\n",
      "Epoch 29/40  Iteration 7824/11040 Training loss: 1.2019 0.4749 sec/batch\n",
      "Epoch 29/40  Iteration 7825/11040 Training loss: 1.2023 0.4756 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40  Iteration 7826/11040 Training loss: 1.2027 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7827/11040 Training loss: 1.2030 0.4748 sec/batch\n",
      "Epoch 29/40  Iteration 7828/11040 Training loss: 1.2029 0.4764 sec/batch\n",
      "Epoch 29/40  Iteration 7829/11040 Training loss: 1.2031 0.4751 sec/batch\n",
      "Epoch 29/40  Iteration 7830/11040 Training loss: 1.2033 0.4737 sec/batch\n",
      "Epoch 29/40  Iteration 7831/11040 Training loss: 1.2029 0.4771 sec/batch\n",
      "Epoch 29/40  Iteration 7832/11040 Training loss: 1.2031 0.4732 sec/batch\n",
      "Epoch 29/40  Iteration 7833/11040 Training loss: 1.2029 0.4646 sec/batch\n",
      "Epoch 29/40  Iteration 7834/11040 Training loss: 1.2028 0.4700 sec/batch\n",
      "Epoch 29/40  Iteration 7835/11040 Training loss: 1.2026 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7836/11040 Training loss: 1.2029 0.4930 sec/batch\n",
      "Epoch 29/40  Iteration 7837/11040 Training loss: 1.2033 0.4902 sec/batch\n",
      "Epoch 29/40  Iteration 7838/11040 Training loss: 1.2033 0.4747 sec/batch\n",
      "Epoch 29/40  Iteration 7839/11040 Training loss: 1.2033 0.4821 sec/batch\n",
      "Epoch 29/40  Iteration 7840/11040 Training loss: 1.2036 0.4866 sec/batch\n",
      "Epoch 29/40  Iteration 7841/11040 Training loss: 1.2036 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7842/11040 Training loss: 1.2036 0.4747 sec/batch\n",
      "Epoch 29/40  Iteration 7843/11040 Training loss: 1.2038 0.4637 sec/batch\n",
      "Epoch 29/40  Iteration 7844/11040 Training loss: 1.2041 0.4706 sec/batch\n",
      "Epoch 29/40  Iteration 7845/11040 Training loss: 1.2039 0.4751 sec/batch\n",
      "Epoch 29/40  Iteration 7846/11040 Training loss: 1.2042 0.4905 sec/batch\n",
      "Epoch 29/40  Iteration 7847/11040 Training loss: 1.2043 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7848/11040 Training loss: 1.2043 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7849/11040 Training loss: 1.2043 0.4764 sec/batch\n",
      "Epoch 29/40  Iteration 7850/11040 Training loss: 1.2044 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7851/11040 Training loss: 1.2047 0.4763 sec/batch\n",
      "Epoch 29/40  Iteration 7852/11040 Training loss: 1.2052 0.4735 sec/batch\n",
      "Epoch 29/40  Iteration 7853/11040 Training loss: 1.2053 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7854/11040 Training loss: 1.2055 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7855/11040 Training loss: 1.2056 0.4592 sec/batch\n",
      "Epoch 29/40  Iteration 7856/11040 Training loss: 1.2056 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7857/11040 Training loss: 1.2054 0.4763 sec/batch\n",
      "Epoch 29/40  Iteration 7858/11040 Training loss: 1.2057 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7859/11040 Training loss: 1.2057 0.4750 sec/batch\n",
      "Epoch 29/40  Iteration 7860/11040 Training loss: 1.2055 0.4740 sec/batch\n",
      "Epoch 29/40  Iteration 7861/11040 Training loss: 1.2055 0.4766 sec/batch\n",
      "Epoch 29/40  Iteration 7862/11040 Training loss: 1.2054 0.4753 sec/batch\n",
      "Epoch 29/40  Iteration 7863/11040 Training loss: 1.2055 0.4732 sec/batch\n",
      "Epoch 29/40  Iteration 7864/11040 Training loss: 1.2056 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7865/11040 Training loss: 1.2055 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7866/11040 Training loss: 1.2057 0.4751 sec/batch\n",
      "Epoch 29/40  Iteration 7867/11040 Training loss: 1.2059 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7868/11040 Training loss: 1.2060 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7869/11040 Training loss: 1.2061 0.4742 sec/batch\n",
      "Epoch 29/40  Iteration 7870/11040 Training loss: 1.2062 0.4596 sec/batch\n",
      "Epoch 29/40  Iteration 7871/11040 Training loss: 1.2062 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7872/11040 Training loss: 1.2062 0.4742 sec/batch\n",
      "Epoch 29/40  Iteration 7873/11040 Training loss: 1.2062 0.4773 sec/batch\n",
      "Epoch 29/40  Iteration 7874/11040 Training loss: 1.2062 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7875/11040 Training loss: 1.2063 0.4742 sec/batch\n",
      "Epoch 29/40  Iteration 7876/11040 Training loss: 1.2062 0.4749 sec/batch\n",
      "Epoch 29/40  Iteration 7877/11040 Training loss: 1.2062 0.4758 sec/batch\n",
      "Epoch 29/40  Iteration 7878/11040 Training loss: 1.2062 0.4757 sec/batch\n",
      "Epoch 29/40  Iteration 7879/11040 Training loss: 1.2062 0.4735 sec/batch\n",
      "Epoch 29/40  Iteration 7880/11040 Training loss: 1.2065 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7881/11040 Training loss: 1.2065 0.4752 sec/batch\n",
      "Epoch 29/40  Iteration 7882/11040 Training loss: 1.2067 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7883/11040 Training loss: 1.2068 0.4760 sec/batch\n",
      "Epoch 29/40  Iteration 7884/11040 Training loss: 1.2067 0.4773 sec/batch\n",
      "Epoch 29/40  Iteration 7885/11040 Training loss: 1.2067 0.4715 sec/batch\n",
      "Epoch 29/40  Iteration 7886/11040 Training loss: 1.2068 0.4766 sec/batch\n",
      "Epoch 29/40  Iteration 7887/11040 Training loss: 1.2070 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7888/11040 Training loss: 1.2070 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7889/11040 Training loss: 1.2072 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7890/11040 Training loss: 1.2072 0.4765 sec/batch\n",
      "Epoch 29/40  Iteration 7891/11040 Training loss: 1.2073 0.4734 sec/batch\n",
      "Epoch 29/40  Iteration 7892/11040 Training loss: 1.2075 0.4767 sec/batch\n",
      "Epoch 29/40  Iteration 7893/11040 Training loss: 1.2076 0.4912 sec/batch\n",
      "Epoch 29/40  Iteration 7894/11040 Training loss: 1.2079 0.4735 sec/batch\n",
      "Epoch 29/40  Iteration 7895/11040 Training loss: 1.2079 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7896/11040 Training loss: 1.2079 0.4615 sec/batch\n",
      "Epoch 29/40  Iteration 7897/11040 Training loss: 1.2079 0.4736 sec/batch\n",
      "Epoch 29/40  Iteration 7898/11040 Training loss: 1.2079 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7899/11040 Training loss: 1.2082 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7900/11040 Training loss: 1.2083 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7901/11040 Training loss: 1.2083 0.4769 sec/batch\n",
      "Epoch 29/40  Iteration 7902/11040 Training loss: 1.2083 0.4733 sec/batch\n",
      "Epoch 29/40  Iteration 7903/11040 Training loss: 1.2083 0.4753 sec/batch\n",
      "Epoch 29/40  Iteration 7904/11040 Training loss: 1.2083 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7905/11040 Training loss: 1.2084 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7906/11040 Training loss: 1.2084 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7907/11040 Training loss: 1.2087 0.4733 sec/batch\n",
      "Epoch 29/40  Iteration 7908/11040 Training loss: 1.2087 0.4787 sec/batch\n",
      "Epoch 29/40  Iteration 7909/11040 Training loss: 1.2088 0.4720 sec/batch\n",
      "Epoch 29/40  Iteration 7910/11040 Training loss: 1.2089 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7911/11040 Training loss: 1.2088 0.4753 sec/batch\n",
      "Epoch 29/40  Iteration 7912/11040 Training loss: 1.2089 0.4763 sec/batch\n",
      "Epoch 29/40  Iteration 7913/11040 Training loss: 1.2090 0.4758 sec/batch\n",
      "Epoch 29/40  Iteration 7914/11040 Training loss: 1.2091 0.4736 sec/batch\n",
      "Epoch 29/40  Iteration 7915/11040 Training loss: 1.2093 0.4599 sec/batch\n",
      "Epoch 29/40  Iteration 7916/11040 Training loss: 1.2093 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7917/11040 Training loss: 1.2094 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7918/11040 Training loss: 1.2094 0.4619 sec/batch\n",
      "Epoch 29/40  Iteration 7919/11040 Training loss: 1.2093 0.4735 sec/batch\n",
      "Epoch 29/40  Iteration 7920/11040 Training loss: 1.2093 0.4756 sec/batch\n",
      "Epoch 29/40  Iteration 7921/11040 Training loss: 1.2094 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7922/11040 Training loss: 1.2094 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7923/11040 Training loss: 1.2097 0.4736 sec/batch\n",
      "Epoch 29/40  Iteration 7924/11040 Training loss: 1.2096 0.4770 sec/batch\n",
      "Epoch 29/40  Iteration 7925/11040 Training loss: 1.2097 0.4743 sec/batch\n",
      "Epoch 29/40  Iteration 7926/11040 Training loss: 1.2098 0.4755 sec/batch\n",
      "Epoch 29/40  Iteration 7927/11040 Training loss: 1.2099 0.4751 sec/batch\n",
      "Epoch 29/40  Iteration 7928/11040 Training loss: 1.2099 0.4744 sec/batch\n",
      "Epoch 29/40  Iteration 7929/11040 Training loss: 1.2101 0.4749 sec/batch\n",
      "Epoch 29/40  Iteration 7930/11040 Training loss: 1.2103 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7931/11040 Training loss: 1.2104 0.4771 sec/batch\n",
      "Epoch 29/40  Iteration 7932/11040 Training loss: 1.2105 0.4748 sec/batch\n",
      "Epoch 29/40  Iteration 7933/11040 Training loss: 1.2104 0.4761 sec/batch\n",
      "Epoch 29/40  Iteration 7934/11040 Training loss: 1.2105 0.4613 sec/batch\n",
      "Epoch 29/40  Iteration 7935/11040 Training loss: 1.2106 0.4738 sec/batch\n",
      "Epoch 29/40  Iteration 7936/11040 Training loss: 1.2106 0.4739 sec/batch\n",
      "Epoch 29/40  Iteration 7937/11040 Training loss: 1.2104 0.4768 sec/batch\n",
      "Epoch 29/40  Iteration 7938/11040 Training loss: 1.2104 0.4751 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40  Iteration 7939/11040 Training loss: 1.2105 0.4901 sec/batch\n",
      "Epoch 29/40  Iteration 7940/11040 Training loss: 1.2105 0.4735 sec/batch\n",
      "Epoch 29/40  Iteration 7941/11040 Training loss: 1.2105 0.4919 sec/batch\n",
      "Epoch 29/40  Iteration 7942/11040 Training loss: 1.2104 0.4794 sec/batch\n",
      "Epoch 29/40  Iteration 7943/11040 Training loss: 1.2103 0.4707 sec/batch\n",
      "Epoch 29/40  Iteration 7944/11040 Training loss: 1.2102 0.4756 sec/batch\n",
      "Epoch 29/40  Iteration 7945/11040 Training loss: 1.2101 0.4761 sec/batch\n",
      "Epoch 29/40  Iteration 7946/11040 Training loss: 1.2100 0.4737 sec/batch\n",
      "Epoch 29/40  Iteration 7947/11040 Training loss: 1.2099 0.4767 sec/batch\n",
      "Epoch 29/40  Iteration 7948/11040 Training loss: 1.2098 0.4918 sec/batch\n",
      "Epoch 29/40  Iteration 7949/11040 Training loss: 1.2097 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7950/11040 Training loss: 1.2095 0.4687 sec/batch\n",
      "Epoch 29/40  Iteration 7951/11040 Training loss: 1.2095 0.4718 sec/batch\n",
      "Epoch 29/40  Iteration 7952/11040 Training loss: 1.2095 0.5016 sec/batch\n",
      "Epoch 29/40  Iteration 7953/11040 Training loss: 1.2095 0.4749 sec/batch\n",
      "Epoch 29/40  Iteration 7954/11040 Training loss: 1.2095 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7955/11040 Training loss: 1.2093 0.4745 sec/batch\n",
      "Epoch 29/40  Iteration 7956/11040 Training loss: 1.2092 0.4762 sec/batch\n",
      "Epoch 29/40  Iteration 7957/11040 Training loss: 1.2093 0.4756 sec/batch\n",
      "Epoch 29/40  Iteration 7958/11040 Training loss: 1.2094 0.4758 sec/batch\n",
      "Epoch 29/40  Iteration 7959/11040 Training loss: 1.2094 0.4917 sec/batch\n",
      "Epoch 29/40  Iteration 7960/11040 Training loss: 1.2095 0.4749 sec/batch\n",
      "Epoch 29/40  Iteration 7961/11040 Training loss: 1.2094 0.4753 sec/batch\n",
      "Epoch 29/40  Iteration 7962/11040 Training loss: 1.2093 0.4767 sec/batch\n",
      "Epoch 29/40  Iteration 7963/11040 Training loss: 1.2095 0.4901 sec/batch\n",
      "Epoch 29/40  Iteration 7964/11040 Training loss: 1.2095 0.4754 sec/batch\n",
      "Epoch 29/40  Iteration 7965/11040 Training loss: 1.2096 0.4777 sec/batch\n",
      "Epoch 29/40  Iteration 7966/11040 Training loss: 1.2096 0.4909 sec/batch\n",
      "Epoch 29/40  Iteration 7967/11040 Training loss: 1.2098 0.4910 sec/batch\n",
      "Epoch 29/40  Iteration 7968/11040 Training loss: 1.2098 0.4752 sec/batch\n",
      "Epoch 29/40  Iteration 7969/11040 Training loss: 1.2099 0.4776 sec/batch\n",
      "Epoch 29/40  Iteration 7970/11040 Training loss: 1.2099 0.4739 sec/batch\n",
      "Epoch 29/40  Iteration 7971/11040 Training loss: 1.2099 0.4795 sec/batch\n",
      "Epoch 29/40  Iteration 7972/11040 Training loss: 1.2099 0.4882 sec/batch\n",
      "Epoch 29/40  Iteration 7973/11040 Training loss: 1.2099 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7974/11040 Training loss: 1.2099 0.4769 sec/batch\n",
      "Epoch 29/40  Iteration 7975/11040 Training loss: 1.2098 0.4906 sec/batch\n",
      "Epoch 29/40  Iteration 7976/11040 Training loss: 1.2098 0.4904 sec/batch\n",
      "Epoch 29/40  Iteration 7977/11040 Training loss: 1.2098 0.4788 sec/batch\n",
      "Epoch 29/40  Iteration 7978/11040 Training loss: 1.2099 0.4738 sec/batch\n",
      "Epoch 29/40  Iteration 7979/11040 Training loss: 1.2098 0.4769 sec/batch\n",
      "Epoch 29/40  Iteration 7980/11040 Training loss: 1.2098 0.4750 sec/batch\n",
      "Epoch 29/40  Iteration 7981/11040 Training loss: 1.2098 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7982/11040 Training loss: 1.2098 0.4760 sec/batch\n",
      "Epoch 29/40  Iteration 7983/11040 Training loss: 1.2099 0.4748 sec/batch\n",
      "Epoch 29/40  Iteration 7984/11040 Training loss: 1.2099 0.4758 sec/batch\n",
      "Epoch 29/40  Iteration 7985/11040 Training loss: 1.2100 0.4760 sec/batch\n",
      "Epoch 29/40  Iteration 7986/11040 Training loss: 1.2100 0.4917 sec/batch\n",
      "Epoch 29/40  Iteration 7987/11040 Training loss: 1.2100 0.4740 sec/batch\n",
      "Epoch 29/40  Iteration 7988/11040 Training loss: 1.2101 0.4772 sec/batch\n",
      "Epoch 29/40  Iteration 7989/11040 Training loss: 1.2102 0.4914 sec/batch\n",
      "Epoch 29/40  Iteration 7990/11040 Training loss: 1.2103 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7991/11040 Training loss: 1.2104 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 7992/11040 Training loss: 1.2105 0.4747 sec/batch\n",
      "Epoch 29/40  Iteration 7993/11040 Training loss: 1.2105 0.4901 sec/batch\n",
      "Epoch 29/40  Iteration 7994/11040 Training loss: 1.2106 0.4786 sec/batch\n",
      "Epoch 29/40  Iteration 7995/11040 Training loss: 1.2108 0.4732 sec/batch\n",
      "Epoch 29/40  Iteration 7996/11040 Training loss: 1.2109 0.4759 sec/batch\n",
      "Epoch 29/40  Iteration 7997/11040 Training loss: 1.2110 0.4763 sec/batch\n",
      "Epoch 29/40  Iteration 7998/11040 Training loss: 1.2111 0.4927 sec/batch\n",
      "Epoch 29/40  Iteration 7999/11040 Training loss: 1.2115 0.4905 sec/batch\n",
      "Epoch 29/40  Iteration 8000/11040 Training loss: 1.2117 0.4770 sec/batch\n",
      "Validation loss: 1.24744 Saving checkpoint!\n",
      "Epoch 29/40  Iteration 8001/11040 Training loss: 1.2123 0.4728 sec/batch\n",
      "Epoch 29/40  Iteration 8002/11040 Training loss: 1.2124 0.4760 sec/batch\n",
      "Epoch 29/40  Iteration 8003/11040 Training loss: 1.2125 0.4746 sec/batch\n",
      "Epoch 29/40  Iteration 8004/11040 Training loss: 1.2126 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8005/11040 Training loss: 1.2972 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8006/11040 Training loss: 1.2583 0.4605 sec/batch\n",
      "Epoch 30/40  Iteration 8007/11040 Training loss: 1.2511 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8008/11040 Training loss: 1.2483 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8009/11040 Training loss: 1.2464 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8010/11040 Training loss: 1.2434 0.4620 sec/batch\n",
      "Epoch 30/40  Iteration 8011/11040 Training loss: 1.2342 0.4730 sec/batch\n",
      "Epoch 30/40  Iteration 8012/11040 Training loss: 1.2308 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8013/11040 Training loss: 1.2266 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8014/11040 Training loss: 1.2254 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8015/11040 Training loss: 1.2214 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8016/11040 Training loss: 1.2193 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8017/11040 Training loss: 1.2161 0.4608 sec/batch\n",
      "Epoch 30/40  Iteration 8018/11040 Training loss: 1.2133 0.4733 sec/batch\n",
      "Epoch 30/40  Iteration 8019/11040 Training loss: 1.2119 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8020/11040 Training loss: 1.2119 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8021/11040 Training loss: 1.2089 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8022/11040 Training loss: 1.2074 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8023/11040 Training loss: 1.2065 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8024/11040 Training loss: 1.2064 0.4607 sec/batch\n",
      "Epoch 30/40  Iteration 8025/11040 Training loss: 1.2078 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8026/11040 Training loss: 1.2086 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8027/11040 Training loss: 1.2084 0.4617 sec/batch\n",
      "Epoch 30/40  Iteration 8028/11040 Training loss: 1.2085 0.4735 sec/batch\n",
      "Epoch 30/40  Iteration 8029/11040 Training loss: 1.2078 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8030/11040 Training loss: 1.2070 0.4750 sec/batch\n",
      "Epoch 30/40  Iteration 8031/11040 Training loss: 1.2065 0.4585 sec/batch\n",
      "Epoch 30/40  Iteration 8032/11040 Training loss: 1.2063 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8033/11040 Training loss: 1.2065 0.4758 sec/batch\n",
      "Epoch 30/40  Iteration 8034/11040 Training loss: 1.2066 0.4767 sec/batch\n",
      "Epoch 30/40  Iteration 8035/11040 Training loss: 1.2064 0.4739 sec/batch\n",
      "Epoch 30/40  Iteration 8036/11040 Training loss: 1.2058 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8037/11040 Training loss: 1.2053 0.4735 sec/batch\n",
      "Epoch 30/40  Iteration 8038/11040 Training loss: 1.2049 0.4617 sec/batch\n",
      "Epoch 30/40  Iteration 8039/11040 Training loss: 1.2050 0.4733 sec/batch\n",
      "Epoch 30/40  Iteration 8040/11040 Training loss: 1.2056 0.4752 sec/batch\n",
      "Epoch 30/40  Iteration 8041/11040 Training loss: 1.2056 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8042/11040 Training loss: 1.2048 0.4632 sec/batch\n",
      "Epoch 30/40  Iteration 8043/11040 Training loss: 1.2042 0.4679 sec/batch\n",
      "Epoch 30/40  Iteration 8044/11040 Training loss: 1.2042 0.4825 sec/batch\n",
      "Epoch 30/40  Iteration 8045/11040 Training loss: 1.2037 0.4707 sec/batch\n",
      "Epoch 30/40  Iteration 8046/11040 Training loss: 1.2027 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8047/11040 Training loss: 1.2030 0.4736 sec/batch\n",
      "Epoch 30/40  Iteration 8048/11040 Training loss: 1.2024 0.4697 sec/batch\n",
      "Epoch 30/40  Iteration 8049/11040 Training loss: 1.2020 0.4836 sec/batch\n",
      "Epoch 30/40  Iteration 8050/11040 Training loss: 1.2015 0.4738 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40  Iteration 8051/11040 Training loss: 1.2008 0.4740 sec/batch\n",
      "Epoch 30/40  Iteration 8052/11040 Training loss: 1.2004 0.4757 sec/batch\n",
      "Epoch 30/40  Iteration 8053/11040 Training loss: 1.2001 0.4737 sec/batch\n",
      "Epoch 30/40  Iteration 8054/11040 Training loss: 1.2005 0.4791 sec/batch\n",
      "Epoch 30/40  Iteration 8055/11040 Training loss: 1.2003 0.4862 sec/batch\n",
      "Epoch 30/40  Iteration 8056/11040 Training loss: 1.2001 0.4765 sec/batch\n",
      "Epoch 30/40  Iteration 8057/11040 Training loss: 1.2006 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8058/11040 Training loss: 1.2004 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8059/11040 Training loss: 1.2004 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8060/11040 Training loss: 1.2005 0.4742 sec/batch\n",
      "Epoch 30/40  Iteration 8061/11040 Training loss: 1.2003 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8062/11040 Training loss: 1.2002 0.4758 sec/batch\n",
      "Epoch 30/40  Iteration 8063/11040 Training loss: 1.2004 0.4755 sec/batch\n",
      "Epoch 30/40  Iteration 8064/11040 Training loss: 1.2000 0.4587 sec/batch\n",
      "Epoch 30/40  Iteration 8065/11040 Training loss: 1.1999 0.4764 sec/batch\n",
      "Epoch 30/40  Iteration 8066/11040 Training loss: 1.1998 0.4888 sec/batch\n",
      "Epoch 30/40  Iteration 8067/11040 Training loss: 1.2000 0.4803 sec/batch\n",
      "Epoch 30/40  Iteration 8068/11040 Training loss: 1.2001 0.4727 sec/batch\n",
      "Epoch 30/40  Iteration 8069/11040 Training loss: 1.2004 0.4725 sec/batch\n",
      "Epoch 30/40  Iteration 8070/11040 Training loss: 1.2002 0.4607 sec/batch\n",
      "Epoch 30/40  Iteration 8071/11040 Training loss: 1.2001 0.4732 sec/batch\n",
      "Epoch 30/40  Iteration 8072/11040 Training loss: 1.2001 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8073/11040 Training loss: 1.1999 0.4606 sec/batch\n",
      "Epoch 30/40  Iteration 8074/11040 Training loss: 1.1998 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8075/11040 Training loss: 1.1996 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8076/11040 Training loss: 1.1991 0.4598 sec/batch\n",
      "Epoch 30/40  Iteration 8077/11040 Training loss: 1.1986 0.4762 sec/batch\n",
      "Epoch 30/40  Iteration 8078/11040 Training loss: 1.1982 0.4730 sec/batch\n",
      "Epoch 30/40  Iteration 8079/11040 Training loss: 1.1981 0.4760 sec/batch\n",
      "Epoch 30/40  Iteration 8080/11040 Training loss: 1.1984 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8081/11040 Training loss: 1.1979 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8082/11040 Training loss: 1.1976 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8083/11040 Training loss: 1.1974 0.4596 sec/batch\n",
      "Epoch 30/40  Iteration 8084/11040 Training loss: 1.1975 0.4755 sec/batch\n",
      "Epoch 30/40  Iteration 8085/11040 Training loss: 1.1972 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8086/11040 Training loss: 1.1973 0.4750 sec/batch\n",
      "Epoch 30/40  Iteration 8087/11040 Training loss: 1.1974 0.4747 sec/batch\n",
      "Epoch 30/40  Iteration 8088/11040 Training loss: 1.1972 0.4766 sec/batch\n",
      "Epoch 30/40  Iteration 8089/11040 Training loss: 1.1970 0.4719 sec/batch\n",
      "Epoch 30/40  Iteration 8090/11040 Training loss: 1.1970 0.4764 sec/batch\n",
      "Epoch 30/40  Iteration 8091/11040 Training loss: 1.1965 0.4741 sec/batch\n",
      "Epoch 30/40  Iteration 8092/11040 Training loss: 1.1956 0.4797 sec/batch\n",
      "Epoch 30/40  Iteration 8093/11040 Training loss: 1.1955 0.4706 sec/batch\n",
      "Epoch 30/40  Iteration 8094/11040 Training loss: 1.1956 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8095/11040 Training loss: 1.1957 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8096/11040 Training loss: 1.1956 0.4607 sec/batch\n",
      "Epoch 30/40  Iteration 8097/11040 Training loss: 1.1954 0.4707 sec/batch\n",
      "Epoch 30/40  Iteration 8098/11040 Training loss: 1.1959 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8099/11040 Training loss: 1.1964 0.4603 sec/batch\n",
      "Epoch 30/40  Iteration 8100/11040 Training loss: 1.1967 0.4735 sec/batch\n",
      "Epoch 30/40  Iteration 8101/11040 Training loss: 1.1972 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8102/11040 Training loss: 1.1977 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8103/11040 Training loss: 1.1979 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8104/11040 Training loss: 1.1977 0.4755 sec/batch\n",
      "Epoch 30/40  Iteration 8105/11040 Training loss: 1.1979 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8106/11040 Training loss: 1.1981 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8107/11040 Training loss: 1.1978 0.4732 sec/batch\n",
      "Epoch 30/40  Iteration 8108/11040 Training loss: 1.1980 0.4775 sec/batch\n",
      "Epoch 30/40  Iteration 8109/11040 Training loss: 1.1979 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8110/11040 Training loss: 1.1978 0.4727 sec/batch\n",
      "Epoch 30/40  Iteration 8111/11040 Training loss: 1.1977 0.4742 sec/batch\n",
      "Epoch 30/40  Iteration 8112/11040 Training loss: 1.1978 0.4777 sec/batch\n",
      "Epoch 30/40  Iteration 8113/11040 Training loss: 1.1981 0.4755 sec/batch\n",
      "Epoch 30/40  Iteration 8114/11040 Training loss: 1.1982 0.4733 sec/batch\n",
      "Epoch 30/40  Iteration 8115/11040 Training loss: 1.1983 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8116/11040 Training loss: 1.1984 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8117/11040 Training loss: 1.1985 0.4747 sec/batch\n",
      "Epoch 30/40  Iteration 8118/11040 Training loss: 1.1985 0.4757 sec/batch\n",
      "Epoch 30/40  Iteration 8119/11040 Training loss: 1.1988 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8120/11040 Training loss: 1.1991 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8121/11040 Training loss: 1.1989 0.4763 sec/batch\n",
      "Epoch 30/40  Iteration 8122/11040 Training loss: 1.1991 0.4737 sec/batch\n",
      "Epoch 30/40  Iteration 8123/11040 Training loss: 1.1992 0.4755 sec/batch\n",
      "Epoch 30/40  Iteration 8124/11040 Training loss: 1.1992 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8125/11040 Training loss: 1.1992 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8126/11040 Training loss: 1.1992 0.4761 sec/batch\n",
      "Epoch 30/40  Iteration 8127/11040 Training loss: 1.1995 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8128/11040 Training loss: 1.2000 0.4701 sec/batch\n",
      "Epoch 30/40  Iteration 8129/11040 Training loss: 1.2001 0.4639 sec/batch\n",
      "Epoch 30/40  Iteration 8130/11040 Training loss: 1.2004 0.4599 sec/batch\n",
      "Epoch 30/40  Iteration 8131/11040 Training loss: 1.2005 0.4741 sec/batch\n",
      "Epoch 30/40  Iteration 8132/11040 Training loss: 1.2005 0.4748 sec/batch\n",
      "Epoch 30/40  Iteration 8133/11040 Training loss: 1.2003 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8134/11040 Training loss: 1.2006 0.4755 sec/batch\n",
      "Epoch 30/40  Iteration 8135/11040 Training loss: 1.2006 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8136/11040 Training loss: 1.2005 0.4848 sec/batch\n",
      "Epoch 30/40  Iteration 8137/11040 Training loss: 1.2005 0.4815 sec/batch\n",
      "Epoch 30/40  Iteration 8138/11040 Training loss: 1.2003 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8139/11040 Training loss: 1.2005 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8140/11040 Training loss: 1.2005 0.4769 sec/batch\n",
      "Epoch 30/40  Iteration 8141/11040 Training loss: 1.2004 0.4749 sec/batch\n",
      "Epoch 30/40  Iteration 8142/11040 Training loss: 1.2005 0.4764 sec/batch\n",
      "Epoch 30/40  Iteration 8143/11040 Training loss: 1.2007 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8144/11040 Training loss: 1.2008 0.4761 sec/batch\n",
      "Epoch 30/40  Iteration 8145/11040 Training loss: 1.2009 0.4741 sec/batch\n",
      "Epoch 30/40  Iteration 8146/11040 Training loss: 1.2010 0.4763 sec/batch\n",
      "Epoch 30/40  Iteration 8147/11040 Training loss: 1.2011 0.4771 sec/batch\n",
      "Epoch 30/40  Iteration 8148/11040 Training loss: 1.2009 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8149/11040 Training loss: 1.2010 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8150/11040 Training loss: 1.2009 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8151/11040 Training loss: 1.2010 0.4737 sec/batch\n",
      "Epoch 30/40  Iteration 8152/11040 Training loss: 1.2009 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8153/11040 Training loss: 1.2008 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8154/11040 Training loss: 1.2008 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8155/11040 Training loss: 1.2008 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8156/11040 Training loss: 1.2010 0.4747 sec/batch\n",
      "Epoch 30/40  Iteration 8157/11040 Training loss: 1.2011 0.4764 sec/batch\n",
      "Epoch 30/40  Iteration 8158/11040 Training loss: 1.2012 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8159/11040 Training loss: 1.2013 0.4731 sec/batch\n",
      "Epoch 30/40  Iteration 8160/11040 Training loss: 1.2013 0.4770 sec/batch\n",
      "Epoch 30/40  Iteration 8161/11040 Training loss: 1.2012 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8162/11040 Training loss: 1.2014 0.4775 sec/batch\n",
      "Epoch 30/40  Iteration 8163/11040 Training loss: 1.2016 0.4910 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40  Iteration 8164/11040 Training loss: 1.2016 0.4737 sec/batch\n",
      "Epoch 30/40  Iteration 8165/11040 Training loss: 1.2017 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8166/11040 Training loss: 1.2018 0.4749 sec/batch\n",
      "Epoch 30/40  Iteration 8167/11040 Training loss: 1.2018 0.4768 sec/batch\n",
      "Epoch 30/40  Iteration 8168/11040 Training loss: 1.2020 0.4738 sec/batch\n",
      "Epoch 30/40  Iteration 8169/11040 Training loss: 1.2022 0.4759 sec/batch\n",
      "Epoch 30/40  Iteration 8170/11040 Training loss: 1.2025 0.4747 sec/batch\n",
      "Epoch 30/40  Iteration 8171/11040 Training loss: 1.2025 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8172/11040 Training loss: 1.2024 0.4772 sec/batch\n",
      "Epoch 30/40  Iteration 8173/11040 Training loss: 1.2025 0.4747 sec/batch\n",
      "Epoch 30/40  Iteration 8174/11040 Training loss: 1.2024 0.4742 sec/batch\n",
      "Epoch 30/40  Iteration 8175/11040 Training loss: 1.2027 0.4921 sec/batch\n",
      "Epoch 30/40  Iteration 8176/11040 Training loss: 1.2028 0.4760 sec/batch\n",
      "Epoch 30/40  Iteration 8177/11040 Training loss: 1.2028 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8178/11040 Training loss: 1.2028 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8179/11040 Training loss: 1.2029 0.4758 sec/batch\n",
      "Epoch 30/40  Iteration 8180/11040 Training loss: 1.2029 0.4734 sec/batch\n",
      "Epoch 30/40  Iteration 8181/11040 Training loss: 1.2030 0.4772 sec/batch\n",
      "Epoch 30/40  Iteration 8182/11040 Training loss: 1.2030 0.4917 sec/batch\n",
      "Epoch 30/40  Iteration 8183/11040 Training loss: 1.2032 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8184/11040 Training loss: 1.2033 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8185/11040 Training loss: 1.2033 0.4770 sec/batch\n",
      "Epoch 30/40  Iteration 8186/11040 Training loss: 1.2033 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8187/11040 Training loss: 1.2032 0.4748 sec/batch\n",
      "Epoch 30/40  Iteration 8188/11040 Training loss: 1.2033 0.4762 sec/batch\n",
      "Epoch 30/40  Iteration 8189/11040 Training loss: 1.2034 0.4740 sec/batch\n",
      "Epoch 30/40  Iteration 8190/11040 Training loss: 1.2035 0.4918 sec/batch\n",
      "Epoch 30/40  Iteration 8191/11040 Training loss: 1.2037 0.4748 sec/batch\n",
      "Epoch 30/40  Iteration 8192/11040 Training loss: 1.2037 0.4752 sec/batch\n",
      "Epoch 30/40  Iteration 8193/11040 Training loss: 1.2037 0.4770 sec/batch\n",
      "Epoch 30/40  Iteration 8194/11040 Training loss: 1.2037 0.4761 sec/batch\n",
      "Epoch 30/40  Iteration 8195/11040 Training loss: 1.2036 0.4903 sec/batch\n",
      "Epoch 30/40  Iteration 8196/11040 Training loss: 1.2036 0.4890 sec/batch\n",
      "Epoch 30/40  Iteration 8197/11040 Training loss: 1.2036 0.4801 sec/batch\n",
      "Epoch 30/40  Iteration 8198/11040 Training loss: 1.2037 0.4713 sec/batch\n",
      "Epoch 30/40  Iteration 8199/11040 Training loss: 1.2039 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8200/11040 Training loss: 1.2038 0.4767 sec/batch\n",
      "Epoch 30/40  Iteration 8201/11040 Training loss: 1.2040 0.4764 sec/batch\n",
      "Epoch 30/40  Iteration 8202/11040 Training loss: 1.2040 0.4896 sec/batch\n",
      "Epoch 30/40  Iteration 8203/11040 Training loss: 1.2040 0.4750 sec/batch\n",
      "Epoch 30/40  Iteration 8204/11040 Training loss: 1.2041 0.4763 sec/batch\n",
      "Epoch 30/40  Iteration 8205/11040 Training loss: 1.2043 0.4767 sec/batch\n",
      "Epoch 30/40  Iteration 8206/11040 Training loss: 1.2044 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8207/11040 Training loss: 1.2045 0.4760 sec/batch\n",
      "Epoch 30/40  Iteration 8208/11040 Training loss: 1.2046 0.5065 sec/batch\n",
      "Epoch 30/40  Iteration 8209/11040 Training loss: 1.2045 0.4770 sec/batch\n",
      "Epoch 30/40  Iteration 8210/11040 Training loss: 1.2045 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8211/11040 Training loss: 1.2046 0.4926 sec/batch\n",
      "Epoch 30/40  Iteration 8212/11040 Training loss: 1.2046 0.4760 sec/batch\n",
      "Epoch 30/40  Iteration 8213/11040 Training loss: 1.2044 0.4756 sec/batch\n",
      "Epoch 30/40  Iteration 8214/11040 Training loss: 1.2043 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8215/11040 Training loss: 1.2044 0.4893 sec/batch\n",
      "Epoch 30/40  Iteration 8216/11040 Training loss: 1.2044 0.4770 sec/batch\n",
      "Epoch 30/40  Iteration 8217/11040 Training loss: 1.2043 0.4886 sec/batch\n",
      "Epoch 30/40  Iteration 8218/11040 Training loss: 1.2043 0.4742 sec/batch\n",
      "Epoch 30/40  Iteration 8219/11040 Training loss: 1.2042 0.4762 sec/batch\n",
      "Epoch 30/40  Iteration 8220/11040 Training loss: 1.2041 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8221/11040 Training loss: 1.2040 0.4777 sec/batch\n",
      "Epoch 30/40  Iteration 8222/11040 Training loss: 1.2039 0.4757 sec/batch\n",
      "Epoch 30/40  Iteration 8223/11040 Training loss: 1.2039 0.4750 sec/batch\n",
      "Epoch 30/40  Iteration 8224/11040 Training loss: 1.2038 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8225/11040 Training loss: 1.2037 0.4749 sec/batch\n",
      "Epoch 30/40  Iteration 8226/11040 Training loss: 1.2036 0.4944 sec/batch\n",
      "Epoch 30/40  Iteration 8227/11040 Training loss: 1.2035 0.4901 sec/batch\n",
      "Epoch 30/40  Iteration 8228/11040 Training loss: 1.2036 0.4751 sec/batch\n",
      "Epoch 30/40  Iteration 8229/11040 Training loss: 1.2035 0.4747 sec/batch\n",
      "Epoch 30/40  Iteration 8230/11040 Training loss: 1.2035 0.4735 sec/batch\n",
      "Epoch 30/40  Iteration 8231/11040 Training loss: 1.2033 0.4768 sec/batch\n",
      "Epoch 30/40  Iteration 8232/11040 Training loss: 1.2032 0.4752 sec/batch\n",
      "Epoch 30/40  Iteration 8233/11040 Training loss: 1.2034 0.4763 sec/batch\n",
      "Epoch 30/40  Iteration 8234/11040 Training loss: 1.2035 0.4741 sec/batch\n",
      "Epoch 30/40  Iteration 8235/11040 Training loss: 1.2035 0.4776 sec/batch\n",
      "Epoch 30/40  Iteration 8236/11040 Training loss: 1.2035 0.4732 sec/batch\n",
      "Epoch 30/40  Iteration 8237/11040 Training loss: 1.2035 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8238/11040 Training loss: 1.2035 0.4760 sec/batch\n",
      "Epoch 30/40  Iteration 8239/11040 Training loss: 1.2036 0.4746 sec/batch\n",
      "Epoch 30/40  Iteration 8240/11040 Training loss: 1.2036 0.4782 sec/batch\n",
      "Epoch 30/40  Iteration 8241/11040 Training loss: 1.2037 0.4825 sec/batch\n",
      "Epoch 30/40  Iteration 8242/11040 Training loss: 1.2038 0.4967 sec/batch\n",
      "Epoch 30/40  Iteration 8243/11040 Training loss: 1.2039 0.4748 sec/batch\n",
      "Epoch 30/40  Iteration 8244/11040 Training loss: 1.2040 0.4773 sec/batch\n",
      "Epoch 30/40  Iteration 8245/11040 Training loss: 1.2041 0.4876 sec/batch\n",
      "Epoch 30/40  Iteration 8246/11040 Training loss: 1.2041 0.4796 sec/batch\n",
      "Epoch 30/40  Iteration 8247/11040 Training loss: 1.2041 0.4750 sec/batch\n",
      "Epoch 30/40  Iteration 8248/11040 Training loss: 1.2041 0.4895 sec/batch\n",
      "Epoch 30/40  Iteration 8249/11040 Training loss: 1.2040 0.4774 sec/batch\n",
      "Epoch 30/40  Iteration 8250/11040 Training loss: 1.2041 0.4735 sec/batch\n",
      "Epoch 30/40  Iteration 8251/11040 Training loss: 1.2041 0.4757 sec/batch\n",
      "Epoch 30/40  Iteration 8252/11040 Training loss: 1.2041 0.4761 sec/batch\n",
      "Epoch 30/40  Iteration 8253/11040 Training loss: 1.2041 0.4736 sec/batch\n",
      "Epoch 30/40  Iteration 8254/11040 Training loss: 1.2041 0.4764 sec/batch\n",
      "Epoch 30/40  Iteration 8255/11040 Training loss: 1.2040 0.4734 sec/batch\n",
      "Epoch 30/40  Iteration 8256/11040 Training loss: 1.2040 0.4759 sec/batch\n",
      "Epoch 30/40  Iteration 8257/11040 Training loss: 1.2040 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8258/11040 Training loss: 1.2041 0.4773 sec/batch\n",
      "Epoch 30/40  Iteration 8259/11040 Training loss: 1.2041 0.4744 sec/batch\n",
      "Epoch 30/40  Iteration 8260/11040 Training loss: 1.2042 0.4733 sec/batch\n",
      "Epoch 30/40  Iteration 8261/11040 Training loss: 1.2043 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8262/11040 Training loss: 1.2044 0.4661 sec/batch\n",
      "Epoch 30/40  Iteration 8263/11040 Training loss: 1.2043 0.4717 sec/batch\n",
      "Epoch 30/40  Iteration 8264/11040 Training loss: 1.2044 0.4893 sec/batch\n",
      "Epoch 30/40  Iteration 8265/11040 Training loss: 1.2045 0.4745 sec/batch\n",
      "Epoch 30/40  Iteration 8266/11040 Training loss: 1.2047 0.4815 sec/batch\n",
      "Epoch 30/40  Iteration 8267/11040 Training loss: 1.2047 0.4864 sec/batch\n",
      "Epoch 30/40  Iteration 8268/11040 Training loss: 1.2048 0.4748 sec/batch\n",
      "Epoch 30/40  Iteration 8269/11040 Training loss: 1.2048 0.4743 sec/batch\n",
      "Epoch 30/40  Iteration 8270/11040 Training loss: 1.2049 0.4933 sec/batch\n",
      "Epoch 30/40  Iteration 8271/11040 Training loss: 1.2051 0.4741 sec/batch\n",
      "Epoch 30/40  Iteration 8272/11040 Training loss: 1.2052 0.4754 sec/batch\n",
      "Epoch 30/40  Iteration 8273/11040 Training loss: 1.2053 0.4888 sec/batch\n",
      "Epoch 30/40  Iteration 8274/11040 Training loss: 1.2054 0.4782 sec/batch\n",
      "Epoch 30/40  Iteration 8275/11040 Training loss: 1.2057 0.4753 sec/batch\n",
      "Epoch 30/40  Iteration 8276/11040 Training loss: 1.2059 0.4762 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40  Iteration 8277/11040 Training loss: 1.2060 0.4918 sec/batch\n",
      "Epoch 30/40  Iteration 8278/11040 Training loss: 1.2061 0.4728 sec/batch\n",
      "Epoch 30/40  Iteration 8279/11040 Training loss: 1.2061 0.4748 sec/batch\n",
      "Epoch 30/40  Iteration 8280/11040 Training loss: 1.2063 0.4770 sec/batch\n",
      "Epoch 31/40  Iteration 8281/11040 Training loss: 1.2818 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8282/11040 Training loss: 1.2490 0.4772 sec/batch\n",
      "Epoch 31/40  Iteration 8283/11040 Training loss: 1.2407 0.4894 sec/batch\n",
      "Epoch 31/40  Iteration 8284/11040 Training loss: 1.2376 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8285/11040 Training loss: 1.2352 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8286/11040 Training loss: 1.2311 0.4734 sec/batch\n",
      "Epoch 31/40  Iteration 8287/11040 Training loss: 1.2219 0.4808 sec/batch\n",
      "Epoch 31/40  Iteration 8288/11040 Training loss: 1.2192 0.4709 sec/batch\n",
      "Epoch 31/40  Iteration 8289/11040 Training loss: 1.2155 0.4757 sec/batch\n",
      "Epoch 31/40  Iteration 8290/11040 Training loss: 1.2156 0.4766 sec/batch\n",
      "Epoch 31/40  Iteration 8291/11040 Training loss: 1.2115 0.4739 sec/batch\n",
      "Epoch 31/40  Iteration 8292/11040 Training loss: 1.2112 0.4858 sec/batch\n",
      "Epoch 31/40  Iteration 8293/11040 Training loss: 1.2073 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8294/11040 Training loss: 1.2050 0.4728 sec/batch\n",
      "Epoch 31/40  Iteration 8295/11040 Training loss: 1.2044 0.4776 sec/batch\n",
      "Epoch 31/40  Iteration 8296/11040 Training loss: 1.2048 0.4924 sec/batch\n",
      "Epoch 31/40  Iteration 8297/11040 Training loss: 1.2022 0.4674 sec/batch\n",
      "Epoch 31/40  Iteration 8298/11040 Training loss: 1.2003 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8299/11040 Training loss: 1.1990 0.4778 sec/batch\n",
      "Epoch 31/40  Iteration 8300/11040 Training loss: 1.1991 0.4731 sec/batch\n",
      "Epoch 31/40  Iteration 8301/11040 Training loss: 1.2008 0.4737 sec/batch\n",
      "Epoch 31/40  Iteration 8302/11040 Training loss: 1.2015 0.4882 sec/batch\n",
      "Epoch 31/40  Iteration 8303/11040 Training loss: 1.2010 0.4796 sec/batch\n",
      "Epoch 31/40  Iteration 8304/11040 Training loss: 1.2012 0.4727 sec/batch\n",
      "Epoch 31/40  Iteration 8305/11040 Training loss: 1.2000 0.4771 sec/batch\n",
      "Epoch 31/40  Iteration 8306/11040 Training loss: 1.1996 0.4762 sec/batch\n",
      "Epoch 31/40  Iteration 8307/11040 Training loss: 1.1992 0.4931 sec/batch\n",
      "Epoch 31/40  Iteration 8308/11040 Training loss: 1.1990 0.4913 sec/batch\n",
      "Epoch 31/40  Iteration 8309/11040 Training loss: 1.1991 0.4902 sec/batch\n",
      "Epoch 31/40  Iteration 8310/11040 Training loss: 1.1991 0.4890 sec/batch\n",
      "Epoch 31/40  Iteration 8311/11040 Training loss: 1.1990 0.4756 sec/batch\n",
      "Epoch 31/40  Iteration 8312/11040 Training loss: 1.1988 0.4758 sec/batch\n",
      "Epoch 31/40  Iteration 8313/11040 Training loss: 1.1979 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8314/11040 Training loss: 1.1978 0.4769 sec/batch\n",
      "Epoch 31/40  Iteration 8315/11040 Training loss: 1.1978 0.4747 sec/batch\n",
      "Epoch 31/40  Iteration 8316/11040 Training loss: 1.1982 0.4744 sec/batch\n",
      "Epoch 31/40  Iteration 8317/11040 Training loss: 1.1981 0.4762 sec/batch\n",
      "Epoch 31/40  Iteration 8318/11040 Training loss: 1.1974 0.4921 sec/batch\n",
      "Epoch 31/40  Iteration 8319/11040 Training loss: 1.1966 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8320/11040 Training loss: 1.1966 0.4736 sec/batch\n",
      "Epoch 31/40  Iteration 8321/11040 Training loss: 1.1962 0.4773 sec/batch\n",
      "Epoch 31/40  Iteration 8322/11040 Training loss: 1.1951 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8323/11040 Training loss: 1.1953 0.4896 sec/batch\n",
      "Epoch 31/40  Iteration 8324/11040 Training loss: 1.1945 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8325/11040 Training loss: 1.1939 0.4770 sec/batch\n",
      "Epoch 31/40  Iteration 8326/11040 Training loss: 1.1933 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8327/11040 Training loss: 1.1926 0.4919 sec/batch\n",
      "Epoch 31/40  Iteration 8328/11040 Training loss: 1.1924 0.4761 sec/batch\n",
      "Epoch 31/40  Iteration 8329/11040 Training loss: 1.1921 0.4921 sec/batch\n",
      "Epoch 31/40  Iteration 8330/11040 Training loss: 1.1926 0.4889 sec/batch\n",
      "Epoch 31/40  Iteration 8331/11040 Training loss: 1.1922 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8332/11040 Training loss: 1.1920 0.4774 sec/batch\n",
      "Epoch 31/40  Iteration 8333/11040 Training loss: 1.1923 0.4896 sec/batch\n",
      "Epoch 31/40  Iteration 8334/11040 Training loss: 1.1921 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8335/11040 Training loss: 1.1920 0.4751 sec/batch\n",
      "Epoch 31/40  Iteration 8336/11040 Training loss: 1.1920 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8337/11040 Training loss: 1.1919 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8338/11040 Training loss: 1.1918 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8339/11040 Training loss: 1.1919 0.4919 sec/batch\n",
      "Epoch 31/40  Iteration 8340/11040 Training loss: 1.1914 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8341/11040 Training loss: 1.1914 0.4816 sec/batch\n",
      "Epoch 31/40  Iteration 8342/11040 Training loss: 1.1914 0.4840 sec/batch\n",
      "Epoch 31/40  Iteration 8343/11040 Training loss: 1.1915 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8344/11040 Training loss: 1.1916 0.4757 sec/batch\n",
      "Epoch 31/40  Iteration 8345/11040 Training loss: 1.1918 0.4776 sec/batch\n",
      "Epoch 31/40  Iteration 8346/11040 Training loss: 1.1915 0.4749 sec/batch\n",
      "Epoch 31/40  Iteration 8347/11040 Training loss: 1.1913 0.4897 sec/batch\n",
      "Epoch 31/40  Iteration 8348/11040 Training loss: 1.1915 0.4773 sec/batch\n",
      "Epoch 31/40  Iteration 8349/11040 Training loss: 1.1912 0.4729 sec/batch\n",
      "Epoch 31/40  Iteration 8350/11040 Training loss: 1.1913 0.4761 sec/batch\n",
      "Epoch 31/40  Iteration 8351/11040 Training loss: 1.1909 0.4741 sec/batch\n",
      "Epoch 31/40  Iteration 8352/11040 Training loss: 1.1903 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8353/11040 Training loss: 1.1900 0.4748 sec/batch\n",
      "Epoch 31/40  Iteration 8354/11040 Training loss: 1.1896 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8355/11040 Training loss: 1.1895 0.4771 sec/batch\n",
      "Epoch 31/40  Iteration 8356/11040 Training loss: 1.1899 0.4761 sec/batch\n",
      "Epoch 31/40  Iteration 8357/11040 Training loss: 1.1893 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8358/11040 Training loss: 1.1889 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8359/11040 Training loss: 1.1887 0.4785 sec/batch\n",
      "Epoch 31/40  Iteration 8360/11040 Training loss: 1.1888 0.4719 sec/batch\n",
      "Epoch 31/40  Iteration 8361/11040 Training loss: 1.1887 0.4770 sec/batch\n",
      "Epoch 31/40  Iteration 8362/11040 Training loss: 1.1888 0.4892 sec/batch\n",
      "Epoch 31/40  Iteration 8363/11040 Training loss: 1.1888 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8364/11040 Training loss: 1.1888 0.4770 sec/batch\n",
      "Epoch 31/40  Iteration 8365/11040 Training loss: 1.1886 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8366/11040 Training loss: 1.1885 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8367/11040 Training loss: 1.1880 0.4755 sec/batch\n",
      "Epoch 31/40  Iteration 8368/11040 Training loss: 1.1871 0.4903 sec/batch\n",
      "Epoch 31/40  Iteration 8369/11040 Training loss: 1.1871 0.4758 sec/batch\n",
      "Epoch 31/40  Iteration 8370/11040 Training loss: 1.1874 0.4790 sec/batch\n",
      "Epoch 31/40  Iteration 8371/11040 Training loss: 1.1875 0.4898 sec/batch\n",
      "Epoch 31/40  Iteration 8372/11040 Training loss: 1.1875 0.4744 sec/batch\n",
      "Epoch 31/40  Iteration 8373/11040 Training loss: 1.1873 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8374/11040 Training loss: 1.1877 0.4773 sec/batch\n",
      "Epoch 31/40  Iteration 8375/11040 Training loss: 1.1882 0.4744 sec/batch\n",
      "Epoch 31/40  Iteration 8376/11040 Training loss: 1.1884 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8377/11040 Training loss: 1.1888 0.4743 sec/batch\n",
      "Epoch 31/40  Iteration 8378/11040 Training loss: 1.1894 0.4902 sec/batch\n",
      "Epoch 31/40  Iteration 8379/11040 Training loss: 1.1896 0.4747 sec/batch\n",
      "Epoch 31/40  Iteration 8380/11040 Training loss: 1.1895 0.4914 sec/batch\n",
      "Epoch 31/40  Iteration 8381/11040 Training loss: 1.1896 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8382/11040 Training loss: 1.1898 0.4766 sec/batch\n",
      "Epoch 31/40  Iteration 8383/11040 Training loss: 1.1895 0.4901 sec/batch\n",
      "Epoch 31/40  Iteration 8384/11040 Training loss: 1.1897 0.4674 sec/batch\n",
      "Epoch 31/40  Iteration 8385/11040 Training loss: 1.1896 0.4703 sec/batch\n",
      "Epoch 31/40  Iteration 8386/11040 Training loss: 1.1894 0.4740 sec/batch\n",
      "Epoch 31/40  Iteration 8387/11040 Training loss: 1.1893 0.4748 sec/batch\n",
      "Epoch 31/40  Iteration 8388/11040 Training loss: 1.1894 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8389/11040 Training loss: 1.1897 0.4757 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40  Iteration 8390/11040 Training loss: 1.1898 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8391/11040 Training loss: 1.1898 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8392/11040 Training loss: 1.1900 0.4751 sec/batch\n",
      "Epoch 31/40  Iteration 8393/11040 Training loss: 1.1901 0.4747 sec/batch\n",
      "Epoch 31/40  Iteration 8394/11040 Training loss: 1.1901 0.4768 sec/batch\n",
      "Epoch 31/40  Iteration 8395/11040 Training loss: 1.1904 0.4766 sec/batch\n",
      "Epoch 31/40  Iteration 8396/11040 Training loss: 1.1907 0.4742 sec/batch\n",
      "Epoch 31/40  Iteration 8397/11040 Training loss: 1.1906 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8398/11040 Training loss: 1.1908 0.4752 sec/batch\n",
      "Epoch 31/40  Iteration 8399/11040 Training loss: 1.1909 0.4911 sec/batch\n",
      "Epoch 31/40  Iteration 8400/11040 Training loss: 1.1910 0.4889 sec/batch\n",
      "Epoch 31/40  Iteration 8401/11040 Training loss: 1.1910 0.4784 sec/batch\n",
      "Epoch 31/40  Iteration 8402/11040 Training loss: 1.1911 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8403/11040 Training loss: 1.1915 0.4909 sec/batch\n",
      "Epoch 31/40  Iteration 8404/11040 Training loss: 1.1920 0.4901 sec/batch\n",
      "Epoch 31/40  Iteration 8405/11040 Training loss: 1.1920 0.4775 sec/batch\n",
      "Epoch 31/40  Iteration 8406/11040 Training loss: 1.1923 0.4742 sec/batch\n",
      "Epoch 31/40  Iteration 8407/11040 Training loss: 1.1924 0.4761 sec/batch\n",
      "Epoch 31/40  Iteration 8408/11040 Training loss: 1.1924 0.4926 sec/batch\n",
      "Epoch 31/40  Iteration 8409/11040 Training loss: 1.1922 0.4749 sec/batch\n",
      "Epoch 31/40  Iteration 8410/11040 Training loss: 1.1925 0.4755 sec/batch\n",
      "Epoch 31/40  Iteration 8411/11040 Training loss: 1.1925 0.4754 sec/batch\n",
      "Epoch 31/40  Iteration 8412/11040 Training loss: 1.1924 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8413/11040 Training loss: 1.1925 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8414/11040 Training loss: 1.1923 0.4944 sec/batch\n",
      "Epoch 31/40  Iteration 8415/11040 Training loss: 1.1924 0.4898 sec/batch\n",
      "Epoch 31/40  Iteration 8416/11040 Training loss: 1.1925 0.4896 sec/batch\n",
      "Epoch 31/40  Iteration 8417/11040 Training loss: 1.1923 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8418/11040 Training loss: 1.1925 0.4763 sec/batch\n",
      "Epoch 31/40  Iteration 8419/11040 Training loss: 1.1927 0.4757 sec/batch\n",
      "Epoch 31/40  Iteration 8420/11040 Training loss: 1.1928 0.4931 sec/batch\n",
      "Epoch 31/40  Iteration 8421/11040 Training loss: 1.1929 0.4885 sec/batch\n",
      "Epoch 31/40  Iteration 8422/11040 Training loss: 1.1930 0.4778 sec/batch\n",
      "Epoch 31/40  Iteration 8423/11040 Training loss: 1.1931 0.4737 sec/batch\n",
      "Epoch 31/40  Iteration 8424/11040 Training loss: 1.1931 0.4778 sec/batch\n",
      "Epoch 31/40  Iteration 8425/11040 Training loss: 1.1931 0.4928 sec/batch\n",
      "Epoch 31/40  Iteration 8426/11040 Training loss: 1.1931 0.4890 sec/batch\n",
      "Epoch 31/40  Iteration 8427/11040 Training loss: 1.1932 0.4765 sec/batch\n",
      "Epoch 31/40  Iteration 8428/11040 Training loss: 1.1931 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8429/11040 Training loss: 1.1930 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8430/11040 Training loss: 1.1931 0.4899 sec/batch\n",
      "Epoch 31/40  Iteration 8431/11040 Training loss: 1.1932 0.4920 sec/batch\n",
      "Epoch 31/40  Iteration 8432/11040 Training loss: 1.1934 0.4769 sec/batch\n",
      "Epoch 31/40  Iteration 8433/11040 Training loss: 1.1935 0.4759 sec/batch\n",
      "Epoch 31/40  Iteration 8434/11040 Training loss: 1.1936 0.4758 sec/batch\n",
      "Epoch 31/40  Iteration 8435/11040 Training loss: 1.1937 0.4748 sec/batch\n",
      "Epoch 31/40  Iteration 8436/11040 Training loss: 1.1938 0.4913 sec/batch\n",
      "Epoch 31/40  Iteration 8437/11040 Training loss: 1.1937 0.4915 sec/batch\n",
      "Epoch 31/40  Iteration 8438/11040 Training loss: 1.1938 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8439/11040 Training loss: 1.1939 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8440/11040 Training loss: 1.1939 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8441/11040 Training loss: 1.1940 0.4895 sec/batch\n",
      "Epoch 31/40  Iteration 8442/11040 Training loss: 1.1941 0.4786 sec/batch\n",
      "Epoch 31/40  Iteration 8443/11040 Training loss: 1.1942 0.4754 sec/batch\n",
      "Epoch 31/40  Iteration 8444/11040 Training loss: 1.1944 0.4909 sec/batch\n",
      "Epoch 31/40  Iteration 8445/11040 Training loss: 1.1947 0.4762 sec/batch\n",
      "Epoch 31/40  Iteration 8446/11040 Training loss: 1.1950 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8447/11040 Training loss: 1.1950 0.4744 sec/batch\n",
      "Epoch 31/40  Iteration 8448/11040 Training loss: 1.1949 0.4747 sec/batch\n",
      "Epoch 31/40  Iteration 8449/11040 Training loss: 1.1950 0.4784 sec/batch\n",
      "Epoch 31/40  Iteration 8450/11040 Training loss: 1.1950 0.5046 sec/batch\n",
      "Epoch 31/40  Iteration 8451/11040 Training loss: 1.1953 0.4784 sec/batch\n",
      "Epoch 31/40  Iteration 8452/11040 Training loss: 1.1954 0.4721 sec/batch\n",
      "Epoch 31/40  Iteration 8453/11040 Training loss: 1.1954 0.4627 sec/batch\n",
      "Epoch 31/40  Iteration 8454/11040 Training loss: 1.1953 0.4719 sec/batch\n",
      "Epoch 31/40  Iteration 8455/11040 Training loss: 1.1954 0.4932 sec/batch\n",
      "Epoch 31/40  Iteration 8456/11040 Training loss: 1.1954 0.4749 sec/batch\n",
      "Epoch 31/40  Iteration 8457/11040 Training loss: 1.1955 0.4587 sec/batch\n",
      "Epoch 31/40  Iteration 8458/11040 Training loss: 1.1955 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8459/11040 Training loss: 1.1957 0.4641 sec/batch\n",
      "Epoch 31/40  Iteration 8460/11040 Training loss: 1.1959 0.4717 sec/batch\n",
      "Epoch 31/40  Iteration 8461/11040 Training loss: 1.1959 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8462/11040 Training loss: 1.1960 0.4735 sec/batch\n",
      "Epoch 31/40  Iteration 8463/11040 Training loss: 1.1959 0.4735 sec/batch\n",
      "Epoch 31/40  Iteration 8464/11040 Training loss: 1.1960 0.4721 sec/batch\n",
      "Epoch 31/40  Iteration 8465/11040 Training loss: 1.1961 0.4588 sec/batch\n",
      "Epoch 31/40  Iteration 8466/11040 Training loss: 1.1962 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8467/11040 Training loss: 1.1965 0.4763 sec/batch\n",
      "Epoch 31/40  Iteration 8468/11040 Training loss: 1.1966 0.4752 sec/batch\n",
      "Epoch 31/40  Iteration 8469/11040 Training loss: 1.1965 0.4766 sec/batch\n",
      "Epoch 31/40  Iteration 8470/11040 Training loss: 1.1965 0.4719 sec/batch\n",
      "Epoch 31/40  Iteration 8471/11040 Training loss: 1.1964 0.4754 sec/batch\n",
      "Epoch 31/40  Iteration 8472/11040 Training loss: 1.1964 0.4743 sec/batch\n",
      "Epoch 31/40  Iteration 8473/11040 Training loss: 1.1964 0.4607 sec/batch\n",
      "Epoch 31/40  Iteration 8474/11040 Training loss: 1.1965 0.4742 sec/batch\n",
      "Epoch 31/40  Iteration 8475/11040 Training loss: 1.1967 0.4757 sec/batch\n",
      "Epoch 31/40  Iteration 8476/11040 Training loss: 1.1966 0.4737 sec/batch\n",
      "Epoch 31/40  Iteration 8477/11040 Training loss: 1.1967 0.4769 sec/batch\n",
      "Epoch 31/40  Iteration 8478/11040 Training loss: 1.1967 0.4677 sec/batch\n",
      "Epoch 31/40  Iteration 8479/11040 Training loss: 1.1968 0.4670 sec/batch\n",
      "Epoch 31/40  Iteration 8480/11040 Training loss: 1.1969 0.4734 sec/batch\n",
      "Epoch 31/40  Iteration 8481/11040 Training loss: 1.1972 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8482/11040 Training loss: 1.1974 0.4744 sec/batch\n",
      "Epoch 31/40  Iteration 8483/11040 Training loss: 1.1975 0.4763 sec/batch\n",
      "Epoch 31/40  Iteration 8484/11040 Training loss: 1.1976 0.4774 sec/batch\n",
      "Epoch 31/40  Iteration 8485/11040 Training loss: 1.1977 0.4713 sec/batch\n",
      "Epoch 31/40  Iteration 8486/11040 Training loss: 1.1976 0.4755 sec/batch\n",
      "Epoch 31/40  Iteration 8487/11040 Training loss: 1.1976 0.4743 sec/batch\n",
      "Epoch 31/40  Iteration 8488/11040 Training loss: 1.1976 0.4774 sec/batch\n",
      "Epoch 31/40  Iteration 8489/11040 Training loss: 1.1975 0.4747 sec/batch\n",
      "Epoch 31/40  Iteration 8490/11040 Training loss: 1.1975 0.4901 sec/batch\n",
      "Epoch 31/40  Iteration 8491/11040 Training loss: 1.1975 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8492/11040 Training loss: 1.1975 0.4757 sec/batch\n",
      "Epoch 31/40  Iteration 8493/11040 Training loss: 1.1975 0.4904 sec/batch\n",
      "Epoch 31/40  Iteration 8494/11040 Training loss: 1.1975 0.4774 sec/batch\n",
      "Epoch 31/40  Iteration 8495/11040 Training loss: 1.1974 0.4743 sec/batch\n",
      "Epoch 31/40  Iteration 8496/11040 Training loss: 1.1973 0.4747 sec/batch\n",
      "Epoch 31/40  Iteration 8497/11040 Training loss: 1.1971 0.4764 sec/batch\n",
      "Epoch 31/40  Iteration 8498/11040 Training loss: 1.1970 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8499/11040 Training loss: 1.1969 0.4740 sec/batch\n",
      "Epoch 31/40  Iteration 8500/11040 Training loss: 1.1969 0.4610 sec/batch\n",
      "Validation loss: 1.24619 Saving checkpoint!\n",
      "Epoch 31/40  Iteration 8501/11040 Training loss: 1.1976 0.4688 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40  Iteration 8502/11040 Training loss: 1.1974 0.4624 sec/batch\n",
      "Epoch 31/40  Iteration 8503/11040 Training loss: 1.1974 0.4724 sec/batch\n",
      "Epoch 31/40  Iteration 8504/11040 Training loss: 1.1975 0.4756 sec/batch\n",
      "Epoch 31/40  Iteration 8505/11040 Training loss: 1.1974 0.4732 sec/batch\n",
      "Epoch 31/40  Iteration 8506/11040 Training loss: 1.1974 0.4738 sec/batch\n",
      "Epoch 31/40  Iteration 8507/11040 Training loss: 1.1973 0.4602 sec/batch\n",
      "Epoch 31/40  Iteration 8508/11040 Training loss: 1.1972 0.4596 sec/batch\n",
      "Epoch 31/40  Iteration 8509/11040 Training loss: 1.1973 0.4583 sec/batch\n",
      "Epoch 31/40  Iteration 8510/11040 Training loss: 1.1973 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8511/11040 Training loss: 1.1974 0.4741 sec/batch\n",
      "Epoch 31/40  Iteration 8512/11040 Training loss: 1.1974 0.4751 sec/batch\n",
      "Epoch 31/40  Iteration 8513/11040 Training loss: 1.1974 0.4756 sec/batch\n",
      "Epoch 31/40  Iteration 8514/11040 Training loss: 1.1973 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8515/11040 Training loss: 1.1975 0.4730 sec/batch\n",
      "Epoch 31/40  Iteration 8516/11040 Training loss: 1.1975 0.4769 sec/batch\n",
      "Epoch 31/40  Iteration 8517/11040 Training loss: 1.1976 0.4736 sec/batch\n",
      "Epoch 31/40  Iteration 8518/11040 Training loss: 1.1977 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8519/11040 Training loss: 1.1978 0.4743 sec/batch\n",
      "Epoch 31/40  Iteration 8520/11040 Training loss: 1.1979 0.4756 sec/batch\n",
      "Epoch 31/40  Iteration 8521/11040 Training loss: 1.1980 0.4760 sec/batch\n",
      "Epoch 31/40  Iteration 8522/11040 Training loss: 1.1980 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8523/11040 Training loss: 1.1981 0.4586 sec/batch\n",
      "Epoch 31/40  Iteration 8524/11040 Training loss: 1.1981 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8525/11040 Training loss: 1.1980 0.4832 sec/batch\n",
      "Epoch 31/40  Iteration 8526/11040 Training loss: 1.1980 0.4633 sec/batch\n",
      "Epoch 31/40  Iteration 8527/11040 Training loss: 1.1980 0.4755 sec/batch\n",
      "Epoch 31/40  Iteration 8528/11040 Training loss: 1.1980 0.4725 sec/batch\n",
      "Epoch 31/40  Iteration 8529/11040 Training loss: 1.1980 0.4753 sec/batch\n",
      "Epoch 31/40  Iteration 8530/11040 Training loss: 1.1980 0.4750 sec/batch\n",
      "Epoch 31/40  Iteration 8531/11040 Training loss: 1.1979 0.4793 sec/batch\n",
      "Epoch 31/40  Iteration 8532/11040 Training loss: 1.1979 0.4709 sec/batch\n",
      "Epoch 31/40  Iteration 8533/11040 Training loss: 1.1979 0.4654 sec/batch\n",
      "Epoch 31/40  Iteration 8534/11040 Training loss: 1.1980 0.4853 sec/batch\n",
      "Epoch 31/40  Iteration 8535/11040 Training loss: 1.1980 0.4736 sec/batch\n",
      "Epoch 31/40  Iteration 8536/11040 Training loss: 1.1981 0.4754 sec/batch\n",
      "Epoch 31/40  Iteration 8537/11040 Training loss: 1.1982 0.4911 sec/batch\n",
      "Epoch 31/40  Iteration 8538/11040 Training loss: 1.1982 0.4729 sec/batch\n",
      "Epoch 31/40  Iteration 8539/11040 Training loss: 1.1982 0.4749 sec/batch\n",
      "Epoch 31/40  Iteration 8540/11040 Training loss: 1.1982 0.4772 sec/batch\n",
      "Epoch 31/40  Iteration 8541/11040 Training loss: 1.1983 0.4737 sec/batch\n",
      "Epoch 31/40  Iteration 8542/11040 Training loss: 1.1985 0.4607 sec/batch\n",
      "Epoch 31/40  Iteration 8543/11040 Training loss: 1.1986 0.4740 sec/batch\n",
      "Epoch 31/40  Iteration 8544/11040 Training loss: 1.1987 0.4746 sec/batch\n",
      "Epoch 31/40  Iteration 8545/11040 Training loss: 1.1987 0.4768 sec/batch\n",
      "Epoch 31/40  Iteration 8546/11040 Training loss: 1.1988 0.4735 sec/batch\n",
      "Epoch 31/40  Iteration 8547/11040 Training loss: 1.1990 0.4912 sec/batch\n",
      "Epoch 31/40  Iteration 8548/11040 Training loss: 1.1991 0.4745 sec/batch\n",
      "Epoch 31/40  Iteration 8549/11040 Training loss: 1.1991 0.4921 sec/batch\n",
      "Epoch 31/40  Iteration 8550/11040 Training loss: 1.1993 0.4739 sec/batch\n",
      "Epoch 31/40  Iteration 8551/11040 Training loss: 1.1996 0.4622 sec/batch\n",
      "Epoch 31/40  Iteration 8552/11040 Training loss: 1.1999 0.4749 sec/batch\n",
      "Epoch 31/40  Iteration 8553/11040 Training loss: 1.2000 0.4732 sec/batch\n",
      "Epoch 31/40  Iteration 8554/11040 Training loss: 1.2001 0.4779 sec/batch\n",
      "Epoch 31/40  Iteration 8555/11040 Training loss: 1.2001 0.4740 sec/batch\n",
      "Epoch 31/40  Iteration 8556/11040 Training loss: 1.2002 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8557/11040 Training loss: 1.2865 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8558/11040 Training loss: 1.2492 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8559/11040 Training loss: 1.2433 0.4755 sec/batch\n",
      "Epoch 32/40  Iteration 8560/11040 Training loss: 1.2394 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8561/11040 Training loss: 1.2361 0.4759 sec/batch\n",
      "Epoch 32/40  Iteration 8562/11040 Training loss: 1.2319 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8563/11040 Training loss: 1.2223 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8564/11040 Training loss: 1.2188 0.4748 sec/batch\n",
      "Epoch 32/40  Iteration 8565/11040 Training loss: 1.2148 0.4757 sec/batch\n",
      "Epoch 32/40  Iteration 8566/11040 Training loss: 1.2138 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8567/11040 Training loss: 1.2096 0.4740 sec/batch\n",
      "Epoch 32/40  Iteration 8568/11040 Training loss: 1.2077 0.4759 sec/batch\n",
      "Epoch 32/40  Iteration 8569/11040 Training loss: 1.2035 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8570/11040 Training loss: 1.2012 0.4758 sec/batch\n",
      "Epoch 32/40  Iteration 8571/11040 Training loss: 1.2008 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8572/11040 Training loss: 1.2005 0.4774 sec/batch\n",
      "Epoch 32/40  Iteration 8573/11040 Training loss: 1.1977 0.4734 sec/batch\n",
      "Epoch 32/40  Iteration 8574/11040 Training loss: 1.1959 0.4751 sec/batch\n",
      "Epoch 32/40  Iteration 8575/11040 Training loss: 1.1946 0.4751 sec/batch\n",
      "Epoch 32/40  Iteration 8576/11040 Training loss: 1.1942 0.4764 sec/batch\n",
      "Epoch 32/40  Iteration 8577/11040 Training loss: 1.1956 0.4780 sec/batch\n",
      "Epoch 32/40  Iteration 8578/11040 Training loss: 1.1959 0.4734 sec/batch\n",
      "Epoch 32/40  Iteration 8579/11040 Training loss: 1.1955 0.4734 sec/batch\n",
      "Epoch 32/40  Iteration 8580/11040 Training loss: 1.1962 0.4751 sec/batch\n",
      "Epoch 32/40  Iteration 8581/11040 Training loss: 1.1954 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8582/11040 Training loss: 1.1949 0.4757 sec/batch\n",
      "Epoch 32/40  Iteration 8583/11040 Training loss: 1.1946 0.4732 sec/batch\n",
      "Epoch 32/40  Iteration 8584/11040 Training loss: 1.1942 0.4787 sec/batch\n",
      "Epoch 32/40  Iteration 8585/11040 Training loss: 1.1945 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8586/11040 Training loss: 1.1945 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8587/11040 Training loss: 1.1945 0.4881 sec/batch\n",
      "Epoch 32/40  Iteration 8588/11040 Training loss: 1.1940 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8589/11040 Training loss: 1.1932 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8590/11040 Training loss: 1.1928 0.4794 sec/batch\n",
      "Epoch 32/40  Iteration 8591/11040 Training loss: 1.1927 0.4715 sec/batch\n",
      "Epoch 32/40  Iteration 8592/11040 Training loss: 1.1932 0.4755 sec/batch\n",
      "Epoch 32/40  Iteration 8593/11040 Training loss: 1.1932 0.4764 sec/batch\n",
      "Epoch 32/40  Iteration 8594/11040 Training loss: 1.1925 0.4773 sec/batch\n",
      "Epoch 32/40  Iteration 8595/11040 Training loss: 1.1920 0.4720 sec/batch\n",
      "Epoch 32/40  Iteration 8596/11040 Training loss: 1.1918 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8597/11040 Training loss: 1.1916 0.4761 sec/batch\n",
      "Epoch 32/40  Iteration 8598/11040 Training loss: 1.1906 0.4755 sec/batch\n",
      "Epoch 32/40  Iteration 8599/11040 Training loss: 1.1910 0.4760 sec/batch\n",
      "Epoch 32/40  Iteration 8600/11040 Training loss: 1.1902 0.4760 sec/batch\n",
      "Epoch 32/40  Iteration 8601/11040 Training loss: 1.1895 0.4733 sec/batch\n",
      "Epoch 32/40  Iteration 8602/11040 Training loss: 1.1888 0.4917 sec/batch\n",
      "Epoch 32/40  Iteration 8603/11040 Training loss: 1.1881 0.4739 sec/batch\n",
      "Epoch 32/40  Iteration 8604/11040 Training loss: 1.1879 0.4759 sec/batch\n",
      "Epoch 32/40  Iteration 8605/11040 Training loss: 1.1875 0.4752 sec/batch\n",
      "Epoch 32/40  Iteration 8606/11040 Training loss: 1.1879 0.4761 sec/batch\n",
      "Epoch 32/40  Iteration 8607/11040 Training loss: 1.1876 0.4740 sec/batch\n",
      "Epoch 32/40  Iteration 8608/11040 Training loss: 1.1873 0.4761 sec/batch\n",
      "Epoch 32/40  Iteration 8609/11040 Training loss: 1.1877 0.4759 sec/batch\n",
      "Epoch 32/40  Iteration 8610/11040 Training loss: 1.1873 0.4742 sec/batch\n",
      "Epoch 32/40  Iteration 8611/11040 Training loss: 1.1872 0.4769 sec/batch\n",
      "Epoch 32/40  Iteration 8612/11040 Training loss: 1.1871 0.4908 sec/batch\n",
      "Epoch 32/40  Iteration 8613/11040 Training loss: 1.1868 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8614/11040 Training loss: 1.1867 0.4757 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40  Iteration 8615/11040 Training loss: 1.1870 0.4919 sec/batch\n",
      "Epoch 32/40  Iteration 8616/11040 Training loss: 1.1865 0.4739 sec/batch\n",
      "Epoch 32/40  Iteration 8617/11040 Training loss: 1.1865 0.4766 sec/batch\n",
      "Epoch 32/40  Iteration 8618/11040 Training loss: 1.1862 0.4738 sec/batch\n",
      "Epoch 32/40  Iteration 8619/11040 Training loss: 1.1864 0.4766 sec/batch\n",
      "Epoch 32/40  Iteration 8620/11040 Training loss: 1.1863 0.4738 sec/batch\n",
      "Epoch 32/40  Iteration 8621/11040 Training loss: 1.1866 0.4761 sec/batch\n",
      "Epoch 32/40  Iteration 8622/11040 Training loss: 1.1865 0.4906 sec/batch\n",
      "Epoch 32/40  Iteration 8623/11040 Training loss: 1.1863 0.4765 sec/batch\n",
      "Epoch 32/40  Iteration 8624/11040 Training loss: 1.1863 0.4761 sec/batch\n",
      "Epoch 32/40  Iteration 8625/11040 Training loss: 1.1862 0.4727 sec/batch\n",
      "Epoch 32/40  Iteration 8626/11040 Training loss: 1.1860 0.4915 sec/batch\n",
      "Epoch 32/40  Iteration 8627/11040 Training loss: 1.1858 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8628/11040 Training loss: 1.1853 0.4758 sec/batch\n",
      "Epoch 32/40  Iteration 8629/11040 Training loss: 1.1848 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8630/11040 Training loss: 1.1842 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8631/11040 Training loss: 1.1842 0.4748 sec/batch\n",
      "Epoch 32/40  Iteration 8632/11040 Training loss: 1.1844 0.4758 sec/batch\n",
      "Epoch 32/40  Iteration 8633/11040 Training loss: 1.1840 0.4914 sec/batch\n",
      "Epoch 32/40  Iteration 8634/11040 Training loss: 1.1836 0.4923 sec/batch\n",
      "Epoch 32/40  Iteration 8635/11040 Training loss: 1.1837 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8636/11040 Training loss: 1.1838 0.4760 sec/batch\n",
      "Epoch 32/40  Iteration 8637/11040 Training loss: 1.1835 0.4755 sec/batch\n",
      "Epoch 32/40  Iteration 8638/11040 Training loss: 1.1838 0.4759 sec/batch\n",
      "Epoch 32/40  Iteration 8639/11040 Training loss: 1.1838 0.4903 sec/batch\n",
      "Epoch 32/40  Iteration 8640/11040 Training loss: 1.1838 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8641/11040 Training loss: 1.1835 0.4728 sec/batch\n",
      "Epoch 32/40  Iteration 8642/11040 Training loss: 1.1835 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8643/11040 Training loss: 1.1828 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8644/11040 Training loss: 1.1818 0.4738 sec/batch\n",
      "Epoch 32/40  Iteration 8645/11040 Training loss: 1.1818 0.4605 sec/batch\n",
      "Epoch 32/40  Iteration 8646/11040 Training loss: 1.1819 0.4596 sec/batch\n",
      "Epoch 32/40  Iteration 8647/11040 Training loss: 1.1820 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8648/11040 Training loss: 1.1819 0.4775 sec/batch\n",
      "Epoch 32/40  Iteration 8649/11040 Training loss: 1.1817 0.4718 sec/batch\n",
      "Epoch 32/40  Iteration 8650/11040 Training loss: 1.1821 0.4603 sec/batch\n",
      "Epoch 32/40  Iteration 8651/11040 Training loss: 1.1825 0.4734 sec/batch\n",
      "Epoch 32/40  Iteration 8652/11040 Training loss: 1.1828 0.4765 sec/batch\n",
      "Epoch 32/40  Iteration 8653/11040 Training loss: 1.1832 0.4738 sec/batch\n",
      "Epoch 32/40  Iteration 8654/11040 Training loss: 1.1836 0.4742 sec/batch\n",
      "Epoch 32/40  Iteration 8655/11040 Training loss: 1.1838 0.4709 sec/batch\n",
      "Epoch 32/40  Iteration 8656/11040 Training loss: 1.1836 0.4586 sec/batch\n",
      "Epoch 32/40  Iteration 8657/11040 Training loss: 1.1837 0.4591 sec/batch\n",
      "Epoch 32/40  Iteration 8658/11040 Training loss: 1.1839 0.4734 sec/batch\n",
      "Epoch 32/40  Iteration 8659/11040 Training loss: 1.1835 0.4757 sec/batch\n",
      "Epoch 32/40  Iteration 8660/11040 Training loss: 1.1837 0.4740 sec/batch\n",
      "Epoch 32/40  Iteration 8661/11040 Training loss: 1.1836 0.4741 sec/batch\n",
      "Epoch 32/40  Iteration 8662/11040 Training loss: 1.1835 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8663/11040 Training loss: 1.1833 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8664/11040 Training loss: 1.1833 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8665/11040 Training loss: 1.1837 0.4766 sec/batch\n",
      "Epoch 32/40  Iteration 8666/11040 Training loss: 1.1836 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8667/11040 Training loss: 1.1837 0.4603 sec/batch\n",
      "Epoch 32/40  Iteration 8668/11040 Training loss: 1.1839 0.4732 sec/batch\n",
      "Epoch 32/40  Iteration 8669/11040 Training loss: 1.1840 0.4749 sec/batch\n",
      "Epoch 32/40  Iteration 8670/11040 Training loss: 1.1840 0.4599 sec/batch\n",
      "Epoch 32/40  Iteration 8671/11040 Training loss: 1.1843 0.4586 sec/batch\n",
      "Epoch 32/40  Iteration 8672/11040 Training loss: 1.1845 0.4595 sec/batch\n",
      "Epoch 32/40  Iteration 8673/11040 Training loss: 1.1844 0.4752 sec/batch\n",
      "Epoch 32/40  Iteration 8674/11040 Training loss: 1.1846 0.4595 sec/batch\n",
      "Epoch 32/40  Iteration 8675/11040 Training loss: 1.1847 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8676/11040 Training loss: 1.1847 0.4614 sec/batch\n",
      "Epoch 32/40  Iteration 8677/11040 Training loss: 1.1847 0.4731 sec/batch\n",
      "Epoch 32/40  Iteration 8678/11040 Training loss: 1.1848 0.4763 sec/batch\n",
      "Epoch 32/40  Iteration 8679/11040 Training loss: 1.1852 0.4735 sec/batch\n",
      "Epoch 32/40  Iteration 8680/11040 Training loss: 1.1857 0.4771 sec/batch\n",
      "Epoch 32/40  Iteration 8681/11040 Training loss: 1.1859 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8682/11040 Training loss: 1.1861 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8683/11040 Training loss: 1.1862 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8684/11040 Training loss: 1.1863 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8685/11040 Training loss: 1.1860 0.4616 sec/batch\n",
      "Epoch 32/40  Iteration 8686/11040 Training loss: 1.1863 0.4783 sec/batch\n",
      "Epoch 32/40  Iteration 8687/11040 Training loss: 1.1863 0.4876 sec/batch\n",
      "Epoch 32/40  Iteration 8688/11040 Training loss: 1.1862 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8689/11040 Training loss: 1.1863 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8690/11040 Training loss: 1.1862 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8691/11040 Training loss: 1.1864 0.4753 sec/batch\n",
      "Epoch 32/40  Iteration 8692/11040 Training loss: 1.1864 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8693/11040 Training loss: 1.1863 0.4736 sec/batch\n",
      "Epoch 32/40  Iteration 8694/11040 Training loss: 1.1864 0.4614 sec/batch\n",
      "Epoch 32/40  Iteration 8695/11040 Training loss: 1.1867 0.4758 sec/batch\n",
      "Epoch 32/40  Iteration 8696/11040 Training loss: 1.1868 0.4604 sec/batch\n",
      "Epoch 32/40  Iteration 8697/11040 Training loss: 1.1869 0.4748 sec/batch\n",
      "Epoch 32/40  Iteration 8698/11040 Training loss: 1.1871 0.4726 sec/batch\n",
      "Epoch 32/40  Iteration 8699/11040 Training loss: 1.1872 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8700/11040 Training loss: 1.1871 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8701/11040 Training loss: 1.1871 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8702/11040 Training loss: 1.1871 0.4891 sec/batch\n",
      "Epoch 32/40  Iteration 8703/11040 Training loss: 1.1871 0.4765 sec/batch\n",
      "Epoch 32/40  Iteration 8704/11040 Training loss: 1.1871 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8705/11040 Training loss: 1.1870 0.4764 sec/batch\n",
      "Epoch 32/40  Iteration 8706/11040 Training loss: 1.1870 0.4741 sec/batch\n",
      "Epoch 32/40  Iteration 8707/11040 Training loss: 1.1870 0.4758 sec/batch\n",
      "Epoch 32/40  Iteration 8708/11040 Training loss: 1.1872 0.4733 sec/batch\n",
      "Epoch 32/40  Iteration 8709/11040 Training loss: 1.1873 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8710/11040 Training loss: 1.1875 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8711/11040 Training loss: 1.1875 0.4771 sec/batch\n",
      "Epoch 32/40  Iteration 8712/11040 Training loss: 1.1875 0.4891 sec/batch\n",
      "Epoch 32/40  Iteration 8713/11040 Training loss: 1.1875 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8714/11040 Training loss: 1.1876 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8715/11040 Training loss: 1.1877 0.4595 sec/batch\n",
      "Epoch 32/40  Iteration 8716/11040 Training loss: 1.1878 0.4760 sec/batch\n",
      "Epoch 32/40  Iteration 8717/11040 Training loss: 1.1879 0.4735 sec/batch\n",
      "Epoch 32/40  Iteration 8718/11040 Training loss: 1.1879 0.4855 sec/batch\n",
      "Epoch 32/40  Iteration 8719/11040 Training loss: 1.1880 0.4627 sec/batch\n",
      "Epoch 32/40  Iteration 8720/11040 Training loss: 1.1882 0.4735 sec/batch\n",
      "Epoch 32/40  Iteration 8721/11040 Training loss: 1.1884 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8722/11040 Training loss: 1.1887 0.4610 sec/batch\n",
      "Epoch 32/40  Iteration 8723/11040 Training loss: 1.1887 0.4766 sec/batch\n",
      "Epoch 32/40  Iteration 8724/11040 Training loss: 1.1887 0.4715 sec/batch\n",
      "Epoch 32/40  Iteration 8725/11040 Training loss: 1.1887 0.4763 sec/batch\n",
      "Epoch 32/40  Iteration 8726/11040 Training loss: 1.1888 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8727/11040 Training loss: 1.1890 0.4775 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40  Iteration 8728/11040 Training loss: 1.1891 0.4733 sec/batch\n",
      "Epoch 32/40  Iteration 8729/11040 Training loss: 1.1891 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8730/11040 Training loss: 1.1891 0.4600 sec/batch\n",
      "Epoch 32/40  Iteration 8731/11040 Training loss: 1.1891 0.4760 sec/batch\n",
      "Epoch 32/40  Iteration 8732/11040 Training loss: 1.1892 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8733/11040 Training loss: 1.1893 0.4902 sec/batch\n",
      "Epoch 32/40  Iteration 8734/11040 Training loss: 1.1893 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8735/11040 Training loss: 1.1896 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8736/11040 Training loss: 1.1896 0.4730 sec/batch\n",
      "Epoch 32/40  Iteration 8737/11040 Training loss: 1.1897 0.4773 sec/batch\n",
      "Epoch 32/40  Iteration 8738/11040 Training loss: 1.1897 0.4749 sec/batch\n",
      "Epoch 32/40  Iteration 8739/11040 Training loss: 1.1896 0.4806 sec/batch\n",
      "Epoch 32/40  Iteration 8740/11040 Training loss: 1.1897 0.4852 sec/batch\n",
      "Epoch 32/40  Iteration 8741/11040 Training loss: 1.1898 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8742/11040 Training loss: 1.1899 0.4751 sec/batch\n",
      "Epoch 32/40  Iteration 8743/11040 Training loss: 1.1902 0.4606 sec/batch\n",
      "Epoch 32/40  Iteration 8744/11040 Training loss: 1.1903 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8745/11040 Training loss: 1.1902 0.4733 sec/batch\n",
      "Epoch 32/40  Iteration 8746/11040 Training loss: 1.1902 0.4772 sec/batch\n",
      "Epoch 32/40  Iteration 8747/11040 Training loss: 1.1901 0.4736 sec/batch\n",
      "Epoch 32/40  Iteration 8748/11040 Training loss: 1.1901 0.4740 sec/batch\n",
      "Epoch 32/40  Iteration 8749/11040 Training loss: 1.1901 0.4806 sec/batch\n",
      "Epoch 32/40  Iteration 8750/11040 Training loss: 1.1902 0.4705 sec/batch\n",
      "Epoch 32/40  Iteration 8751/11040 Training loss: 1.1905 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8752/11040 Training loss: 1.1903 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8753/11040 Training loss: 1.1905 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8754/11040 Training loss: 1.1906 0.4754 sec/batch\n",
      "Epoch 32/40  Iteration 8755/11040 Training loss: 1.1907 0.4763 sec/batch\n",
      "Epoch 32/40  Iteration 8756/11040 Training loss: 1.1908 0.4899 sec/batch\n",
      "Epoch 32/40  Iteration 8757/11040 Training loss: 1.1910 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8758/11040 Training loss: 1.1912 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8759/11040 Training loss: 1.1914 0.4763 sec/batch\n",
      "Epoch 32/40  Iteration 8760/11040 Training loss: 1.1914 0.4740 sec/batch\n",
      "Epoch 32/40  Iteration 8761/11040 Training loss: 1.1914 0.4774 sec/batch\n",
      "Epoch 32/40  Iteration 8762/11040 Training loss: 1.1914 0.4759 sec/batch\n",
      "Epoch 32/40  Iteration 8763/11040 Training loss: 1.1914 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8764/11040 Training loss: 1.1915 0.4739 sec/batch\n",
      "Epoch 32/40  Iteration 8765/11040 Training loss: 1.1913 0.4761 sec/batch\n",
      "Epoch 32/40  Iteration 8766/11040 Training loss: 1.1913 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8767/11040 Training loss: 1.1913 0.4733 sec/batch\n",
      "Epoch 32/40  Iteration 8768/11040 Training loss: 1.1913 0.4917 sec/batch\n",
      "Epoch 32/40  Iteration 8769/11040 Training loss: 1.1913 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8770/11040 Training loss: 1.1914 0.4788 sec/batch\n",
      "Epoch 32/40  Iteration 8771/11040 Training loss: 1.1912 0.4667 sec/batch\n",
      "Epoch 32/40  Iteration 8772/11040 Training loss: 1.1911 0.4735 sec/batch\n",
      "Epoch 32/40  Iteration 8773/11040 Training loss: 1.1910 0.4760 sec/batch\n",
      "Epoch 32/40  Iteration 8774/11040 Training loss: 1.1909 0.4854 sec/batch\n",
      "Epoch 32/40  Iteration 8775/11040 Training loss: 1.1908 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8776/11040 Training loss: 1.1907 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8777/11040 Training loss: 1.1906 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8778/11040 Training loss: 1.1905 0.4769 sec/batch\n",
      "Epoch 32/40  Iteration 8779/11040 Training loss: 1.1904 0.4752 sec/batch\n",
      "Epoch 32/40  Iteration 8780/11040 Training loss: 1.1905 0.4610 sec/batch\n",
      "Epoch 32/40  Iteration 8781/11040 Training loss: 1.1905 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8782/11040 Training loss: 1.1905 0.4919 sec/batch\n",
      "Epoch 32/40  Iteration 8783/11040 Training loss: 1.1904 0.4732 sec/batch\n",
      "Epoch 32/40  Iteration 8784/11040 Training loss: 1.1903 0.4762 sec/batch\n",
      "Epoch 32/40  Iteration 8785/11040 Training loss: 1.1903 0.4763 sec/batch\n",
      "Epoch 32/40  Iteration 8786/11040 Training loss: 1.1904 0.4745 sec/batch\n",
      "Epoch 32/40  Iteration 8787/11040 Training loss: 1.1905 0.4750 sec/batch\n",
      "Epoch 32/40  Iteration 8788/11040 Training loss: 1.1905 0.4905 sec/batch\n",
      "Epoch 32/40  Iteration 8789/11040 Training loss: 1.1905 0.4748 sec/batch\n",
      "Epoch 32/40  Iteration 8790/11040 Training loss: 1.1904 0.4757 sec/batch\n",
      "Epoch 32/40  Iteration 8791/11040 Training loss: 1.1905 0.4906 sec/batch\n",
      "Epoch 32/40  Iteration 8792/11040 Training loss: 1.1906 0.4771 sec/batch\n",
      "Epoch 32/40  Iteration 8793/11040 Training loss: 1.1906 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8794/11040 Training loss: 1.1906 0.4773 sec/batch\n",
      "Epoch 32/40  Iteration 8795/11040 Training loss: 1.1908 0.4868 sec/batch\n",
      "Epoch 32/40  Iteration 8796/11040 Training loss: 1.1909 0.4778 sec/batch\n",
      "Epoch 32/40  Iteration 8797/11040 Training loss: 1.1910 0.4749 sec/batch\n",
      "Epoch 32/40  Iteration 8798/11040 Training loss: 1.1910 0.4774 sec/batch\n",
      "Epoch 32/40  Iteration 8799/11040 Training loss: 1.1910 0.4739 sec/batch\n",
      "Epoch 32/40  Iteration 8800/11040 Training loss: 1.1911 0.4919 sec/batch\n",
      "Epoch 32/40  Iteration 8801/11040 Training loss: 1.1910 0.4740 sec/batch\n",
      "Epoch 32/40  Iteration 8802/11040 Training loss: 1.1910 0.4748 sec/batch\n",
      "Epoch 32/40  Iteration 8803/11040 Training loss: 1.1911 0.4746 sec/batch\n",
      "Epoch 32/40  Iteration 8804/11040 Training loss: 1.1910 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8805/11040 Training loss: 1.1910 0.4861 sec/batch\n",
      "Epoch 32/40  Iteration 8806/11040 Training loss: 1.1911 0.4755 sec/batch\n",
      "Epoch 32/40  Iteration 8807/11040 Training loss: 1.1910 0.4771 sec/batch\n",
      "Epoch 32/40  Iteration 8808/11040 Training loss: 1.1910 0.4743 sec/batch\n",
      "Epoch 32/40  Iteration 8809/11040 Training loss: 1.1911 0.4770 sec/batch\n",
      "Epoch 32/40  Iteration 8810/11040 Training loss: 1.1911 0.4930 sec/batch\n",
      "Epoch 32/40  Iteration 8811/11040 Training loss: 1.1911 0.4735 sec/batch\n",
      "Epoch 32/40  Iteration 8812/11040 Training loss: 1.1912 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8813/11040 Training loss: 1.1913 0.4907 sec/batch\n",
      "Epoch 32/40  Iteration 8814/11040 Training loss: 1.1914 0.4753 sec/batch\n",
      "Epoch 32/40  Iteration 8815/11040 Training loss: 1.1913 0.4747 sec/batch\n",
      "Epoch 32/40  Iteration 8816/11040 Training loss: 1.1914 0.4771 sec/batch\n",
      "Epoch 32/40  Iteration 8817/11040 Training loss: 1.1915 0.4753 sec/batch\n",
      "Epoch 32/40  Iteration 8818/11040 Training loss: 1.1917 0.4736 sec/batch\n",
      "Epoch 32/40  Iteration 8819/11040 Training loss: 1.1918 0.4766 sec/batch\n",
      "Epoch 32/40  Iteration 8820/11040 Training loss: 1.1919 0.4756 sec/batch\n",
      "Epoch 32/40  Iteration 8821/11040 Training loss: 1.1919 0.4758 sec/batch\n",
      "Epoch 32/40  Iteration 8822/11040 Training loss: 1.1920 0.4812 sec/batch\n",
      "Epoch 32/40  Iteration 8823/11040 Training loss: 1.1922 0.4696 sec/batch\n",
      "Epoch 32/40  Iteration 8824/11040 Training loss: 1.1923 0.4741 sec/batch\n",
      "Epoch 32/40  Iteration 8825/11040 Training loss: 1.1924 0.4772 sec/batch\n",
      "Epoch 32/40  Iteration 8826/11040 Training loss: 1.1924 0.4753 sec/batch\n",
      "Epoch 32/40  Iteration 8827/11040 Training loss: 1.1928 0.4765 sec/batch\n",
      "Epoch 32/40  Iteration 8828/11040 Training loss: 1.1930 0.4918 sec/batch\n",
      "Epoch 32/40  Iteration 8829/11040 Training loss: 1.1931 0.4732 sec/batch\n",
      "Epoch 32/40  Iteration 8830/11040 Training loss: 1.1932 0.4744 sec/batch\n",
      "Epoch 32/40  Iteration 8831/11040 Training loss: 1.1933 0.4923 sec/batch\n",
      "Epoch 32/40  Iteration 8832/11040 Training loss: 1.1934 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8833/11040 Training loss: 1.2763 0.4918 sec/batch\n",
      "Epoch 33/40  Iteration 8834/11040 Training loss: 1.2392 0.4771 sec/batch\n",
      "Epoch 33/40  Iteration 8835/11040 Training loss: 1.2355 0.4756 sec/batch\n",
      "Epoch 33/40  Iteration 8836/11040 Training loss: 1.2298 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 8837/11040 Training loss: 1.2279 0.4752 sec/batch\n",
      "Epoch 33/40  Iteration 8838/11040 Training loss: 1.2239 0.4766 sec/batch\n",
      "Epoch 33/40  Iteration 8839/11040 Training loss: 1.2136 0.4890 sec/batch\n",
      "Epoch 33/40  Iteration 8840/11040 Training loss: 1.2107 0.4743 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40  Iteration 8841/11040 Training loss: 1.2059 0.4769 sec/batch\n",
      "Epoch 33/40  Iteration 8842/11040 Training loss: 1.2050 0.4764 sec/batch\n",
      "Epoch 33/40  Iteration 8843/11040 Training loss: 1.2000 0.4925 sec/batch\n",
      "Epoch 33/40  Iteration 8844/11040 Training loss: 1.1991 0.4728 sec/batch\n",
      "Epoch 33/40  Iteration 8845/11040 Training loss: 1.1955 0.4761 sec/batch\n",
      "Epoch 33/40  Iteration 8846/11040 Training loss: 1.1930 0.4894 sec/batch\n",
      "Epoch 33/40  Iteration 8847/11040 Training loss: 1.1922 0.4772 sec/batch\n",
      "Epoch 33/40  Iteration 8848/11040 Training loss: 1.1917 0.4752 sec/batch\n",
      "Epoch 33/40  Iteration 8849/11040 Training loss: 1.1888 0.4760 sec/batch\n",
      "Epoch 33/40  Iteration 8850/11040 Training loss: 1.1867 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8851/11040 Training loss: 1.1859 0.4759 sec/batch\n",
      "Epoch 33/40  Iteration 8852/11040 Training loss: 1.1855 0.4759 sec/batch\n",
      "Epoch 33/40  Iteration 8853/11040 Training loss: 1.1871 0.4748 sec/batch\n",
      "Epoch 33/40  Iteration 8854/11040 Training loss: 1.1880 0.4749 sec/batch\n",
      "Epoch 33/40  Iteration 8855/11040 Training loss: 1.1878 0.4767 sec/batch\n",
      "Epoch 33/40  Iteration 8856/11040 Training loss: 1.1883 0.4929 sec/batch\n",
      "Epoch 33/40  Iteration 8857/11040 Training loss: 1.1876 0.4893 sec/batch\n",
      "Epoch 33/40  Iteration 8858/11040 Training loss: 1.1869 0.4923 sec/batch\n",
      "Epoch 33/40  Iteration 8859/11040 Training loss: 1.1865 0.4766 sec/batch\n",
      "Epoch 33/40  Iteration 8860/11040 Training loss: 1.1862 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8861/11040 Training loss: 1.1864 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 8862/11040 Training loss: 1.1868 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 8863/11040 Training loss: 1.1872 0.4749 sec/batch\n",
      "Epoch 33/40  Iteration 8864/11040 Training loss: 1.1868 0.4785 sec/batch\n",
      "Epoch 33/40  Iteration 8865/11040 Training loss: 1.1859 0.4911 sec/batch\n",
      "Epoch 33/40  Iteration 8866/11040 Training loss: 1.1855 0.4900 sec/batch\n",
      "Epoch 33/40  Iteration 8867/11040 Training loss: 1.1854 0.4892 sec/batch\n",
      "Epoch 33/40  Iteration 8868/11040 Training loss: 1.1860 0.4788 sec/batch\n",
      "Epoch 33/40  Iteration 8869/11040 Training loss: 1.1859 0.4918 sec/batch\n",
      "Epoch 33/40  Iteration 8870/11040 Training loss: 1.1853 0.4927 sec/batch\n",
      "Epoch 33/40  Iteration 8871/11040 Training loss: 1.1846 0.4900 sec/batch\n",
      "Epoch 33/40  Iteration 8872/11040 Training loss: 1.1847 0.4923 sec/batch\n",
      "Epoch 33/40  Iteration 8873/11040 Training loss: 1.1844 0.4886 sec/batch\n",
      "Epoch 33/40  Iteration 8874/11040 Training loss: 1.1834 0.4784 sec/batch\n",
      "Epoch 33/40  Iteration 8875/11040 Training loss: 1.1837 0.4890 sec/batch\n",
      "Epoch 33/40  Iteration 8876/11040 Training loss: 1.1830 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 8877/11040 Training loss: 1.1823 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8878/11040 Training loss: 1.1816 0.4615 sec/batch\n",
      "Epoch 33/40  Iteration 8879/11040 Training loss: 1.1809 0.4575 sec/batch\n",
      "Epoch 33/40  Iteration 8880/11040 Training loss: 1.1808 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 8881/11040 Training loss: 1.1807 0.4766 sec/batch\n",
      "Epoch 33/40  Iteration 8882/11040 Training loss: 1.1812 0.4740 sec/batch\n",
      "Epoch 33/40  Iteration 8883/11040 Training loss: 1.1810 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8884/11040 Training loss: 1.1809 0.4751 sec/batch\n",
      "Epoch 33/40  Iteration 8885/11040 Training loss: 1.1812 0.4760 sec/batch\n",
      "Epoch 33/40  Iteration 8886/11040 Training loss: 1.1806 0.4587 sec/batch\n",
      "Epoch 33/40  Iteration 8887/11040 Training loss: 1.1805 0.4593 sec/batch\n",
      "Epoch 33/40  Iteration 8888/11040 Training loss: 1.1805 0.4591 sec/batch\n",
      "Epoch 33/40  Iteration 8889/11040 Training loss: 1.1804 0.4596 sec/batch\n",
      "Epoch 33/40  Iteration 8890/11040 Training loss: 1.1802 0.4575 sec/batch\n",
      "Epoch 33/40  Iteration 8891/11040 Training loss: 1.1805 0.4605 sec/batch\n",
      "Epoch 33/40  Iteration 8892/11040 Training loss: 1.1801 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 8893/11040 Training loss: 1.1801 0.4747 sec/batch\n",
      "Epoch 33/40  Iteration 8894/11040 Training loss: 1.1802 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 8895/11040 Training loss: 1.1805 0.4748 sec/batch\n",
      "Epoch 33/40  Iteration 8896/11040 Training loss: 1.1806 0.4748 sec/batch\n",
      "Epoch 33/40  Iteration 8897/11040 Training loss: 1.1808 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 8898/11040 Training loss: 1.1807 0.4762 sec/batch\n",
      "Epoch 33/40  Iteration 8899/11040 Training loss: 1.1807 0.4740 sec/batch\n",
      "Epoch 33/40  Iteration 8900/11040 Training loss: 1.1808 0.4597 sec/batch\n",
      "Epoch 33/40  Iteration 8901/11040 Training loss: 1.1807 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 8902/11040 Training loss: 1.1806 0.4769 sec/batch\n",
      "Epoch 33/40  Iteration 8903/11040 Training loss: 1.1802 0.4722 sec/batch\n",
      "Epoch 33/40  Iteration 8904/11040 Training loss: 1.1797 0.4740 sec/batch\n",
      "Epoch 33/40  Iteration 8905/11040 Training loss: 1.1793 0.4760 sec/batch\n",
      "Epoch 33/40  Iteration 8906/11040 Training loss: 1.1789 0.4760 sec/batch\n",
      "Epoch 33/40  Iteration 8907/11040 Training loss: 1.1788 0.4719 sec/batch\n",
      "Epoch 33/40  Iteration 8908/11040 Training loss: 1.1792 0.4761 sec/batch\n",
      "Epoch 33/40  Iteration 8909/11040 Training loss: 1.1787 0.4730 sec/batch\n",
      "Epoch 33/40  Iteration 8910/11040 Training loss: 1.1783 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 8911/11040 Training loss: 1.1783 0.4747 sec/batch\n",
      "Epoch 33/40  Iteration 8912/11040 Training loss: 1.1783 0.4733 sec/batch\n",
      "Epoch 33/40  Iteration 8913/11040 Training loss: 1.1781 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 8914/11040 Training loss: 1.1783 0.4732 sec/batch\n",
      "Epoch 33/40  Iteration 8915/11040 Training loss: 1.1784 0.4762 sec/batch\n",
      "Epoch 33/40  Iteration 8916/11040 Training loss: 1.1783 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8917/11040 Training loss: 1.1780 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 8918/11040 Training loss: 1.1779 0.4756 sec/batch\n",
      "Epoch 33/40  Iteration 8919/11040 Training loss: 1.1773 0.4763 sec/batch\n",
      "Epoch 33/40  Iteration 8920/11040 Training loss: 1.1763 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 8921/11040 Training loss: 1.1762 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8922/11040 Training loss: 1.1762 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 8923/11040 Training loss: 1.1762 0.4616 sec/batch\n",
      "Epoch 33/40  Iteration 8924/11040 Training loss: 1.1761 0.4733 sec/batch\n",
      "Epoch 33/40  Iteration 8925/11040 Training loss: 1.1759 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8926/11040 Training loss: 1.1764 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8927/11040 Training loss: 1.1768 0.4749 sec/batch\n",
      "Epoch 33/40  Iteration 8928/11040 Training loss: 1.1772 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 8929/11040 Training loss: 1.1776 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8930/11040 Training loss: 1.1780 0.4761 sec/batch\n",
      "Epoch 33/40  Iteration 8931/11040 Training loss: 1.1784 0.4750 sec/batch\n",
      "Epoch 33/40  Iteration 8932/11040 Training loss: 1.1782 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 8933/11040 Training loss: 1.1783 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 8934/11040 Training loss: 1.1785 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 8935/11040 Training loss: 1.1781 0.4780 sec/batch\n",
      "Epoch 33/40  Iteration 8936/11040 Training loss: 1.1783 0.4716 sec/batch\n",
      "Epoch 33/40  Iteration 8937/11040 Training loss: 1.1782 0.4770 sec/batch\n",
      "Epoch 33/40  Iteration 8938/11040 Training loss: 1.1780 0.4735 sec/batch\n",
      "Epoch 33/40  Iteration 8939/11040 Training loss: 1.1779 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 8940/11040 Training loss: 1.1780 0.4765 sec/batch\n",
      "Epoch 33/40  Iteration 8941/11040 Training loss: 1.1783 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 8942/11040 Training loss: 1.1783 0.4759 sec/batch\n",
      "Epoch 33/40  Iteration 8943/11040 Training loss: 1.1783 0.4901 sec/batch\n",
      "Epoch 33/40  Iteration 8944/11040 Training loss: 1.1785 0.4758 sec/batch\n",
      "Epoch 33/40  Iteration 8945/11040 Training loss: 1.1787 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 8946/11040 Training loss: 1.1787 0.4587 sec/batch\n",
      "Epoch 33/40  Iteration 8947/11040 Training loss: 1.1790 0.4759 sec/batch\n",
      "Epoch 33/40  Iteration 8948/11040 Training loss: 1.1791 0.4732 sec/batch\n",
      "Epoch 33/40  Iteration 8949/11040 Training loss: 1.1790 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8950/11040 Training loss: 1.1793 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8951/11040 Training loss: 1.1794 0.4622 sec/batch\n",
      "Epoch 33/40  Iteration 8952/11040 Training loss: 1.1795 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 8953/11040 Training loss: 1.1795 0.4724 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40  Iteration 8954/11040 Training loss: 1.1797 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8955/11040 Training loss: 1.1800 0.4766 sec/batch\n",
      "Epoch 33/40  Iteration 8956/11040 Training loss: 1.1805 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 8957/11040 Training loss: 1.1806 0.4607 sec/batch\n",
      "Epoch 33/40  Iteration 8958/11040 Training loss: 1.1808 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 8959/11040 Training loss: 1.1808 0.4738 sec/batch\n",
      "Epoch 33/40  Iteration 8960/11040 Training loss: 1.1809 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 8961/11040 Training loss: 1.1807 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 8962/11040 Training loss: 1.1810 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8963/11040 Training loss: 1.1809 0.4732 sec/batch\n",
      "Epoch 33/40  Iteration 8964/11040 Training loss: 1.1808 0.4603 sec/batch\n",
      "Epoch 33/40  Iteration 8965/11040 Training loss: 1.1808 0.4771 sec/batch\n",
      "Epoch 33/40  Iteration 8966/11040 Training loss: 1.1806 0.4719 sec/batch\n",
      "Epoch 33/40  Iteration 8967/11040 Training loss: 1.1807 0.4855 sec/batch\n",
      "Epoch 33/40  Iteration 8968/11040 Training loss: 1.1807 0.4651 sec/batch\n",
      "Epoch 33/40  Iteration 8969/11040 Training loss: 1.1806 0.4757 sec/batch\n",
      "Epoch 33/40  Iteration 8970/11040 Training loss: 1.1808 0.4734 sec/batch\n",
      "Epoch 33/40  Iteration 8971/11040 Training loss: 1.1809 0.4607 sec/batch\n",
      "Epoch 33/40  Iteration 8972/11040 Training loss: 1.1811 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8973/11040 Training loss: 1.1812 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8974/11040 Training loss: 1.1814 0.4598 sec/batch\n",
      "Epoch 33/40  Iteration 8975/11040 Training loss: 1.1814 0.4774 sec/batch\n",
      "Epoch 33/40  Iteration 8976/11040 Training loss: 1.1813 0.4714 sec/batch\n",
      "Epoch 33/40  Iteration 8977/11040 Training loss: 1.1813 0.4808 sec/batch\n",
      "Epoch 33/40  Iteration 8978/11040 Training loss: 1.1813 0.4700 sec/batch\n",
      "Epoch 33/40  Iteration 8979/11040 Training loss: 1.1814 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8980/11040 Training loss: 1.1814 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8981/11040 Training loss: 1.1813 0.4753 sec/batch\n",
      "Epoch 33/40  Iteration 8982/11040 Training loss: 1.1813 0.4896 sec/batch\n",
      "Epoch 33/40  Iteration 8983/11040 Training loss: 1.1813 0.4752 sec/batch\n",
      "Epoch 33/40  Iteration 8984/11040 Training loss: 1.1815 0.4731 sec/batch\n",
      "Epoch 33/40  Iteration 8985/11040 Training loss: 1.1816 0.4783 sec/batch\n",
      "Epoch 33/40  Iteration 8986/11040 Training loss: 1.1818 0.4626 sec/batch\n",
      "Epoch 33/40  Iteration 8987/11040 Training loss: 1.1819 0.4706 sec/batch\n",
      "Epoch 33/40  Iteration 8988/11040 Training loss: 1.1819 0.4739 sec/batch\n",
      "Epoch 33/40  Iteration 8989/11040 Training loss: 1.1818 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 8990/11040 Training loss: 1.1820 0.4759 sec/batch\n",
      "Epoch 33/40  Iteration 8991/11040 Training loss: 1.1822 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 8992/11040 Training loss: 1.1822 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 8993/11040 Training loss: 1.1823 0.4766 sec/batch\n",
      "Epoch 33/40  Iteration 8994/11040 Training loss: 1.1823 0.4893 sec/batch\n",
      "Epoch 33/40  Iteration 8995/11040 Training loss: 1.1825 0.4771 sec/batch\n",
      "Epoch 33/40  Iteration 8996/11040 Training loss: 1.1826 0.4762 sec/batch\n",
      "Epoch 33/40  Iteration 8997/11040 Training loss: 1.1828 0.4733 sec/batch\n",
      "Epoch 33/40  Iteration 8998/11040 Training loss: 1.1831 0.4796 sec/batch\n",
      "Epoch 33/40  Iteration 8999/11040 Training loss: 1.1832 0.4710 sec/batch\n",
      "Epoch 33/40  Iteration 9000/11040 Training loss: 1.1832 0.4741 sec/batch\n",
      "Validation loss: 1.24118 Saving checkpoint!\n",
      "Epoch 33/40  Iteration 9001/11040 Training loss: 1.1842 0.4688 sec/batch\n",
      "Epoch 33/40  Iteration 9002/11040 Training loss: 1.1842 0.4697 sec/batch\n",
      "Epoch 33/40  Iteration 9003/11040 Training loss: 1.1844 0.4855 sec/batch\n",
      "Epoch 33/40  Iteration 9004/11040 Training loss: 1.1846 0.4781 sec/batch\n",
      "Epoch 33/40  Iteration 9005/11040 Training loss: 1.1846 0.4730 sec/batch\n",
      "Epoch 33/40  Iteration 9006/11040 Training loss: 1.1846 0.4603 sec/batch\n",
      "Epoch 33/40  Iteration 9007/11040 Training loss: 1.1846 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 9008/11040 Training loss: 1.1846 0.4749 sec/batch\n",
      "Epoch 33/40  Iteration 9009/11040 Training loss: 1.1847 0.4747 sec/batch\n",
      "Epoch 33/40  Iteration 9010/11040 Training loss: 1.1847 0.4752 sec/batch\n",
      "Epoch 33/40  Iteration 9011/11040 Training loss: 1.1850 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 9012/11040 Training loss: 1.1851 0.4604 sec/batch\n",
      "Epoch 33/40  Iteration 9013/11040 Training loss: 1.1851 0.4578 sec/batch\n",
      "Epoch 33/40  Iteration 9014/11040 Training loss: 1.1851 0.4586 sec/batch\n",
      "Epoch 33/40  Iteration 9015/11040 Training loss: 1.1850 0.4752 sec/batch\n",
      "Epoch 33/40  Iteration 9016/11040 Training loss: 1.1851 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 9017/11040 Training loss: 1.1852 0.4599 sec/batch\n",
      "Epoch 33/40  Iteration 9018/11040 Training loss: 1.1853 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 9019/11040 Training loss: 1.1855 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 9020/11040 Training loss: 1.1855 0.4615 sec/batch\n",
      "Epoch 33/40  Iteration 9021/11040 Training loss: 1.1855 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 9022/11040 Training loss: 1.1855 0.4756 sec/batch\n",
      "Epoch 33/40  Iteration 9023/11040 Training loss: 1.1854 0.4757 sec/batch\n",
      "Epoch 33/40  Iteration 9024/11040 Training loss: 1.1855 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 9025/11040 Training loss: 1.1855 0.4597 sec/batch\n",
      "Epoch 33/40  Iteration 9026/11040 Training loss: 1.1855 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 9027/11040 Training loss: 1.1858 0.4759 sec/batch\n",
      "Epoch 33/40  Iteration 9028/11040 Training loss: 1.1856 0.4728 sec/batch\n",
      "Epoch 33/40  Iteration 9029/11040 Training loss: 1.1858 0.4610 sec/batch\n",
      "Epoch 33/40  Iteration 9030/11040 Training loss: 1.1858 0.4747 sec/batch\n",
      "Epoch 33/40  Iteration 9031/11040 Training loss: 1.1860 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 9032/11040 Training loss: 1.1861 0.4598 sec/batch\n",
      "Epoch 33/40  Iteration 9033/11040 Training loss: 1.1863 0.4669 sec/batch\n",
      "Epoch 33/40  Iteration 9034/11040 Training loss: 1.1865 0.4693 sec/batch\n",
      "Epoch 33/40  Iteration 9035/11040 Training loss: 1.1866 0.4891 sec/batch\n",
      "Epoch 33/40  Iteration 9036/11040 Training loss: 1.1867 0.4637 sec/batch\n",
      "Epoch 33/40  Iteration 9037/11040 Training loss: 1.1866 0.4707 sec/batch\n",
      "Epoch 33/40  Iteration 9038/11040 Training loss: 1.1866 0.4733 sec/batch\n",
      "Epoch 33/40  Iteration 9039/11040 Training loss: 1.1867 0.4762 sec/batch\n",
      "Epoch 33/40  Iteration 9040/11040 Training loss: 1.1868 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 9041/11040 Training loss: 1.1866 0.4762 sec/batch\n",
      "Epoch 33/40  Iteration 9042/11040 Training loss: 1.1865 0.4586 sec/batch\n",
      "Epoch 33/40  Iteration 9043/11040 Training loss: 1.1866 0.4740 sec/batch\n",
      "Epoch 33/40  Iteration 9044/11040 Training loss: 1.1865 0.4754 sec/batch\n",
      "Epoch 33/40  Iteration 9045/11040 Training loss: 1.1865 0.4761 sec/batch\n",
      "Epoch 33/40  Iteration 9046/11040 Training loss: 1.1865 0.4732 sec/batch\n",
      "Epoch 33/40  Iteration 9047/11040 Training loss: 1.1864 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 9048/11040 Training loss: 1.1863 0.4764 sec/batch\n",
      "Epoch 33/40  Iteration 9049/11040 Training loss: 1.1862 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 9050/11040 Training loss: 1.1860 0.4768 sec/batch\n",
      "Epoch 33/40  Iteration 9051/11040 Training loss: 1.1860 0.4729 sec/batch\n",
      "Epoch 33/40  Iteration 9052/11040 Training loss: 1.1859 0.4606 sec/batch\n",
      "Epoch 33/40  Iteration 9053/11040 Training loss: 1.1858 0.4732 sec/batch\n",
      "Epoch 33/40  Iteration 9054/11040 Training loss: 1.1856 0.4776 sec/batch\n",
      "Epoch 33/40  Iteration 9055/11040 Training loss: 1.1856 0.4758 sec/batch\n",
      "Epoch 33/40  Iteration 9056/11040 Training loss: 1.1856 0.4729 sec/batch\n",
      "Epoch 33/40  Iteration 9057/11040 Training loss: 1.1856 0.4745 sec/batch\n",
      "Epoch 33/40  Iteration 9058/11040 Training loss: 1.1856 0.4742 sec/batch\n",
      "Epoch 33/40  Iteration 9059/11040 Training loss: 1.1854 0.4660 sec/batch\n",
      "Epoch 33/40  Iteration 9060/11040 Training loss: 1.1854 0.4701 sec/batch\n",
      "Epoch 33/40  Iteration 9061/11040 Training loss: 1.1854 0.4753 sec/batch\n",
      "Epoch 33/40  Iteration 9062/11040 Training loss: 1.1855 0.4866 sec/batch\n",
      "Epoch 33/40  Iteration 9063/11040 Training loss: 1.1856 0.4834 sec/batch\n",
      "Epoch 33/40  Iteration 9064/11040 Training loss: 1.1856 0.4713 sec/batch\n",
      "Epoch 33/40  Iteration 9065/11040 Training loss: 1.1856 0.4751 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40  Iteration 9066/11040 Training loss: 1.1855 0.4761 sec/batch\n",
      "Epoch 33/40  Iteration 9067/11040 Training loss: 1.1856 0.4890 sec/batch\n",
      "Epoch 33/40  Iteration 9068/11040 Training loss: 1.1857 0.4697 sec/batch\n",
      "Epoch 33/40  Iteration 9069/11040 Training loss: 1.1857 0.4797 sec/batch\n",
      "Epoch 33/40  Iteration 9070/11040 Training loss: 1.1858 0.4857 sec/batch\n",
      "Epoch 33/40  Iteration 9071/11040 Training loss: 1.1859 0.4763 sec/batch\n",
      "Epoch 33/40  Iteration 9072/11040 Training loss: 1.1860 0.4752 sec/batch\n",
      "Epoch 33/40  Iteration 9073/11040 Training loss: 1.1860 0.4734 sec/batch\n",
      "Epoch 33/40  Iteration 9074/11040 Training loss: 1.1861 0.4747 sec/batch\n",
      "Epoch 33/40  Iteration 9075/11040 Training loss: 1.1860 0.4757 sec/batch\n",
      "Epoch 33/40  Iteration 9076/11040 Training loss: 1.1861 0.4757 sec/batch\n",
      "Epoch 33/40  Iteration 9077/11040 Training loss: 1.1860 0.4751 sec/batch\n",
      "Epoch 33/40  Iteration 9078/11040 Training loss: 1.1861 0.4750 sec/batch\n",
      "Epoch 33/40  Iteration 9079/11040 Training loss: 1.1860 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 9080/11040 Training loss: 1.1861 0.4777 sec/batch\n",
      "Epoch 33/40  Iteration 9081/11040 Training loss: 1.1861 0.4891 sec/batch\n",
      "Epoch 33/40  Iteration 9082/11040 Training loss: 1.1861 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 9083/11040 Training loss: 1.1860 0.4762 sec/batch\n",
      "Epoch 33/40  Iteration 9084/11040 Training loss: 1.1860 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 9085/11040 Training loss: 1.1861 0.4596 sec/batch\n",
      "Epoch 33/40  Iteration 9086/11040 Training loss: 1.1861 0.4583 sec/batch\n",
      "Epoch 33/40  Iteration 9087/11040 Training loss: 1.1862 0.4744 sec/batch\n",
      "Epoch 33/40  Iteration 9088/11040 Training loss: 1.1862 0.4764 sec/batch\n",
      "Epoch 33/40  Iteration 9089/11040 Training loss: 1.1863 0.4734 sec/batch\n",
      "Epoch 33/40  Iteration 9090/11040 Training loss: 1.1864 0.4768 sec/batch\n",
      "Epoch 33/40  Iteration 9091/11040 Training loss: 1.1864 0.4735 sec/batch\n",
      "Epoch 33/40  Iteration 9092/11040 Training loss: 1.1864 0.4595 sec/batch\n",
      "Epoch 33/40  Iteration 9093/11040 Training loss: 1.1865 0.4609 sec/batch\n",
      "Epoch 33/40  Iteration 9094/11040 Training loss: 1.1867 0.4723 sec/batch\n",
      "Epoch 33/40  Iteration 9095/11040 Training loss: 1.1869 0.4758 sec/batch\n",
      "Epoch 33/40  Iteration 9096/11040 Training loss: 1.1870 0.4731 sec/batch\n",
      "Epoch 33/40  Iteration 9097/11040 Training loss: 1.1870 0.4755 sec/batch\n",
      "Epoch 33/40  Iteration 9098/11040 Training loss: 1.1871 0.4741 sec/batch\n",
      "Epoch 33/40  Iteration 9099/11040 Training loss: 1.1873 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 9100/11040 Training loss: 1.1874 0.4715 sec/batch\n",
      "Epoch 33/40  Iteration 9101/11040 Training loss: 1.1875 0.4743 sec/batch\n",
      "Epoch 33/40  Iteration 9102/11040 Training loss: 1.1876 0.4773 sec/batch\n",
      "Epoch 33/40  Iteration 9103/11040 Training loss: 1.1880 0.4731 sec/batch\n",
      "Epoch 33/40  Iteration 9104/11040 Training loss: 1.1882 0.4594 sec/batch\n",
      "Epoch 33/40  Iteration 9105/11040 Training loss: 1.1883 0.4760 sec/batch\n",
      "Epoch 33/40  Iteration 9106/11040 Training loss: 1.1883 0.4746 sec/batch\n",
      "Epoch 33/40  Iteration 9107/11040 Training loss: 1.1884 0.4733 sec/batch\n",
      "Epoch 33/40  Iteration 9108/11040 Training loss: 1.1885 0.4756 sec/batch\n",
      "Epoch 34/40  Iteration 9109/11040 Training loss: 1.2743 0.4732 sec/batch\n",
      "Epoch 34/40  Iteration 9110/11040 Training loss: 1.2348 0.4604 sec/batch\n",
      "Epoch 34/40  Iteration 9111/11040 Training loss: 1.2283 0.4585 sec/batch\n",
      "Epoch 34/40  Iteration 9112/11040 Training loss: 1.2254 0.4767 sec/batch\n",
      "Epoch 34/40  Iteration 9113/11040 Training loss: 1.2234 0.4586 sec/batch\n",
      "Epoch 34/40  Iteration 9114/11040 Training loss: 1.2196 0.4747 sec/batch\n",
      "Epoch 34/40  Iteration 9115/11040 Training loss: 1.2111 0.4604 sec/batch\n",
      "Epoch 34/40  Iteration 9116/11040 Training loss: 1.2078 0.4742 sec/batch\n",
      "Epoch 34/40  Iteration 9117/11040 Training loss: 1.2023 0.4751 sec/batch\n",
      "Epoch 34/40  Iteration 9118/11040 Training loss: 1.2012 0.4766 sec/batch\n",
      "Epoch 34/40  Iteration 9119/11040 Training loss: 1.1971 0.4746 sec/batch\n",
      "Epoch 34/40  Iteration 9120/11040 Training loss: 1.1961 0.4762 sec/batch\n",
      "Epoch 34/40  Iteration 9121/11040 Training loss: 1.1931 0.4736 sec/batch\n",
      "Epoch 34/40  Iteration 9122/11040 Training loss: 1.1902 0.4746 sec/batch\n",
      "Epoch 34/40  Iteration 9123/11040 Training loss: 1.1887 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9124/11040 Training loss: 1.1885 0.4754 sec/batch\n",
      "Epoch 34/40  Iteration 9125/11040 Training loss: 1.1857 0.4749 sec/batch\n",
      "Epoch 34/40  Iteration 9126/11040 Training loss: 1.1838 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9127/11040 Training loss: 1.1819 0.4613 sec/batch\n",
      "Epoch 34/40  Iteration 9128/11040 Training loss: 1.1818 0.4735 sec/batch\n",
      "Epoch 34/40  Iteration 9129/11040 Training loss: 1.1835 0.4753 sec/batch\n",
      "Epoch 34/40  Iteration 9130/11040 Training loss: 1.1839 0.4783 sec/batch\n",
      "Epoch 34/40  Iteration 9131/11040 Training loss: 1.1837 0.4887 sec/batch\n",
      "Epoch 34/40  Iteration 9132/11040 Training loss: 1.1840 0.4817 sec/batch\n",
      "Epoch 34/40  Iteration 9133/11040 Training loss: 1.1832 0.4829 sec/batch\n",
      "Epoch 34/40  Iteration 9134/11040 Training loss: 1.1825 0.4752 sec/batch\n",
      "Epoch 34/40  Iteration 9135/11040 Training loss: 1.1819 0.4922 sec/batch\n",
      "Epoch 34/40  Iteration 9136/11040 Training loss: 1.1821 0.4743 sec/batch\n",
      "Epoch 34/40  Iteration 9137/11040 Training loss: 1.1821 0.4742 sec/batch\n",
      "Epoch 34/40  Iteration 9138/11040 Training loss: 1.1822 0.4905 sec/batch\n",
      "Epoch 34/40  Iteration 9139/11040 Training loss: 1.1821 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9140/11040 Training loss: 1.1815 0.4762 sec/batch\n",
      "Epoch 34/40  Iteration 9141/11040 Training loss: 1.1809 0.4751 sec/batch\n",
      "Epoch 34/40  Iteration 9142/11040 Training loss: 1.1807 0.4737 sec/batch\n",
      "Epoch 34/40  Iteration 9143/11040 Training loss: 1.1805 0.4765 sec/batch\n",
      "Epoch 34/40  Iteration 9144/11040 Training loss: 1.1808 0.4753 sec/batch\n",
      "Epoch 34/40  Iteration 9145/11040 Training loss: 1.1807 0.4755 sec/batch\n",
      "Epoch 34/40  Iteration 9146/11040 Training loss: 1.1802 0.4735 sec/batch\n",
      "Epoch 34/40  Iteration 9147/11040 Training loss: 1.1794 0.4765 sec/batch\n",
      "Epoch 34/40  Iteration 9148/11040 Training loss: 1.1795 0.4755 sec/batch\n",
      "Epoch 34/40  Iteration 9149/11040 Training loss: 1.1792 0.4905 sec/batch\n",
      "Epoch 34/40  Iteration 9150/11040 Training loss: 1.1783 0.4741 sec/batch\n",
      "Epoch 34/40  Iteration 9151/11040 Training loss: 1.1786 0.4765 sec/batch\n",
      "Epoch 34/40  Iteration 9152/11040 Training loss: 1.1776 0.4912 sec/batch\n",
      "Epoch 34/40  Iteration 9153/11040 Training loss: 1.1771 0.4747 sec/batch\n",
      "Epoch 34/40  Iteration 9154/11040 Training loss: 1.1764 0.4770 sec/batch\n",
      "Epoch 34/40  Iteration 9155/11040 Training loss: 1.1758 0.4729 sec/batch\n",
      "Epoch 34/40  Iteration 9156/11040 Training loss: 1.1758 0.4774 sec/batch\n",
      "Epoch 34/40  Iteration 9157/11040 Training loss: 1.1757 0.4728 sec/batch\n",
      "Epoch 34/40  Iteration 9158/11040 Training loss: 1.1762 0.4775 sec/batch\n",
      "Epoch 34/40  Iteration 9159/11040 Training loss: 1.1762 0.4902 sec/batch\n",
      "Epoch 34/40  Iteration 9160/11040 Training loss: 1.1762 0.4756 sec/batch\n",
      "Epoch 34/40  Iteration 9161/11040 Training loss: 1.1764 0.4754 sec/batch\n",
      "Epoch 34/40  Iteration 9162/11040 Training loss: 1.1762 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9163/11040 Training loss: 1.1760 0.4754 sec/batch\n",
      "Epoch 34/40  Iteration 9164/11040 Training loss: 1.1759 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9165/11040 Training loss: 1.1757 0.4750 sec/batch\n",
      "Epoch 34/40  Iteration 9166/11040 Training loss: 1.1754 0.4909 sec/batch\n",
      "Epoch 34/40  Iteration 9167/11040 Training loss: 1.1755 0.4738 sec/batch\n",
      "Epoch 34/40  Iteration 9168/11040 Training loss: 1.1751 0.4752 sec/batch\n",
      "Epoch 34/40  Iteration 9169/11040 Training loss: 1.1750 0.4905 sec/batch\n",
      "Epoch 34/40  Iteration 9170/11040 Training loss: 1.1752 0.4775 sec/batch\n",
      "Epoch 34/40  Iteration 9171/11040 Training loss: 1.1755 0.4743 sec/batch\n",
      "Epoch 34/40  Iteration 9172/11040 Training loss: 1.1757 0.4749 sec/batch\n",
      "Epoch 34/40  Iteration 9173/11040 Training loss: 1.1760 0.4772 sec/batch\n",
      "Epoch 34/40  Iteration 9174/11040 Training loss: 1.1759 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9175/11040 Training loss: 1.1758 0.4772 sec/batch\n",
      "Epoch 34/40  Iteration 9176/11040 Training loss: 1.1759 0.4733 sec/batch\n",
      "Epoch 34/40  Iteration 9177/11040 Training loss: 1.1756 0.4755 sec/batch\n",
      "Epoch 34/40  Iteration 9178/11040 Training loss: 1.1757 0.4762 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40  Iteration 9179/11040 Training loss: 1.1752 0.4735 sec/batch\n",
      "Epoch 34/40  Iteration 9180/11040 Training loss: 1.1748 0.4615 sec/batch\n",
      "Epoch 34/40  Iteration 9181/11040 Training loss: 1.1743 0.4737 sec/batch\n",
      "Epoch 34/40  Iteration 9182/11040 Training loss: 1.1740 0.4766 sec/batch\n",
      "Epoch 34/40  Iteration 9183/11040 Training loss: 1.1739 0.4750 sec/batch\n",
      "Epoch 34/40  Iteration 9184/11040 Training loss: 1.1742 0.4763 sec/batch\n",
      "Epoch 34/40  Iteration 9185/11040 Training loss: 1.1738 0.4726 sec/batch\n",
      "Epoch 34/40  Iteration 9186/11040 Training loss: 1.1734 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9187/11040 Training loss: 1.1732 0.4749 sec/batch\n",
      "Epoch 34/40  Iteration 9188/11040 Training loss: 1.1732 0.4758 sec/batch\n",
      "Epoch 34/40  Iteration 9189/11040 Training loss: 1.1728 0.4613 sec/batch\n",
      "Epoch 34/40  Iteration 9190/11040 Training loss: 1.1730 0.4738 sec/batch\n",
      "Epoch 34/40  Iteration 9191/11040 Training loss: 1.1731 0.4767 sec/batch\n",
      "Epoch 34/40  Iteration 9192/11040 Training loss: 1.1729 0.4738 sec/batch\n",
      "Epoch 34/40  Iteration 9193/11040 Training loss: 1.1728 0.4737 sec/batch\n",
      "Epoch 34/40  Iteration 9194/11040 Training loss: 1.1726 0.4771 sec/batch\n",
      "Epoch 34/40  Iteration 9195/11040 Training loss: 1.1720 0.4913 sec/batch\n",
      "Epoch 34/40  Iteration 9196/11040 Training loss: 1.1710 0.4736 sec/batch\n",
      "Epoch 34/40  Iteration 9197/11040 Training loss: 1.1709 0.4727 sec/batch\n",
      "Epoch 34/40  Iteration 9198/11040 Training loss: 1.1710 0.4896 sec/batch\n",
      "Epoch 34/40  Iteration 9199/11040 Training loss: 1.1711 0.4768 sec/batch\n",
      "Epoch 34/40  Iteration 9200/11040 Training loss: 1.1709 0.4768 sec/batch\n",
      "Epoch 34/40  Iteration 9201/11040 Training loss: 1.1707 0.4741 sec/batch\n",
      "Epoch 34/40  Iteration 9202/11040 Training loss: 1.1710 0.4752 sec/batch\n",
      "Epoch 34/40  Iteration 9203/11040 Training loss: 1.1715 0.4914 sec/batch\n",
      "Epoch 34/40  Iteration 9204/11040 Training loss: 1.1718 0.4874 sec/batch\n",
      "Epoch 34/40  Iteration 9205/11040 Training loss: 1.1723 0.4919 sec/batch\n",
      "Epoch 34/40  Iteration 9206/11040 Training loss: 1.1728 0.4771 sec/batch\n",
      "Epoch 34/40  Iteration 9207/11040 Training loss: 1.1731 0.4761 sec/batch\n",
      "Epoch 34/40  Iteration 9208/11040 Training loss: 1.1730 0.4738 sec/batch\n",
      "Epoch 34/40  Iteration 9209/11040 Training loss: 1.1731 0.4748 sec/batch\n",
      "Epoch 34/40  Iteration 9210/11040 Training loss: 1.1732 0.4770 sec/batch\n",
      "Epoch 34/40  Iteration 9211/11040 Training loss: 1.1728 0.4906 sec/batch\n",
      "Epoch 34/40  Iteration 9212/11040 Training loss: 1.1730 0.4774 sec/batch\n",
      "Epoch 34/40  Iteration 9213/11040 Training loss: 1.1729 0.4739 sec/batch\n",
      "Epoch 34/40  Iteration 9214/11040 Training loss: 1.1727 0.4629 sec/batch\n",
      "Epoch 34/40  Iteration 9215/11040 Training loss: 1.1727 0.4734 sec/batch\n",
      "Epoch 34/40  Iteration 9216/11040 Training loss: 1.1727 0.4749 sec/batch\n",
      "Epoch 34/40  Iteration 9217/11040 Training loss: 1.1732 0.4897 sec/batch\n",
      "Epoch 34/40  Iteration 9218/11040 Training loss: 1.1732 0.4762 sec/batch\n",
      "Epoch 34/40  Iteration 9219/11040 Training loss: 1.1732 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9220/11040 Training loss: 1.1733 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9221/11040 Training loss: 1.1733 0.4739 sec/batch\n",
      "Epoch 34/40  Iteration 9222/11040 Training loss: 1.1734 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9223/11040 Training loss: 1.1736 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9224/11040 Training loss: 1.1738 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9225/11040 Training loss: 1.1737 0.4741 sec/batch\n",
      "Epoch 34/40  Iteration 9226/11040 Training loss: 1.1741 0.4775 sec/batch\n",
      "Epoch 34/40  Iteration 9227/11040 Training loss: 1.1742 0.4917 sec/batch\n",
      "Epoch 34/40  Iteration 9228/11040 Training loss: 1.1742 0.4742 sec/batch\n",
      "Epoch 34/40  Iteration 9229/11040 Training loss: 1.1743 0.4745 sec/batch\n",
      "Epoch 34/40  Iteration 9230/11040 Training loss: 1.1743 0.4925 sec/batch\n",
      "Epoch 34/40  Iteration 9231/11040 Training loss: 1.1746 0.4739 sec/batch\n",
      "Epoch 34/40  Iteration 9232/11040 Training loss: 1.1751 0.4758 sec/batch\n",
      "Epoch 34/40  Iteration 9233/11040 Training loss: 1.1751 0.4771 sec/batch\n",
      "Epoch 34/40  Iteration 9234/11040 Training loss: 1.1753 0.4731 sec/batch\n",
      "Epoch 34/40  Iteration 9235/11040 Training loss: 1.1754 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9236/11040 Training loss: 1.1755 0.4758 sec/batch\n",
      "Epoch 34/40  Iteration 9237/11040 Training loss: 1.1754 0.4758 sec/batch\n",
      "Epoch 34/40  Iteration 9238/11040 Training loss: 1.1757 0.4776 sec/batch\n",
      "Epoch 34/40  Iteration 9239/11040 Training loss: 1.1758 0.4746 sec/batch\n",
      "Epoch 34/40  Iteration 9240/11040 Training loss: 1.1756 0.4743 sec/batch\n",
      "Epoch 34/40  Iteration 9241/11040 Training loss: 1.1756 0.4917 sec/batch\n",
      "Epoch 34/40  Iteration 9242/11040 Training loss: 1.1755 0.4747 sec/batch\n",
      "Epoch 34/40  Iteration 9243/11040 Training loss: 1.1756 0.4768 sec/batch\n",
      "Epoch 34/40  Iteration 9244/11040 Training loss: 1.1756 0.4921 sec/batch\n",
      "Epoch 34/40  Iteration 9245/11040 Training loss: 1.1755 0.4743 sec/batch\n",
      "Epoch 34/40  Iteration 9246/11040 Training loss: 1.1756 0.4797 sec/batch\n",
      "Epoch 34/40  Iteration 9247/11040 Training loss: 1.1758 0.4754 sec/batch\n",
      "Epoch 34/40  Iteration 9248/11040 Training loss: 1.1759 0.4779 sec/batch\n",
      "Epoch 34/40  Iteration 9249/11040 Training loss: 1.1760 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9250/11040 Training loss: 1.1761 0.4749 sec/batch\n",
      "Epoch 34/40  Iteration 9251/11040 Training loss: 1.1761 0.4757 sec/batch\n",
      "Epoch 34/40  Iteration 9252/11040 Training loss: 1.1760 0.4761 sec/batch\n",
      "Epoch 34/40  Iteration 9253/11040 Training loss: 1.1761 0.4898 sec/batch\n",
      "Epoch 34/40  Iteration 9254/11040 Training loss: 1.1761 0.4761 sec/batch\n",
      "Epoch 34/40  Iteration 9255/11040 Training loss: 1.1761 0.4940 sec/batch\n",
      "Epoch 34/40  Iteration 9256/11040 Training loss: 1.1761 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9257/11040 Training loss: 1.1760 0.4748 sec/batch\n",
      "Epoch 34/40  Iteration 9258/11040 Training loss: 1.1761 0.4904 sec/batch\n",
      "Epoch 34/40  Iteration 9259/11040 Training loss: 1.1761 0.4897 sec/batch\n",
      "Epoch 34/40  Iteration 9260/11040 Training loss: 1.1763 0.4769 sec/batch\n",
      "Epoch 34/40  Iteration 9261/11040 Training loss: 1.1764 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9262/11040 Training loss: 1.1765 0.4903 sec/batch\n",
      "Epoch 34/40  Iteration 9263/11040 Training loss: 1.1766 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9264/11040 Training loss: 1.1765 0.4747 sec/batch\n",
      "Epoch 34/40  Iteration 9265/11040 Training loss: 1.1764 0.4897 sec/batch\n",
      "Epoch 34/40  Iteration 9266/11040 Training loss: 1.1766 0.4757 sec/batch\n",
      "Epoch 34/40  Iteration 9267/11040 Training loss: 1.1767 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9268/11040 Training loss: 1.1768 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9269/11040 Training loss: 1.1769 0.4741 sec/batch\n",
      "Epoch 34/40  Iteration 9270/11040 Training loss: 1.1769 0.4852 sec/batch\n",
      "Epoch 34/40  Iteration 9271/11040 Training loss: 1.1771 0.4776 sec/batch\n",
      "Epoch 34/40  Iteration 9272/11040 Training loss: 1.1772 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9273/11040 Training loss: 1.1775 0.4775 sec/batch\n",
      "Epoch 34/40  Iteration 9274/11040 Training loss: 1.1778 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9275/11040 Training loss: 1.1778 0.4902 sec/batch\n",
      "Epoch 34/40  Iteration 9276/11040 Training loss: 1.1777 0.4747 sec/batch\n",
      "Epoch 34/40  Iteration 9277/11040 Training loss: 1.1777 0.4779 sec/batch\n",
      "Epoch 34/40  Iteration 9278/11040 Training loss: 1.1778 0.4734 sec/batch\n",
      "Epoch 34/40  Iteration 9279/11040 Training loss: 1.1780 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9280/11040 Training loss: 1.1782 0.4756 sec/batch\n",
      "Epoch 34/40  Iteration 9281/11040 Training loss: 1.1782 0.4755 sec/batch\n",
      "Epoch 34/40  Iteration 9282/11040 Training loss: 1.1782 0.4954 sec/batch\n",
      "Epoch 34/40  Iteration 9283/11040 Training loss: 1.1783 0.4902 sec/batch\n",
      "Epoch 34/40  Iteration 9284/11040 Training loss: 1.1782 0.4887 sec/batch\n",
      "Epoch 34/40  Iteration 9285/11040 Training loss: 1.1784 0.4911 sec/batch\n",
      "Epoch 34/40  Iteration 9286/11040 Training loss: 1.1783 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9287/11040 Training loss: 1.1786 0.4757 sec/batch\n",
      "Epoch 34/40  Iteration 9288/11040 Training loss: 1.1787 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9289/11040 Training loss: 1.1787 0.4916 sec/batch\n",
      "Epoch 34/40  Iteration 9290/11040 Training loss: 1.1787 0.4912 sec/batch\n",
      "Epoch 34/40  Iteration 9291/11040 Training loss: 1.1786 0.4896 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40  Iteration 9292/11040 Training loss: 1.1787 0.4778 sec/batch\n",
      "Epoch 34/40  Iteration 9293/11040 Training loss: 1.1788 0.4748 sec/batch\n",
      "Epoch 34/40  Iteration 9294/11040 Training loss: 1.1788 0.4917 sec/batch\n",
      "Epoch 34/40  Iteration 9295/11040 Training loss: 1.1791 0.4925 sec/batch\n",
      "Epoch 34/40  Iteration 9296/11040 Training loss: 1.1791 0.4933 sec/batch\n",
      "Epoch 34/40  Iteration 9297/11040 Training loss: 1.1790 0.4913 sec/batch\n",
      "Epoch 34/40  Iteration 9298/11040 Training loss: 1.1790 0.4900 sec/batch\n",
      "Epoch 34/40  Iteration 9299/11040 Training loss: 1.1789 0.4789 sec/batch\n",
      "Epoch 34/40  Iteration 9300/11040 Training loss: 1.1789 0.4908 sec/batch\n",
      "Epoch 34/40  Iteration 9301/11040 Training loss: 1.1789 0.4942 sec/batch\n",
      "Epoch 34/40  Iteration 9302/11040 Training loss: 1.1790 0.4910 sec/batch\n",
      "Epoch 34/40  Iteration 9303/11040 Training loss: 1.1792 0.4921 sec/batch\n",
      "Epoch 34/40  Iteration 9304/11040 Training loss: 1.1790 0.4899 sec/batch\n",
      "Epoch 34/40  Iteration 9305/11040 Training loss: 1.1792 0.4918 sec/batch\n",
      "Epoch 34/40  Iteration 9306/11040 Training loss: 1.1792 0.4934 sec/batch\n",
      "Epoch 34/40  Iteration 9307/11040 Training loss: 1.1793 0.4927 sec/batch\n",
      "Epoch 34/40  Iteration 9308/11040 Training loss: 1.1794 0.4928 sec/batch\n",
      "Epoch 34/40  Iteration 9309/11040 Training loss: 1.1796 0.4915 sec/batch\n",
      "Epoch 34/40  Iteration 9310/11040 Training loss: 1.1798 0.4917 sec/batch\n",
      "Epoch 34/40  Iteration 9311/11040 Training loss: 1.1798 0.4928 sec/batch\n",
      "Epoch 34/40  Iteration 9312/11040 Training loss: 1.1800 0.4917 sec/batch\n",
      "Epoch 34/40  Iteration 9313/11040 Training loss: 1.1800 0.4912 sec/batch\n",
      "Epoch 34/40  Iteration 9314/11040 Training loss: 1.1799 0.4942 sec/batch\n",
      "Epoch 34/40  Iteration 9315/11040 Training loss: 1.1800 0.4902 sec/batch\n",
      "Epoch 34/40  Iteration 9316/11040 Training loss: 1.1800 0.4772 sec/batch\n",
      "Epoch 34/40  Iteration 9317/11040 Training loss: 1.1798 0.4762 sec/batch\n",
      "Epoch 34/40  Iteration 9318/11040 Training loss: 1.1798 0.4915 sec/batch\n",
      "Epoch 34/40  Iteration 9319/11040 Training loss: 1.1798 0.4953 sec/batch\n",
      "Epoch 34/40  Iteration 9320/11040 Training loss: 1.1798 0.5048 sec/batch\n",
      "Epoch 34/40  Iteration 9321/11040 Training loss: 1.1798 0.4768 sec/batch\n",
      "Epoch 34/40  Iteration 9322/11040 Training loss: 1.1798 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9323/11040 Training loss: 1.1797 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9324/11040 Training loss: 1.1796 0.4897 sec/batch\n",
      "Epoch 34/40  Iteration 9325/11040 Training loss: 1.1794 0.4972 sec/batch\n",
      "Epoch 34/40  Iteration 9326/11040 Training loss: 1.1794 0.4708 sec/batch\n",
      "Epoch 34/40  Iteration 9327/11040 Training loss: 1.1793 0.4981 sec/batch\n",
      "Epoch 34/40  Iteration 9328/11040 Training loss: 1.1793 0.4841 sec/batch\n",
      "Epoch 34/40  Iteration 9329/11040 Training loss: 1.1792 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9330/11040 Training loss: 1.1790 0.4742 sec/batch\n",
      "Epoch 34/40  Iteration 9331/11040 Training loss: 1.1789 0.4917 sec/batch\n",
      "Epoch 34/40  Iteration 9332/11040 Training loss: 1.1790 0.4898 sec/batch\n",
      "Epoch 34/40  Iteration 9333/11040 Training loss: 1.1790 0.4785 sec/batch\n",
      "Epoch 34/40  Iteration 9334/11040 Training loss: 1.1790 0.4738 sec/batch\n",
      "Epoch 34/40  Iteration 9335/11040 Training loss: 1.1789 0.4769 sec/batch\n",
      "Epoch 34/40  Iteration 9336/11040 Training loss: 1.1788 0.4901 sec/batch\n",
      "Epoch 34/40  Iteration 9337/11040 Training loss: 1.1789 0.4743 sec/batch\n",
      "Epoch 34/40  Iteration 9338/11040 Training loss: 1.1789 0.4766 sec/batch\n",
      "Epoch 34/40  Iteration 9339/11040 Training loss: 1.1789 0.4769 sec/batch\n",
      "Epoch 34/40  Iteration 9340/11040 Training loss: 1.1790 0.4746 sec/batch\n",
      "Epoch 34/40  Iteration 9341/11040 Training loss: 1.1790 0.4768 sec/batch\n",
      "Epoch 34/40  Iteration 9342/11040 Training loss: 1.1789 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9343/11040 Training loss: 1.1790 0.4751 sec/batch\n",
      "Epoch 34/40  Iteration 9344/11040 Training loss: 1.1790 0.4755 sec/batch\n",
      "Epoch 34/40  Iteration 9345/11040 Training loss: 1.1791 0.4747 sec/batch\n",
      "Epoch 34/40  Iteration 9346/11040 Training loss: 1.1791 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9347/11040 Training loss: 1.1792 0.4760 sec/batch\n",
      "Epoch 34/40  Iteration 9348/11040 Training loss: 1.1793 0.4921 sec/batch\n",
      "Epoch 34/40  Iteration 9349/11040 Training loss: 1.1794 0.4779 sec/batch\n",
      "Epoch 34/40  Iteration 9350/11040 Training loss: 1.1794 0.4737 sec/batch\n",
      "Epoch 34/40  Iteration 9351/11040 Training loss: 1.1794 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9352/11040 Training loss: 1.1795 0.4739 sec/batch\n",
      "Epoch 34/40  Iteration 9353/11040 Training loss: 1.1794 0.4762 sec/batch\n",
      "Epoch 34/40  Iteration 9354/11040 Training loss: 1.1795 0.4764 sec/batch\n",
      "Epoch 34/40  Iteration 9355/11040 Training loss: 1.1794 0.4740 sec/batch\n",
      "Epoch 34/40  Iteration 9356/11040 Training loss: 1.1795 0.4766 sec/batch\n",
      "Epoch 34/40  Iteration 9357/11040 Training loss: 1.1795 0.4757 sec/batch\n",
      "Epoch 34/40  Iteration 9358/11040 Training loss: 1.1795 0.4768 sec/batch\n",
      "Epoch 34/40  Iteration 9359/11040 Training loss: 1.1795 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9360/11040 Training loss: 1.1795 0.4785 sec/batch\n",
      "Epoch 34/40  Iteration 9361/11040 Training loss: 1.1795 0.4723 sec/batch\n",
      "Epoch 34/40  Iteration 9362/11040 Training loss: 1.1796 0.4774 sec/batch\n",
      "Epoch 34/40  Iteration 9363/11040 Training loss: 1.1796 0.4748 sec/batch\n",
      "Epoch 34/40  Iteration 9364/11040 Training loss: 1.1797 0.4744 sec/batch\n",
      "Epoch 34/40  Iteration 9365/11040 Training loss: 1.1798 0.4767 sec/batch\n",
      "Epoch 34/40  Iteration 9366/11040 Training loss: 1.1799 0.4902 sec/batch\n",
      "Epoch 34/40  Iteration 9367/11040 Training loss: 1.1799 0.4759 sec/batch\n",
      "Epoch 34/40  Iteration 9368/11040 Training loss: 1.1799 0.4900 sec/batch\n",
      "Epoch 34/40  Iteration 9369/11040 Training loss: 1.1800 0.4750 sec/batch\n",
      "Epoch 34/40  Iteration 9370/11040 Training loss: 1.1802 0.4767 sec/batch\n",
      "Epoch 34/40  Iteration 9371/11040 Training loss: 1.1804 0.4754 sec/batch\n",
      "Epoch 34/40  Iteration 9372/11040 Training loss: 1.1805 0.4750 sec/batch\n",
      "Epoch 34/40  Iteration 9373/11040 Training loss: 1.1804 0.4752 sec/batch\n",
      "Epoch 34/40  Iteration 9374/11040 Training loss: 1.1806 0.4776 sec/batch\n",
      "Epoch 34/40  Iteration 9375/11040 Training loss: 1.1808 0.4737 sec/batch\n",
      "Epoch 34/40  Iteration 9376/11040 Training loss: 1.1809 0.4656 sec/batch\n",
      "Epoch 34/40  Iteration 9377/11040 Training loss: 1.1810 0.4851 sec/batch\n",
      "Epoch 34/40  Iteration 9378/11040 Training loss: 1.1811 0.4756 sec/batch\n",
      "Epoch 34/40  Iteration 9379/11040 Training loss: 1.1814 0.4722 sec/batch\n",
      "Epoch 34/40  Iteration 9380/11040 Training loss: 1.1817 0.4799 sec/batch\n",
      "Epoch 34/40  Iteration 9381/11040 Training loss: 1.1818 0.4763 sec/batch\n",
      "Epoch 34/40  Iteration 9382/11040 Training loss: 1.1818 0.4732 sec/batch\n",
      "Epoch 34/40  Iteration 9383/11040 Training loss: 1.1819 0.4772 sec/batch\n",
      "Epoch 34/40  Iteration 9384/11040 Training loss: 1.1820 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9385/11040 Training loss: 1.2656 0.4748 sec/batch\n",
      "Epoch 35/40  Iteration 9386/11040 Training loss: 1.2252 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9387/11040 Training loss: 1.2229 0.4924 sec/batch\n",
      "Epoch 35/40  Iteration 9388/11040 Training loss: 1.2194 0.4746 sec/batch\n",
      "Epoch 35/40  Iteration 9389/11040 Training loss: 1.2174 0.4765 sec/batch\n",
      "Epoch 35/40  Iteration 9390/11040 Training loss: 1.2128 0.4737 sec/batch\n",
      "Epoch 35/40  Iteration 9391/11040 Training loss: 1.2024 0.4929 sec/batch\n",
      "Epoch 35/40  Iteration 9392/11040 Training loss: 1.1984 0.4761 sec/batch\n",
      "Epoch 35/40  Iteration 9393/11040 Training loss: 1.1944 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9394/11040 Training loss: 1.1931 0.4919 sec/batch\n",
      "Epoch 35/40  Iteration 9395/11040 Training loss: 1.1891 0.4747 sec/batch\n",
      "Epoch 35/40  Iteration 9396/11040 Training loss: 1.1868 0.4765 sec/batch\n",
      "Epoch 35/40  Iteration 9397/11040 Training loss: 1.1832 0.4763 sec/batch\n",
      "Epoch 35/40  Iteration 9398/11040 Training loss: 1.1809 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9399/11040 Training loss: 1.1805 0.4745 sec/batch\n",
      "Epoch 35/40  Iteration 9400/11040 Training loss: 1.1803 0.4769 sec/batch\n",
      "Epoch 35/40  Iteration 9401/11040 Training loss: 1.1778 0.4732 sec/batch\n",
      "Epoch 35/40  Iteration 9402/11040 Training loss: 1.1753 0.4761 sec/batch\n",
      "Epoch 35/40  Iteration 9403/11040 Training loss: 1.1745 0.4913 sec/batch\n",
      "Epoch 35/40  Iteration 9404/11040 Training loss: 1.1747 0.4745 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40  Iteration 9405/11040 Training loss: 1.1765 0.4782 sec/batch\n",
      "Epoch 35/40  Iteration 9406/11040 Training loss: 1.1773 0.4879 sec/batch\n",
      "Epoch 35/40  Iteration 9407/11040 Training loss: 1.1767 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9408/11040 Training loss: 1.1772 0.4771 sec/batch\n",
      "Epoch 35/40  Iteration 9409/11040 Training loss: 1.1767 0.4804 sec/batch\n",
      "Epoch 35/40  Iteration 9410/11040 Training loss: 1.1763 0.4891 sec/batch\n",
      "Epoch 35/40  Iteration 9411/11040 Training loss: 1.1758 0.4864 sec/batch\n",
      "Epoch 35/40  Iteration 9412/11040 Training loss: 1.1757 0.4764 sec/batch\n",
      "Epoch 35/40  Iteration 9413/11040 Training loss: 1.1760 0.4770 sec/batch\n",
      "Epoch 35/40  Iteration 9414/11040 Training loss: 1.1765 0.4748 sec/batch\n",
      "Epoch 35/40  Iteration 9415/11040 Training loss: 1.1763 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9416/11040 Training loss: 1.1759 0.4919 sec/batch\n",
      "Epoch 35/40  Iteration 9417/11040 Training loss: 1.1749 0.4898 sec/batch\n",
      "Epoch 35/40  Iteration 9418/11040 Training loss: 1.1747 0.4764 sec/batch\n",
      "Epoch 35/40  Iteration 9419/11040 Training loss: 1.1748 0.4758 sec/batch\n",
      "Epoch 35/40  Iteration 9420/11040 Training loss: 1.1755 0.4758 sec/batch\n",
      "Epoch 35/40  Iteration 9421/11040 Training loss: 1.1756 0.4782 sec/batch\n",
      "Epoch 35/40  Iteration 9422/11040 Training loss: 1.1749 0.4734 sec/batch\n",
      "Epoch 35/40  Iteration 9423/11040 Training loss: 1.1743 0.4768 sec/batch\n",
      "Epoch 35/40  Iteration 9424/11040 Training loss: 1.1742 0.4780 sec/batch\n",
      "Epoch 35/40  Iteration 9425/11040 Training loss: 1.1738 0.4732 sec/batch\n",
      "Epoch 35/40  Iteration 9426/11040 Training loss: 1.1727 0.4920 sec/batch\n",
      "Epoch 35/40  Iteration 9427/11040 Training loss: 1.1729 0.4754 sec/batch\n",
      "Epoch 35/40  Iteration 9428/11040 Training loss: 1.1718 0.4763 sec/batch\n",
      "Epoch 35/40  Iteration 9429/11040 Training loss: 1.1712 0.4754 sec/batch\n",
      "Epoch 35/40  Iteration 9430/11040 Training loss: 1.1705 0.4757 sec/batch\n",
      "Epoch 35/40  Iteration 9431/11040 Training loss: 1.1698 0.4764 sec/batch\n",
      "Epoch 35/40  Iteration 9432/11040 Training loss: 1.1697 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9433/11040 Training loss: 1.1695 0.4765 sec/batch\n",
      "Epoch 35/40  Iteration 9434/11040 Training loss: 1.1700 0.4755 sec/batch\n",
      "Epoch 35/40  Iteration 9435/11040 Training loss: 1.1697 0.4914 sec/batch\n",
      "Epoch 35/40  Iteration 9436/11040 Training loss: 1.1696 0.4921 sec/batch\n",
      "Epoch 35/40  Iteration 9437/11040 Training loss: 1.1698 0.4918 sec/batch\n",
      "Epoch 35/40  Iteration 9438/11040 Training loss: 1.1693 0.4766 sec/batch\n",
      "Epoch 35/40  Iteration 9439/11040 Training loss: 1.1693 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9440/11040 Training loss: 1.1692 0.4819 sec/batch\n",
      "Epoch 35/40  Iteration 9441/11040 Training loss: 1.1691 0.4827 sec/batch\n",
      "Epoch 35/40  Iteration 9442/11040 Training loss: 1.1690 0.4811 sec/batch\n",
      "Epoch 35/40  Iteration 9443/11040 Training loss: 1.1693 0.4900 sec/batch\n",
      "Epoch 35/40  Iteration 9444/11040 Training loss: 1.1688 0.4913 sec/batch\n",
      "Epoch 35/40  Iteration 9445/11040 Training loss: 1.1687 0.4898 sec/batch\n",
      "Epoch 35/40  Iteration 9446/11040 Training loss: 1.1687 0.4914 sec/batch\n",
      "Epoch 35/40  Iteration 9447/11040 Training loss: 1.1689 0.4763 sec/batch\n",
      "Epoch 35/40  Iteration 9448/11040 Training loss: 1.1688 0.4770 sec/batch\n",
      "Epoch 35/40  Iteration 9449/11040 Training loss: 1.1692 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9450/11040 Training loss: 1.1690 0.4770 sec/batch\n",
      "Epoch 35/40  Iteration 9451/11040 Training loss: 1.1690 0.4903 sec/batch\n",
      "Epoch 35/40  Iteration 9452/11040 Training loss: 1.1690 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9453/11040 Training loss: 1.1688 0.4756 sec/batch\n",
      "Epoch 35/40  Iteration 9454/11040 Training loss: 1.1688 0.4777 sec/batch\n",
      "Epoch 35/40  Iteration 9455/11040 Training loss: 1.1684 0.4908 sec/batch\n",
      "Epoch 35/40  Iteration 9456/11040 Training loss: 1.1680 0.4926 sec/batch\n",
      "Epoch 35/40  Iteration 9457/11040 Training loss: 1.1676 0.4907 sec/batch\n",
      "Epoch 35/40  Iteration 9458/11040 Training loss: 1.1672 0.4904 sec/batch\n",
      "Epoch 35/40  Iteration 9459/11040 Training loss: 1.1670 0.4900 sec/batch\n",
      "Epoch 35/40  Iteration 9460/11040 Training loss: 1.1674 0.4918 sec/batch\n",
      "Epoch 35/40  Iteration 9461/11040 Training loss: 1.1670 0.4815 sec/batch\n",
      "Epoch 35/40  Iteration 9462/11040 Training loss: 1.1665 0.4869 sec/batch\n",
      "Epoch 35/40  Iteration 9463/11040 Training loss: 1.1663 0.4924 sec/batch\n",
      "Epoch 35/40  Iteration 9464/11040 Training loss: 1.1664 0.4918 sec/batch\n",
      "Epoch 35/40  Iteration 9465/11040 Training loss: 1.1662 0.4750 sec/batch\n",
      "Epoch 35/40  Iteration 9466/11040 Training loss: 1.1663 0.4745 sec/batch\n",
      "Epoch 35/40  Iteration 9467/11040 Training loss: 1.1663 0.4763 sec/batch\n",
      "Epoch 35/40  Iteration 9468/11040 Training loss: 1.1662 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9469/11040 Training loss: 1.1660 0.4767 sec/batch\n",
      "Epoch 35/40  Iteration 9470/11040 Training loss: 1.1660 0.4914 sec/batch\n",
      "Epoch 35/40  Iteration 9471/11040 Training loss: 1.1653 0.4917 sec/batch\n",
      "Epoch 35/40  Iteration 9472/11040 Training loss: 1.1643 0.4914 sec/batch\n",
      "Epoch 35/40  Iteration 9473/11040 Training loss: 1.1641 0.4915 sec/batch\n",
      "Epoch 35/40  Iteration 9474/11040 Training loss: 1.1642 0.4907 sec/batch\n",
      "Epoch 35/40  Iteration 9475/11040 Training loss: 1.1642 0.4761 sec/batch\n",
      "Epoch 35/40  Iteration 9476/11040 Training loss: 1.1642 0.4756 sec/batch\n",
      "Epoch 35/40  Iteration 9477/11040 Training loss: 1.1639 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9478/11040 Training loss: 1.1642 0.4784 sec/batch\n",
      "Epoch 35/40  Iteration 9479/11040 Training loss: 1.1647 0.4875 sec/batch\n",
      "Epoch 35/40  Iteration 9480/11040 Training loss: 1.1649 0.4901 sec/batch\n",
      "Epoch 35/40  Iteration 9481/11040 Training loss: 1.1654 0.4933 sec/batch\n",
      "Epoch 35/40  Iteration 9482/11040 Training loss: 1.1658 0.4895 sec/batch\n",
      "Epoch 35/40  Iteration 9483/11040 Training loss: 1.1660 0.4950 sec/batch\n",
      "Epoch 35/40  Iteration 9484/11040 Training loss: 1.1659 0.4887 sec/batch\n",
      "Epoch 35/40  Iteration 9485/11040 Training loss: 1.1661 0.4909 sec/batch\n",
      "Epoch 35/40  Iteration 9486/11040 Training loss: 1.1663 0.4917 sec/batch\n",
      "Epoch 35/40  Iteration 9487/11040 Training loss: 1.1660 0.4759 sec/batch\n",
      "Epoch 35/40  Iteration 9488/11040 Training loss: 1.1662 0.4768 sec/batch\n",
      "Epoch 35/40  Iteration 9489/11040 Training loss: 1.1661 0.4749 sec/batch\n",
      "Epoch 35/40  Iteration 9490/11040 Training loss: 1.1659 0.4767 sec/batch\n",
      "Epoch 35/40  Iteration 9491/11040 Training loss: 1.1657 0.4766 sec/batch\n",
      "Epoch 35/40  Iteration 9492/11040 Training loss: 1.1658 0.5051 sec/batch\n",
      "Epoch 35/40  Iteration 9493/11040 Training loss: 1.1662 0.4713 sec/batch\n",
      "Epoch 35/40  Iteration 9494/11040 Training loss: 1.1661 0.4729 sec/batch\n",
      "Epoch 35/40  Iteration 9495/11040 Training loss: 1.1662 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9496/11040 Training loss: 1.1665 0.4868 sec/batch\n",
      "Epoch 35/40  Iteration 9497/11040 Training loss: 1.1664 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9498/11040 Training loss: 1.1665 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9499/11040 Training loss: 1.1667 0.4761 sec/batch\n",
      "Epoch 35/40  Iteration 9500/11040 Training loss: 1.1670 0.4771 sec/batch\n",
      "Validation loss: 1.24185 Saving checkpoint!\n",
      "Epoch 35/40  Iteration 9501/11040 Training loss: 1.1684 0.4687 sec/batch\n",
      "Epoch 35/40  Iteration 9502/11040 Training loss: 1.1687 0.4671 sec/batch\n",
      "Epoch 35/40  Iteration 9503/11040 Training loss: 1.1687 0.4724 sec/batch\n",
      "Epoch 35/40  Iteration 9504/11040 Training loss: 1.1688 0.4830 sec/batch\n",
      "Epoch 35/40  Iteration 9505/11040 Training loss: 1.1687 0.4676 sec/batch\n",
      "Epoch 35/40  Iteration 9506/11040 Training loss: 1.1689 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9507/11040 Training loss: 1.1693 0.4750 sec/batch\n",
      "Epoch 35/40  Iteration 9508/11040 Training loss: 1.1698 0.4760 sec/batch\n",
      "Epoch 35/40  Iteration 9509/11040 Training loss: 1.1699 0.4740 sec/batch\n",
      "Epoch 35/40  Iteration 9510/11040 Training loss: 1.1702 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9511/11040 Training loss: 1.1702 0.4739 sec/batch\n",
      "Epoch 35/40  Iteration 9512/11040 Training loss: 1.1703 0.4754 sec/batch\n",
      "Epoch 35/40  Iteration 9513/11040 Training loss: 1.1701 0.4596 sec/batch\n",
      "Epoch 35/40  Iteration 9514/11040 Training loss: 1.1704 0.4608 sec/batch\n",
      "Epoch 35/40  Iteration 9515/11040 Training loss: 1.1703 0.4725 sec/batch\n",
      "Epoch 35/40  Iteration 9516/11040 Training loss: 1.1702 0.4752 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40  Iteration 9517/11040 Training loss: 1.1702 0.4606 sec/batch\n",
      "Epoch 35/40  Iteration 9518/11040 Training loss: 1.1700 0.4742 sec/batch\n",
      "Epoch 35/40  Iteration 9519/11040 Training loss: 1.1703 0.4624 sec/batch\n",
      "Epoch 35/40  Iteration 9520/11040 Training loss: 1.1703 0.4711 sec/batch\n",
      "Epoch 35/40  Iteration 9521/11040 Training loss: 1.1702 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9522/11040 Training loss: 1.1703 0.4755 sec/batch\n",
      "Epoch 35/40  Iteration 9523/11040 Training loss: 1.1705 0.4638 sec/batch\n",
      "Epoch 35/40  Iteration 9524/11040 Training loss: 1.1706 0.4714 sec/batch\n",
      "Epoch 35/40  Iteration 9525/11040 Training loss: 1.1707 0.4734 sec/batch\n",
      "Epoch 35/40  Iteration 9526/11040 Training loss: 1.1708 0.4768 sec/batch\n",
      "Epoch 35/40  Iteration 9527/11040 Training loss: 1.1708 0.4607 sec/batch\n",
      "Epoch 35/40  Iteration 9528/11040 Training loss: 1.1707 0.4730 sec/batch\n",
      "Epoch 35/40  Iteration 9529/11040 Training loss: 1.1708 0.4602 sec/batch\n",
      "Epoch 35/40  Iteration 9530/11040 Training loss: 1.1708 0.4736 sec/batch\n",
      "Epoch 35/40  Iteration 9531/11040 Training loss: 1.1707 0.4740 sec/batch\n",
      "Epoch 35/40  Iteration 9532/11040 Training loss: 1.1707 0.4620 sec/batch\n",
      "Epoch 35/40  Iteration 9533/11040 Training loss: 1.1706 0.4583 sec/batch\n",
      "Epoch 35/40  Iteration 9534/11040 Training loss: 1.1706 0.4738 sec/batch\n",
      "Epoch 35/40  Iteration 9535/11040 Training loss: 1.1706 0.4775 sec/batch\n",
      "Epoch 35/40  Iteration 9536/11040 Training loss: 1.1708 0.4890 sec/batch\n",
      "Epoch 35/40  Iteration 9537/11040 Training loss: 1.1710 0.4611 sec/batch\n",
      "Epoch 35/40  Iteration 9538/11040 Training loss: 1.1711 0.4746 sec/batch\n",
      "Epoch 35/40  Iteration 9539/11040 Training loss: 1.1711 0.4731 sec/batch\n",
      "Epoch 35/40  Iteration 9540/11040 Training loss: 1.1711 0.4749 sec/batch\n",
      "Epoch 35/40  Iteration 9541/11040 Training loss: 1.1710 0.4585 sec/batch\n",
      "Epoch 35/40  Iteration 9542/11040 Training loss: 1.1711 0.4754 sec/batch\n",
      "Epoch 35/40  Iteration 9543/11040 Training loss: 1.1713 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9544/11040 Training loss: 1.1713 0.4761 sec/batch\n",
      "Epoch 35/40  Iteration 9545/11040 Training loss: 1.1715 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9546/11040 Training loss: 1.1716 0.4758 sec/batch\n",
      "Epoch 35/40  Iteration 9547/11040 Training loss: 1.1717 0.4586 sec/batch\n",
      "Epoch 35/40  Iteration 9548/11040 Training loss: 1.1719 0.4588 sec/batch\n",
      "Epoch 35/40  Iteration 9549/11040 Training loss: 1.1722 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9550/11040 Training loss: 1.1725 0.4745 sec/batch\n",
      "Epoch 35/40  Iteration 9551/11040 Training loss: 1.1725 0.4731 sec/batch\n",
      "Epoch 35/40  Iteration 9552/11040 Training loss: 1.1725 0.4782 sec/batch\n",
      "Epoch 35/40  Iteration 9553/11040 Training loss: 1.1726 0.4713 sec/batch\n",
      "Epoch 35/40  Iteration 9554/11040 Training loss: 1.1726 0.4727 sec/batch\n",
      "Epoch 35/40  Iteration 9555/11040 Training loss: 1.1728 0.4774 sec/batch\n",
      "Epoch 35/40  Iteration 9556/11040 Training loss: 1.1730 0.4728 sec/batch\n",
      "Epoch 35/40  Iteration 9557/11040 Training loss: 1.1729 0.4591 sec/batch\n",
      "Epoch 35/40  Iteration 9558/11040 Training loss: 1.1730 0.4596 sec/batch\n",
      "Epoch 35/40  Iteration 9559/11040 Training loss: 1.1730 0.4583 sec/batch\n",
      "Epoch 35/40  Iteration 9560/11040 Training loss: 1.1730 0.4755 sec/batch\n",
      "Epoch 35/40  Iteration 9561/11040 Training loss: 1.1731 0.4738 sec/batch\n",
      "Epoch 35/40  Iteration 9562/11040 Training loss: 1.1730 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9563/11040 Training loss: 1.1734 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9564/11040 Training loss: 1.1735 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9565/11040 Training loss: 1.1736 0.4770 sec/batch\n",
      "Epoch 35/40  Iteration 9566/11040 Training loss: 1.1737 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9567/11040 Training loss: 1.1735 0.4733 sec/batch\n",
      "Epoch 35/40  Iteration 9568/11040 Training loss: 1.1737 0.4742 sec/batch\n",
      "Epoch 35/40  Iteration 9569/11040 Training loss: 1.1738 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9570/11040 Training loss: 1.1738 0.4767 sec/batch\n",
      "Epoch 35/40  Iteration 9571/11040 Training loss: 1.1741 0.4586 sec/batch\n",
      "Epoch 35/40  Iteration 9572/11040 Training loss: 1.1742 0.4758 sec/batch\n",
      "Epoch 35/40  Iteration 9573/11040 Training loss: 1.1741 0.4736 sec/batch\n",
      "Epoch 35/40  Iteration 9574/11040 Training loss: 1.1741 0.4611 sec/batch\n",
      "Epoch 35/40  Iteration 9575/11040 Training loss: 1.1740 0.4734 sec/batch\n",
      "Epoch 35/40  Iteration 9576/11040 Training loss: 1.1740 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9577/11040 Training loss: 1.1740 0.4765 sec/batch\n",
      "Epoch 35/40  Iteration 9578/11040 Training loss: 1.1741 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9579/11040 Training loss: 1.1743 0.4600 sec/batch\n",
      "Epoch 35/40  Iteration 9580/11040 Training loss: 1.1741 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9581/11040 Training loss: 1.1743 0.4735 sec/batch\n",
      "Epoch 35/40  Iteration 9582/11040 Training loss: 1.1743 0.4783 sec/batch\n",
      "Epoch 35/40  Iteration 9583/11040 Training loss: 1.1743 0.4735 sec/batch\n",
      "Epoch 35/40  Iteration 9584/11040 Training loss: 1.1744 0.4748 sec/batch\n",
      "Epoch 35/40  Iteration 9585/11040 Training loss: 1.1746 0.4604 sec/batch\n",
      "Epoch 35/40  Iteration 9586/11040 Training loss: 1.1748 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9587/11040 Training loss: 1.1749 0.4770 sec/batch\n",
      "Epoch 35/40  Iteration 9588/11040 Training loss: 1.1750 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9589/11040 Training loss: 1.1751 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9590/11040 Training loss: 1.1750 0.4792 sec/batch\n",
      "Epoch 35/40  Iteration 9591/11040 Training loss: 1.1751 0.4864 sec/batch\n",
      "Epoch 35/40  Iteration 9592/11040 Training loss: 1.1752 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9593/11040 Training loss: 1.1750 0.4775 sec/batch\n",
      "Epoch 35/40  Iteration 9594/11040 Training loss: 1.1750 0.4732 sec/batch\n",
      "Epoch 35/40  Iteration 9595/11040 Training loss: 1.1751 0.4746 sec/batch\n",
      "Epoch 35/40  Iteration 9596/11040 Training loss: 1.1751 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9597/11040 Training loss: 1.1751 0.4725 sec/batch\n",
      "Epoch 35/40  Iteration 9598/11040 Training loss: 1.1751 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9599/11040 Training loss: 1.1750 0.4762 sec/batch\n",
      "Epoch 35/40  Iteration 9600/11040 Training loss: 1.1749 0.4722 sec/batch\n",
      "Epoch 35/40  Iteration 9601/11040 Training loss: 1.1748 0.4608 sec/batch\n",
      "Epoch 35/40  Iteration 9602/11040 Training loss: 1.1747 0.4733 sec/batch\n",
      "Epoch 35/40  Iteration 9603/11040 Training loss: 1.1746 0.4751 sec/batch\n",
      "Epoch 35/40  Iteration 9604/11040 Training loss: 1.1746 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9605/11040 Training loss: 1.1745 0.4773 sec/batch\n",
      "Epoch 35/40  Iteration 9606/11040 Training loss: 1.1744 0.4754 sec/batch\n",
      "Epoch 35/40  Iteration 9607/11040 Training loss: 1.1743 0.4632 sec/batch\n",
      "Epoch 35/40  Iteration 9608/11040 Training loss: 1.1744 0.4864 sec/batch\n",
      "Epoch 35/40  Iteration 9609/11040 Training loss: 1.1744 0.4734 sec/batch\n",
      "Epoch 35/40  Iteration 9610/11040 Training loss: 1.1744 0.4742 sec/batch\n",
      "Epoch 35/40  Iteration 9611/11040 Training loss: 1.1743 0.4833 sec/batch\n",
      "Epoch 35/40  Iteration 9612/11040 Training loss: 1.1742 0.4696 sec/batch\n",
      "Epoch 35/40  Iteration 9613/11040 Training loss: 1.1743 0.4735 sec/batch\n",
      "Epoch 35/40  Iteration 9614/11040 Training loss: 1.1744 0.4657 sec/batch\n",
      "Epoch 35/40  Iteration 9615/11040 Training loss: 1.1743 0.4861 sec/batch\n",
      "Epoch 35/40  Iteration 9616/11040 Training loss: 1.1744 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9617/11040 Training loss: 1.1744 0.4734 sec/batch\n",
      "Epoch 35/40  Iteration 9618/11040 Training loss: 1.1744 0.4755 sec/batch\n",
      "Epoch 35/40  Iteration 9619/11040 Training loss: 1.1745 0.4770 sec/batch\n",
      "Epoch 35/40  Iteration 9620/11040 Training loss: 1.1746 0.4895 sec/batch\n",
      "Epoch 35/40  Iteration 9621/11040 Training loss: 1.1746 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9622/11040 Training loss: 1.1747 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9623/11040 Training loss: 1.1748 0.4751 sec/batch\n",
      "Epoch 35/40  Iteration 9624/11040 Training loss: 1.1749 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9625/11040 Training loss: 1.1750 0.4741 sec/batch\n",
      "Epoch 35/40  Iteration 9626/11040 Training loss: 1.1750 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9627/11040 Training loss: 1.1751 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9628/11040 Training loss: 1.1751 0.4739 sec/batch\n",
      "Epoch 35/40  Iteration 9629/11040 Training loss: 1.1750 0.4764 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40  Iteration 9630/11040 Training loss: 1.1750 0.4750 sec/batch\n",
      "Epoch 35/40  Iteration 9631/11040 Training loss: 1.1750 0.4731 sec/batch\n",
      "Epoch 35/40  Iteration 9632/11040 Training loss: 1.1750 0.4788 sec/batch\n",
      "Epoch 35/40  Iteration 9633/11040 Training loss: 1.1750 0.4740 sec/batch\n",
      "Epoch 35/40  Iteration 9634/11040 Training loss: 1.1750 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9635/11040 Training loss: 1.1750 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9636/11040 Training loss: 1.1750 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9637/11040 Training loss: 1.1751 0.4745 sec/batch\n",
      "Epoch 35/40  Iteration 9638/11040 Training loss: 1.1751 0.4787 sec/batch\n",
      "Epoch 35/40  Iteration 9639/11040 Training loss: 1.1751 0.4736 sec/batch\n",
      "Epoch 35/40  Iteration 9640/11040 Training loss: 1.1753 0.4740 sec/batch\n",
      "Epoch 35/40  Iteration 9641/11040 Training loss: 1.1753 0.4768 sec/batch\n",
      "Epoch 35/40  Iteration 9642/11040 Training loss: 1.1754 0.4726 sec/batch\n",
      "Epoch 35/40  Iteration 9643/11040 Training loss: 1.1754 0.4757 sec/batch\n",
      "Epoch 35/40  Iteration 9644/11040 Training loss: 1.1755 0.4596 sec/batch\n",
      "Epoch 35/40  Iteration 9645/11040 Training loss: 1.1756 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9646/11040 Training loss: 1.1758 0.4746 sec/batch\n",
      "Epoch 35/40  Iteration 9647/11040 Training loss: 1.1759 0.4752 sec/batch\n",
      "Epoch 35/40  Iteration 9648/11040 Training loss: 1.1760 0.4607 sec/batch\n",
      "Epoch 35/40  Iteration 9649/11040 Training loss: 1.1759 0.4617 sec/batch\n",
      "Epoch 35/40  Iteration 9650/11040 Training loss: 1.1761 0.4725 sec/batch\n",
      "Epoch 35/40  Iteration 9651/11040 Training loss: 1.1763 0.4736 sec/batch\n",
      "Epoch 35/40  Iteration 9652/11040 Training loss: 1.1764 0.4744 sec/batch\n",
      "Epoch 35/40  Iteration 9653/11040 Training loss: 1.1765 0.4655 sec/batch\n",
      "Epoch 35/40  Iteration 9654/11040 Training loss: 1.1766 0.4699 sec/batch\n",
      "Epoch 35/40  Iteration 9655/11040 Training loss: 1.1770 0.4734 sec/batch\n",
      "Epoch 35/40  Iteration 9656/11040 Training loss: 1.1772 0.4753 sec/batch\n",
      "Epoch 35/40  Iteration 9657/11040 Training loss: 1.1773 0.4743 sec/batch\n",
      "Epoch 35/40  Iteration 9658/11040 Training loss: 1.1774 0.4764 sec/batch\n",
      "Epoch 35/40  Iteration 9659/11040 Training loss: 1.1774 0.4765 sec/batch\n",
      "Epoch 35/40  Iteration 9660/11040 Training loss: 1.1775 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9661/11040 Training loss: 1.2624 0.4768 sec/batch\n",
      "Epoch 36/40  Iteration 9662/11040 Training loss: 1.2233 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9663/11040 Training loss: 1.2200 0.4731 sec/batch\n",
      "Epoch 36/40  Iteration 9664/11040 Training loss: 1.2151 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9665/11040 Training loss: 1.2112 0.4754 sec/batch\n",
      "Epoch 36/40  Iteration 9666/11040 Training loss: 1.2069 0.4631 sec/batch\n",
      "Epoch 36/40  Iteration 9667/11040 Training loss: 1.1995 0.4727 sec/batch\n",
      "Epoch 36/40  Iteration 9668/11040 Training loss: 1.1952 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9669/11040 Training loss: 1.1896 0.4636 sec/batch\n",
      "Epoch 36/40  Iteration 9670/11040 Training loss: 1.1887 0.4725 sec/batch\n",
      "Epoch 36/40  Iteration 9671/11040 Training loss: 1.1840 0.4727 sec/batch\n",
      "Epoch 36/40  Iteration 9672/11040 Training loss: 1.1824 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9673/11040 Training loss: 1.1790 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9674/11040 Training loss: 1.1772 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9675/11040 Training loss: 1.1767 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9676/11040 Training loss: 1.1767 0.4742 sec/batch\n",
      "Epoch 36/40  Iteration 9677/11040 Training loss: 1.1740 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9678/11040 Training loss: 1.1723 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9679/11040 Training loss: 1.1712 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9680/11040 Training loss: 1.1713 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9681/11040 Training loss: 1.1731 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9682/11040 Training loss: 1.1738 0.4914 sec/batch\n",
      "Epoch 36/40  Iteration 9683/11040 Training loss: 1.1732 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9684/11040 Training loss: 1.1737 0.4771 sec/batch\n",
      "Epoch 36/40  Iteration 9685/11040 Training loss: 1.1728 0.4736 sec/batch\n",
      "Epoch 36/40  Iteration 9686/11040 Training loss: 1.1724 0.4768 sec/batch\n",
      "Epoch 36/40  Iteration 9687/11040 Training loss: 1.1719 0.4734 sec/batch\n",
      "Epoch 36/40  Iteration 9688/11040 Training loss: 1.1721 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9689/11040 Training loss: 1.1721 0.4774 sec/batch\n",
      "Epoch 36/40  Iteration 9690/11040 Training loss: 1.1723 0.4750 sec/batch\n",
      "Epoch 36/40  Iteration 9691/11040 Training loss: 1.1727 0.4754 sec/batch\n",
      "Epoch 36/40  Iteration 9692/11040 Training loss: 1.1723 0.4750 sec/batch\n",
      "Epoch 36/40  Iteration 9693/11040 Training loss: 1.1715 0.4914 sec/batch\n",
      "Epoch 36/40  Iteration 9694/11040 Training loss: 1.1710 0.4757 sec/batch\n",
      "Epoch 36/40  Iteration 9695/11040 Training loss: 1.1709 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9696/11040 Training loss: 1.1718 0.4764 sec/batch\n",
      "Epoch 36/40  Iteration 9697/11040 Training loss: 1.1716 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9698/11040 Training loss: 1.1710 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9699/11040 Training loss: 1.1705 0.4736 sec/batch\n",
      "Epoch 36/40  Iteration 9700/11040 Training loss: 1.1706 0.4770 sec/batch\n",
      "Epoch 36/40  Iteration 9701/11040 Training loss: 1.1703 0.4850 sec/batch\n",
      "Epoch 36/40  Iteration 9702/11040 Training loss: 1.1693 0.4791 sec/batch\n",
      "Epoch 36/40  Iteration 9703/11040 Training loss: 1.1696 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9704/11040 Training loss: 1.1685 0.4600 sec/batch\n",
      "Epoch 36/40  Iteration 9705/11040 Training loss: 1.1679 0.4756 sec/batch\n",
      "Epoch 36/40  Iteration 9706/11040 Training loss: 1.1672 0.4733 sec/batch\n",
      "Epoch 36/40  Iteration 9707/11040 Training loss: 1.1665 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9708/11040 Training loss: 1.1666 0.4770 sec/batch\n",
      "Epoch 36/40  Iteration 9709/11040 Training loss: 1.1664 0.4757 sec/batch\n",
      "Epoch 36/40  Iteration 9710/11040 Training loss: 1.1670 0.4891 sec/batch\n",
      "Epoch 36/40  Iteration 9711/11040 Training loss: 1.1668 0.4766 sec/batch\n",
      "Epoch 36/40  Iteration 9712/11040 Training loss: 1.1666 0.4753 sec/batch\n",
      "Epoch 36/40  Iteration 9713/11040 Training loss: 1.1668 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9714/11040 Training loss: 1.1665 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9715/11040 Training loss: 1.1664 0.4761 sec/batch\n",
      "Epoch 36/40  Iteration 9716/11040 Training loss: 1.1663 0.4749 sec/batch\n",
      "Epoch 36/40  Iteration 9717/11040 Training loss: 1.1660 0.4730 sec/batch\n",
      "Epoch 36/40  Iteration 9718/11040 Training loss: 1.1658 0.4606 sec/batch\n",
      "Epoch 36/40  Iteration 9719/11040 Training loss: 1.1660 0.4748 sec/batch\n",
      "Epoch 36/40  Iteration 9720/11040 Training loss: 1.1655 0.4831 sec/batch\n",
      "Epoch 36/40  Iteration 9721/11040 Training loss: 1.1655 0.4790 sec/batch\n",
      "Epoch 36/40  Iteration 9722/11040 Training loss: 1.1654 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9723/11040 Training loss: 1.1656 0.4736 sec/batch\n",
      "Epoch 36/40  Iteration 9724/11040 Training loss: 1.1655 0.4650 sec/batch\n",
      "Epoch 36/40  Iteration 9725/11040 Training loss: 1.1658 0.4710 sec/batch\n",
      "Epoch 36/40  Iteration 9726/11040 Training loss: 1.1657 0.4730 sec/batch\n",
      "Epoch 36/40  Iteration 9727/11040 Training loss: 1.1657 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9728/11040 Training loss: 1.1658 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9729/11040 Training loss: 1.1656 0.4608 sec/batch\n",
      "Epoch 36/40  Iteration 9730/11040 Training loss: 1.1656 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9731/11040 Training loss: 1.1653 0.4686 sec/batch\n",
      "Epoch 36/40  Iteration 9732/11040 Training loss: 1.1649 0.4674 sec/batch\n",
      "Epoch 36/40  Iteration 9733/11040 Training loss: 1.1645 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9734/11040 Training loss: 1.1640 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9735/11040 Training loss: 1.1637 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9736/11040 Training loss: 1.1639 0.4775 sec/batch\n",
      "Epoch 36/40  Iteration 9737/11040 Training loss: 1.1634 0.4733 sec/batch\n",
      "Epoch 36/40  Iteration 9738/11040 Training loss: 1.1630 0.4769 sec/batch\n",
      "Epoch 36/40  Iteration 9739/11040 Training loss: 1.1628 0.4733 sec/batch\n",
      "Epoch 36/40  Iteration 9740/11040 Training loss: 1.1627 0.4751 sec/batch\n",
      "Epoch 36/40  Iteration 9741/11040 Training loss: 1.1624 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9742/11040 Training loss: 1.1625 0.4782 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40  Iteration 9743/11040 Training loss: 1.1624 0.4720 sec/batch\n",
      "Epoch 36/40  Iteration 9744/11040 Training loss: 1.1622 0.4751 sec/batch\n",
      "Epoch 36/40  Iteration 9745/11040 Training loss: 1.1620 0.4895 sec/batch\n",
      "Epoch 36/40  Iteration 9746/11040 Training loss: 1.1618 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9747/11040 Training loss: 1.1611 0.4770 sec/batch\n",
      "Epoch 36/40  Iteration 9748/11040 Training loss: 1.1600 0.4739 sec/batch\n",
      "Epoch 36/40  Iteration 9749/11040 Training loss: 1.1600 0.4613 sec/batch\n",
      "Epoch 36/40  Iteration 9750/11040 Training loss: 1.1601 0.4735 sec/batch\n",
      "Epoch 36/40  Iteration 9751/11040 Training loss: 1.1601 0.4906 sec/batch\n",
      "Epoch 36/40  Iteration 9752/11040 Training loss: 1.1599 0.4749 sec/batch\n",
      "Epoch 36/40  Iteration 9753/11040 Training loss: 1.1597 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9754/11040 Training loss: 1.1600 0.4915 sec/batch\n",
      "Epoch 36/40  Iteration 9755/11040 Training loss: 1.1605 0.4749 sec/batch\n",
      "Epoch 36/40  Iteration 9756/11040 Training loss: 1.1608 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9757/11040 Training loss: 1.1612 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9758/11040 Training loss: 1.1616 0.4765 sec/batch\n",
      "Epoch 36/40  Iteration 9759/11040 Training loss: 1.1619 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9760/11040 Training loss: 1.1617 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9761/11040 Training loss: 1.1618 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9762/11040 Training loss: 1.1620 0.4918 sec/batch\n",
      "Epoch 36/40  Iteration 9763/11040 Training loss: 1.1617 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9764/11040 Training loss: 1.1619 0.4750 sec/batch\n",
      "Epoch 36/40  Iteration 9765/11040 Training loss: 1.1618 0.4750 sec/batch\n",
      "Epoch 36/40  Iteration 9766/11040 Training loss: 1.1614 0.4805 sec/batch\n",
      "Epoch 36/40  Iteration 9767/11040 Training loss: 1.1612 0.4856 sec/batch\n",
      "Epoch 36/40  Iteration 9768/11040 Training loss: 1.1613 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9769/11040 Training loss: 1.1616 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9770/11040 Training loss: 1.1617 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9771/11040 Training loss: 1.1617 0.4781 sec/batch\n",
      "Epoch 36/40  Iteration 9772/11040 Training loss: 1.1618 0.4723 sec/batch\n",
      "Epoch 36/40  Iteration 9773/11040 Training loss: 1.1619 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9774/11040 Training loss: 1.1618 0.4762 sec/batch\n",
      "Epoch 36/40  Iteration 9775/11040 Training loss: 1.1620 0.4726 sec/batch\n",
      "Epoch 36/40  Iteration 9776/11040 Training loss: 1.1623 0.4765 sec/batch\n",
      "Epoch 36/40  Iteration 9777/11040 Training loss: 1.1621 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9778/11040 Training loss: 1.1624 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9779/11040 Training loss: 1.1625 0.4742 sec/batch\n",
      "Epoch 36/40  Iteration 9780/11040 Training loss: 1.1625 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9781/11040 Training loss: 1.1624 0.4607 sec/batch\n",
      "Epoch 36/40  Iteration 9782/11040 Training loss: 1.1626 0.4597 sec/batch\n",
      "Epoch 36/40  Iteration 9783/11040 Training loss: 1.1628 0.4591 sec/batch\n",
      "Epoch 36/40  Iteration 9784/11040 Training loss: 1.1633 0.4733 sec/batch\n",
      "Epoch 36/40  Iteration 9785/11040 Training loss: 1.1634 0.4748 sec/batch\n",
      "Epoch 36/40  Iteration 9786/11040 Training loss: 1.1637 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9787/11040 Training loss: 1.1637 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9788/11040 Training loss: 1.1637 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9789/11040 Training loss: 1.1635 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9790/11040 Training loss: 1.1638 0.4742 sec/batch\n",
      "Epoch 36/40  Iteration 9791/11040 Training loss: 1.1639 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9792/11040 Training loss: 1.1638 0.4757 sec/batch\n",
      "Epoch 36/40  Iteration 9793/11040 Training loss: 1.1638 0.4597 sec/batch\n",
      "Epoch 36/40  Iteration 9794/11040 Training loss: 1.1638 0.4581 sec/batch\n",
      "Epoch 36/40  Iteration 9795/11040 Training loss: 1.1639 0.4748 sec/batch\n",
      "Epoch 36/40  Iteration 9796/11040 Training loss: 1.1639 0.4735 sec/batch\n",
      "Epoch 36/40  Iteration 9797/11040 Training loss: 1.1638 0.4761 sec/batch\n",
      "Epoch 36/40  Iteration 9798/11040 Training loss: 1.1639 0.4753 sec/batch\n",
      "Epoch 36/40  Iteration 9799/11040 Training loss: 1.1640 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9800/11040 Training loss: 1.1641 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9801/11040 Training loss: 1.1641 0.4734 sec/batch\n",
      "Epoch 36/40  Iteration 9802/11040 Training loss: 1.1643 0.4606 sec/batch\n",
      "Epoch 36/40  Iteration 9803/11040 Training loss: 1.1643 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9804/11040 Training loss: 1.1643 0.4732 sec/batch\n",
      "Epoch 36/40  Iteration 9805/11040 Training loss: 1.1643 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9806/11040 Training loss: 1.1643 0.4754 sec/batch\n",
      "Epoch 36/40  Iteration 9807/11040 Training loss: 1.1643 0.4874 sec/batch\n",
      "Epoch 36/40  Iteration 9808/11040 Training loss: 1.1643 0.4774 sec/batch\n",
      "Epoch 36/40  Iteration 9809/11040 Training loss: 1.1643 0.4767 sec/batch\n",
      "Epoch 36/40  Iteration 9810/11040 Training loss: 1.1643 0.4748 sec/batch\n",
      "Epoch 36/40  Iteration 9811/11040 Training loss: 1.1643 0.4603 sec/batch\n",
      "Epoch 36/40  Iteration 9812/11040 Training loss: 1.1645 0.4731 sec/batch\n",
      "Epoch 36/40  Iteration 9813/11040 Training loss: 1.1646 0.4745 sec/batch\n",
      "Epoch 36/40  Iteration 9814/11040 Training loss: 1.1648 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9815/11040 Training loss: 1.1649 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9816/11040 Training loss: 1.1649 0.4733 sec/batch\n",
      "Epoch 36/40  Iteration 9817/11040 Training loss: 1.1647 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9818/11040 Training loss: 1.1650 0.4765 sec/batch\n",
      "Epoch 36/40  Iteration 9819/11040 Training loss: 1.1651 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9820/11040 Training loss: 1.1651 0.4745 sec/batch\n",
      "Epoch 36/40  Iteration 9821/11040 Training loss: 1.1651 0.4745 sec/batch\n",
      "Epoch 36/40  Iteration 9822/11040 Training loss: 1.1652 0.4764 sec/batch\n",
      "Epoch 36/40  Iteration 9823/11040 Training loss: 1.1654 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9824/11040 Training loss: 1.1656 0.4751 sec/batch\n",
      "Epoch 36/40  Iteration 9825/11040 Training loss: 1.1658 0.4753 sec/batch\n",
      "Epoch 36/40  Iteration 9826/11040 Training loss: 1.1661 0.4618 sec/batch\n",
      "Epoch 36/40  Iteration 9827/11040 Training loss: 1.1661 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9828/11040 Training loss: 1.1661 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9829/11040 Training loss: 1.1662 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9830/11040 Training loss: 1.1662 0.4774 sec/batch\n",
      "Epoch 36/40  Iteration 9831/11040 Training loss: 1.1664 0.4890 sec/batch\n",
      "Epoch 36/40  Iteration 9832/11040 Training loss: 1.1665 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9833/11040 Training loss: 1.1665 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9834/11040 Training loss: 1.1665 0.4767 sec/batch\n",
      "Epoch 36/40  Iteration 9835/11040 Training loss: 1.1666 0.4735 sec/batch\n",
      "Epoch 36/40  Iteration 9836/11040 Training loss: 1.1666 0.4754 sec/batch\n",
      "Epoch 36/40  Iteration 9837/11040 Training loss: 1.1667 0.4754 sec/batch\n",
      "Epoch 36/40  Iteration 9838/11040 Training loss: 1.1667 0.4598 sec/batch\n",
      "Epoch 36/40  Iteration 9839/11040 Training loss: 1.1670 0.4730 sec/batch\n",
      "Epoch 36/40  Iteration 9840/11040 Training loss: 1.1671 0.4753 sec/batch\n",
      "Epoch 36/40  Iteration 9841/11040 Training loss: 1.1671 0.4756 sec/batch\n",
      "Epoch 36/40  Iteration 9842/11040 Training loss: 1.1672 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9843/11040 Training loss: 1.1671 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9844/11040 Training loss: 1.1673 0.4749 sec/batch\n",
      "Epoch 36/40  Iteration 9845/11040 Training loss: 1.1674 0.4755 sec/batch\n",
      "Epoch 36/40  Iteration 9846/11040 Training loss: 1.1674 0.4758 sec/batch\n",
      "Epoch 36/40  Iteration 9847/11040 Training loss: 1.1676 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9848/11040 Training loss: 1.1676 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9849/11040 Training loss: 1.1676 0.4757 sec/batch\n",
      "Epoch 36/40  Iteration 9850/11040 Training loss: 1.1677 0.4750 sec/batch\n",
      "Epoch 36/40  Iteration 9851/11040 Training loss: 1.1676 0.4742 sec/batch\n",
      "Epoch 36/40  Iteration 9852/11040 Training loss: 1.1676 0.4736 sec/batch\n",
      "Epoch 36/40  Iteration 9853/11040 Training loss: 1.1676 0.4769 sec/batch\n",
      "Epoch 36/40  Iteration 9854/11040 Training loss: 1.1677 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9855/11040 Training loss: 1.1679 0.4762 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40  Iteration 9856/11040 Training loss: 1.1678 0.4725 sec/batch\n",
      "Epoch 36/40  Iteration 9857/11040 Training loss: 1.1679 0.4770 sec/batch\n",
      "Epoch 36/40  Iteration 9858/11040 Training loss: 1.1679 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9859/11040 Training loss: 1.1681 0.4753 sec/batch\n",
      "Epoch 36/40  Iteration 9860/11040 Training loss: 1.1682 0.4617 sec/batch\n",
      "Epoch 36/40  Iteration 9861/11040 Training loss: 1.1684 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9862/11040 Training loss: 1.1686 0.4899 sec/batch\n",
      "Epoch 36/40  Iteration 9863/11040 Training loss: 1.1687 0.4750 sec/batch\n",
      "Epoch 36/40  Iteration 9864/11040 Training loss: 1.1688 0.4921 sec/batch\n",
      "Epoch 36/40  Iteration 9865/11040 Training loss: 1.1688 0.4800 sec/batch\n",
      "Epoch 36/40  Iteration 9866/11040 Training loss: 1.1688 0.4860 sec/batch\n",
      "Epoch 36/40  Iteration 9867/11040 Training loss: 1.1688 0.4790 sec/batch\n",
      "Epoch 36/40  Iteration 9868/11040 Training loss: 1.1689 0.4851 sec/batch\n",
      "Epoch 36/40  Iteration 9869/11040 Training loss: 1.1687 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9870/11040 Training loss: 1.1687 0.4904 sec/batch\n",
      "Epoch 36/40  Iteration 9871/11040 Training loss: 1.1687 0.4801 sec/batch\n",
      "Epoch 36/40  Iteration 9872/11040 Training loss: 1.1688 0.4863 sec/batch\n",
      "Epoch 36/40  Iteration 9873/11040 Training loss: 1.1687 0.4753 sec/batch\n",
      "Epoch 36/40  Iteration 9874/11040 Training loss: 1.1687 0.4617 sec/batch\n",
      "Epoch 36/40  Iteration 9875/11040 Training loss: 1.1686 0.4739 sec/batch\n",
      "Epoch 36/40  Iteration 9876/11040 Training loss: 1.1685 0.4745 sec/batch\n",
      "Epoch 36/40  Iteration 9877/11040 Training loss: 1.1684 0.4828 sec/batch\n",
      "Epoch 36/40  Iteration 9878/11040 Training loss: 1.1683 0.4845 sec/batch\n",
      "Epoch 36/40  Iteration 9879/11040 Training loss: 1.1683 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9880/11040 Training loss: 1.1682 0.4773 sec/batch\n",
      "Epoch 36/40  Iteration 9881/11040 Training loss: 1.1681 0.4714 sec/batch\n",
      "Epoch 36/40  Iteration 9882/11040 Training loss: 1.1681 0.4824 sec/batch\n",
      "Epoch 36/40  Iteration 9883/11040 Training loss: 1.1680 0.4700 sec/batch\n",
      "Epoch 36/40  Iteration 9884/11040 Training loss: 1.1680 0.4735 sec/batch\n",
      "Epoch 36/40  Iteration 9885/11040 Training loss: 1.1680 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9886/11040 Training loss: 1.1680 0.4742 sec/batch\n",
      "Epoch 36/40  Iteration 9887/11040 Training loss: 1.1679 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9888/11040 Training loss: 1.1678 0.4928 sec/batch\n",
      "Epoch 36/40  Iteration 9889/11040 Training loss: 1.1679 0.4896 sec/batch\n",
      "Epoch 36/40  Iteration 9890/11040 Training loss: 1.1681 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9891/11040 Training loss: 1.1681 0.4905 sec/batch\n",
      "Epoch 36/40  Iteration 9892/11040 Training loss: 1.1682 0.4741 sec/batch\n",
      "Epoch 36/40  Iteration 9893/11040 Training loss: 1.1681 0.4606 sec/batch\n",
      "Epoch 36/40  Iteration 9894/11040 Training loss: 1.1680 0.4743 sec/batch\n",
      "Epoch 36/40  Iteration 9895/11040 Training loss: 1.1682 0.4756 sec/batch\n",
      "Epoch 36/40  Iteration 9896/11040 Training loss: 1.1683 0.4598 sec/batch\n",
      "Epoch 36/40  Iteration 9897/11040 Training loss: 1.1684 0.4737 sec/batch\n",
      "Epoch 36/40  Iteration 9898/11040 Training loss: 1.1685 0.4749 sec/batch\n",
      "Epoch 36/40  Iteration 9899/11040 Training loss: 1.1687 0.4763 sec/batch\n",
      "Epoch 36/40  Iteration 9900/11040 Training loss: 1.1688 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9901/11040 Training loss: 1.1689 0.4758 sec/batch\n",
      "Epoch 36/40  Iteration 9902/11040 Training loss: 1.1689 0.4760 sec/batch\n",
      "Epoch 36/40  Iteration 9903/11040 Training loss: 1.1690 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9904/11040 Training loss: 1.1690 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9905/11040 Training loss: 1.1689 0.4613 sec/batch\n",
      "Epoch 36/40  Iteration 9906/11040 Training loss: 1.1689 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9907/11040 Training loss: 1.1689 0.4912 sec/batch\n",
      "Epoch 36/40  Iteration 9908/11040 Training loss: 1.1689 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9909/11040 Training loss: 1.1690 0.4790 sec/batch\n",
      "Epoch 36/40  Iteration 9910/11040 Training loss: 1.1690 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9911/11040 Training loss: 1.1690 0.4722 sec/batch\n",
      "Epoch 36/40  Iteration 9912/11040 Training loss: 1.1690 0.4773 sec/batch\n",
      "Epoch 36/40  Iteration 9913/11040 Training loss: 1.1690 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9914/11040 Training loss: 1.1691 0.4754 sec/batch\n",
      "Epoch 36/40  Iteration 9915/11040 Training loss: 1.1691 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9916/11040 Training loss: 1.1692 0.4737 sec/batch\n",
      "Epoch 36/40  Iteration 9917/11040 Training loss: 1.1693 0.4761 sec/batch\n",
      "Epoch 36/40  Iteration 9918/11040 Training loss: 1.1694 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9919/11040 Training loss: 1.1694 0.4759 sec/batch\n",
      "Epoch 36/40  Iteration 9920/11040 Training loss: 1.1695 0.4732 sec/batch\n",
      "Epoch 36/40  Iteration 9921/11040 Training loss: 1.1696 0.4761 sec/batch\n",
      "Epoch 36/40  Iteration 9922/11040 Training loss: 1.1698 0.4744 sec/batch\n",
      "Epoch 36/40  Iteration 9923/11040 Training loss: 1.1699 0.4756 sec/batch\n",
      "Epoch 36/40  Iteration 9924/11040 Training loss: 1.1700 0.4609 sec/batch\n",
      "Epoch 36/40  Iteration 9925/11040 Training loss: 1.1700 0.4758 sec/batch\n",
      "Epoch 36/40  Iteration 9926/11040 Training loss: 1.1702 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9927/11040 Training loss: 1.1704 0.4747 sec/batch\n",
      "Epoch 36/40  Iteration 9928/11040 Training loss: 1.1705 0.4771 sec/batch\n",
      "Epoch 36/40  Iteration 9929/11040 Training loss: 1.1706 0.4746 sec/batch\n",
      "Epoch 36/40  Iteration 9930/11040 Training loss: 1.1707 0.4777 sec/batch\n",
      "Epoch 36/40  Iteration 9931/11040 Training loss: 1.1711 0.4727 sec/batch\n",
      "Epoch 36/40  Iteration 9932/11040 Training loss: 1.1713 0.4752 sec/batch\n",
      "Epoch 36/40  Iteration 9933/11040 Training loss: 1.1715 0.4740 sec/batch\n",
      "Epoch 36/40  Iteration 9934/11040 Training loss: 1.1716 0.4778 sec/batch\n",
      "Epoch 36/40  Iteration 9935/11040 Training loss: 1.1717 0.4905 sec/batch\n",
      "Epoch 36/40  Iteration 9936/11040 Training loss: 1.1718 0.4741 sec/batch\n",
      "Epoch 37/40  Iteration 9937/11040 Training loss: 1.2533 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 9938/11040 Training loss: 1.2140 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 9939/11040 Training loss: 1.2111 0.4748 sec/batch\n",
      "Epoch 37/40  Iteration 9940/11040 Training loss: 1.2084 0.4816 sec/batch\n",
      "Epoch 37/40  Iteration 9941/11040 Training loss: 1.2059 0.4857 sec/batch\n",
      "Epoch 37/40  Iteration 9942/11040 Training loss: 1.2021 0.4732 sec/batch\n",
      "Epoch 37/40  Iteration 9943/11040 Training loss: 1.1928 0.4607 sec/batch\n",
      "Epoch 37/40  Iteration 9944/11040 Training loss: 1.1905 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 9945/11040 Training loss: 1.1866 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 9946/11040 Training loss: 1.1841 0.4608 sec/batch\n",
      "Epoch 37/40  Iteration 9947/11040 Training loss: 1.1797 0.4623 sec/batch\n",
      "Epoch 37/40  Iteration 9948/11040 Training loss: 1.1785 0.4668 sec/batch\n",
      "Epoch 37/40  Iteration 9949/11040 Training loss: 1.1752 0.4772 sec/batch\n",
      "Epoch 37/40  Iteration 9950/11040 Training loss: 1.1725 0.4739 sec/batch\n",
      "Epoch 37/40  Iteration 9951/11040 Training loss: 1.1713 0.4739 sec/batch\n",
      "Epoch 37/40  Iteration 9952/11040 Training loss: 1.1717 0.4740 sec/batch\n",
      "Epoch 37/40  Iteration 9953/11040 Training loss: 1.1687 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 9954/11040 Training loss: 1.1668 0.4771 sec/batch\n",
      "Epoch 37/40  Iteration 9955/11040 Training loss: 1.1658 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 9956/11040 Training loss: 1.1658 0.4741 sec/batch\n",
      "Epoch 37/40  Iteration 9957/11040 Training loss: 1.1679 0.4763 sec/batch\n",
      "Epoch 37/40  Iteration 9958/11040 Training loss: 1.1684 0.4740 sec/batch\n",
      "Epoch 37/40  Iteration 9959/11040 Training loss: 1.1676 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 9960/11040 Training loss: 1.1679 0.4759 sec/batch\n",
      "Epoch 37/40  Iteration 9961/11040 Training loss: 1.1670 0.4831 sec/batch\n",
      "Epoch 37/40  Iteration 9962/11040 Training loss: 1.1661 0.4811 sec/batch\n",
      "Epoch 37/40  Iteration 9963/11040 Training loss: 1.1659 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 9964/11040 Training loss: 1.1658 0.4753 sec/batch\n",
      "Epoch 37/40  Iteration 9965/11040 Training loss: 1.1660 0.4656 sec/batch\n",
      "Epoch 37/40  Iteration 9966/11040 Training loss: 1.1662 0.4853 sec/batch\n",
      "Epoch 37/40  Iteration 9967/11040 Training loss: 1.1662 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 9968/11040 Training loss: 1.1661 0.4770 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40  Iteration 9969/11040 Training loss: 1.1652 0.4771 sec/batch\n",
      "Epoch 37/40  Iteration 9970/11040 Training loss: 1.1652 0.4694 sec/batch\n",
      "Epoch 37/40  Iteration 9971/11040 Training loss: 1.1650 0.4796 sec/batch\n",
      "Epoch 37/40  Iteration 9972/11040 Training loss: 1.1657 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 9973/11040 Training loss: 1.1656 0.4769 sec/batch\n",
      "Epoch 37/40  Iteration 9974/11040 Training loss: 1.1649 0.4738 sec/batch\n",
      "Epoch 37/40  Iteration 9975/11040 Training loss: 1.1644 0.4782 sec/batch\n",
      "Epoch 37/40  Iteration 9976/11040 Training loss: 1.1647 0.4732 sec/batch\n",
      "Epoch 37/40  Iteration 9977/11040 Training loss: 1.1644 0.4744 sec/batch\n",
      "Epoch 37/40  Iteration 9978/11040 Training loss: 1.1634 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 9979/11040 Training loss: 1.1637 0.4615 sec/batch\n",
      "Epoch 37/40  Iteration 9980/11040 Training loss: 1.1628 0.4788 sec/batch\n",
      "Epoch 37/40  Iteration 9981/11040 Training loss: 1.1622 0.4658 sec/batch\n",
      "Epoch 37/40  Iteration 9982/11040 Training loss: 1.1616 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 9983/11040 Training loss: 1.1607 0.4754 sec/batch\n",
      "Epoch 37/40  Iteration 9984/11040 Training loss: 1.1608 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 9985/11040 Training loss: 1.1605 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 9986/11040 Training loss: 1.1610 0.4756 sec/batch\n",
      "Epoch 37/40  Iteration 9987/11040 Training loss: 1.1607 0.4752 sec/batch\n",
      "Epoch 37/40  Iteration 9988/11040 Training loss: 1.1605 0.4766 sec/batch\n",
      "Epoch 37/40  Iteration 9989/11040 Training loss: 1.1609 0.4733 sec/batch\n",
      "Epoch 37/40  Iteration 9990/11040 Training loss: 1.1606 0.4771 sec/batch\n",
      "Epoch 37/40  Iteration 9991/11040 Training loss: 1.1607 0.4741 sec/batch\n",
      "Epoch 37/40  Iteration 9992/11040 Training loss: 1.1607 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 9993/11040 Training loss: 1.1604 0.4786 sec/batch\n",
      "Epoch 37/40  Iteration 9994/11040 Training loss: 1.1602 0.4710 sec/batch\n",
      "Epoch 37/40  Iteration 9995/11040 Training loss: 1.1604 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 9996/11040 Training loss: 1.1599 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 9997/11040 Training loss: 1.1601 0.4744 sec/batch\n",
      "Epoch 37/40  Iteration 9998/11040 Training loss: 1.1600 0.4763 sec/batch\n",
      "Epoch 37/40  Iteration 9999/11040 Training loss: 1.1601 0.4800 sec/batch\n",
      "Epoch 37/40  Iteration 10000/11040 Training loss: 1.1602 0.4711 sec/batch\n",
      "Validation loss: 1.23603 Saving checkpoint!\n",
      "Epoch 37/40  Iteration 10001/11040 Training loss: 1.1634 0.4688 sec/batch\n",
      "Epoch 37/40  Iteration 10002/11040 Training loss: 1.1633 0.4697 sec/batch\n",
      "Epoch 37/40  Iteration 10003/11040 Training loss: 1.1633 0.4717 sec/batch\n",
      "Epoch 37/40  Iteration 10004/11040 Training loss: 1.1633 0.4657 sec/batch\n",
      "Epoch 37/40  Iteration 10005/11040 Training loss: 1.1631 0.4695 sec/batch\n",
      "Epoch 37/40  Iteration 10006/11040 Training loss: 1.1629 0.4765 sec/batch\n",
      "Epoch 37/40  Iteration 10007/11040 Training loss: 1.1626 0.4720 sec/batch\n",
      "Epoch 37/40  Iteration 10008/11040 Training loss: 1.1620 0.4606 sec/batch\n",
      "Epoch 37/40  Iteration 10009/11040 Training loss: 1.1616 0.4585 sec/batch\n",
      "Epoch 37/40  Iteration 10010/11040 Training loss: 1.1612 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 10011/11040 Training loss: 1.1611 0.4757 sec/batch\n",
      "Epoch 37/40  Iteration 10012/11040 Training loss: 1.1615 0.4741 sec/batch\n",
      "Epoch 37/40  Iteration 10013/11040 Training loss: 1.1610 0.4592 sec/batch\n",
      "Epoch 37/40  Iteration 10014/11040 Training loss: 1.1605 0.4611 sec/batch\n",
      "Epoch 37/40  Iteration 10015/11040 Training loss: 1.1604 0.4661 sec/batch\n",
      "Epoch 37/40  Iteration 10016/11040 Training loss: 1.1605 0.4809 sec/batch\n",
      "Epoch 37/40  Iteration 10017/11040 Training loss: 1.1602 0.4752 sec/batch\n",
      "Epoch 37/40  Iteration 10018/11040 Training loss: 1.1603 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 10019/11040 Training loss: 1.1602 0.4753 sec/batch\n",
      "Epoch 37/40  Iteration 10020/11040 Training loss: 1.1601 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 10021/11040 Training loss: 1.1599 0.4595 sec/batch\n",
      "Epoch 37/40  Iteration 10022/11040 Training loss: 1.1599 0.4750 sec/batch\n",
      "Epoch 37/40  Iteration 10023/11040 Training loss: 1.1593 0.4760 sec/batch\n",
      "Epoch 37/40  Iteration 10024/11040 Training loss: 1.1583 0.4731 sec/batch\n",
      "Epoch 37/40  Iteration 10025/11040 Training loss: 1.1581 0.4596 sec/batch\n",
      "Epoch 37/40  Iteration 10026/11040 Training loss: 1.1582 0.4592 sec/batch\n",
      "Epoch 37/40  Iteration 10027/11040 Training loss: 1.1583 0.4749 sec/batch\n",
      "Epoch 37/40  Iteration 10028/11040 Training loss: 1.1581 0.4782 sec/batch\n",
      "Epoch 37/40  Iteration 10029/11040 Training loss: 1.1580 0.4718 sec/batch\n",
      "Epoch 37/40  Iteration 10030/11040 Training loss: 1.1585 0.4761 sec/batch\n",
      "Epoch 37/40  Iteration 10031/11040 Training loss: 1.1589 0.4724 sec/batch\n",
      "Epoch 37/40  Iteration 10032/11040 Training loss: 1.1592 0.4784 sec/batch\n",
      "Epoch 37/40  Iteration 10033/11040 Training loss: 1.1597 0.4890 sec/batch\n",
      "Epoch 37/40  Iteration 10034/11040 Training loss: 1.1601 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 10035/11040 Training loss: 1.1604 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10036/11040 Training loss: 1.1602 0.4622 sec/batch\n",
      "Epoch 37/40  Iteration 10037/11040 Training loss: 1.1603 0.4734 sec/batch\n",
      "Epoch 37/40  Iteration 10038/11040 Training loss: 1.1605 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 10039/11040 Training loss: 1.1601 0.4763 sec/batch\n",
      "Epoch 37/40  Iteration 10040/11040 Training loss: 1.1602 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 10041/11040 Training loss: 1.1600 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 10042/11040 Training loss: 1.1598 0.4741 sec/batch\n",
      "Epoch 37/40  Iteration 10043/11040 Training loss: 1.1596 0.4775 sec/batch\n",
      "Epoch 37/40  Iteration 10044/11040 Training loss: 1.1598 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10045/11040 Training loss: 1.1602 0.4752 sec/batch\n",
      "Epoch 37/40  Iteration 10046/11040 Training loss: 1.1602 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 10047/11040 Training loss: 1.1603 0.4739 sec/batch\n",
      "Epoch 37/40  Iteration 10048/11040 Training loss: 1.1605 0.4618 sec/batch\n",
      "Epoch 37/40  Iteration 10049/11040 Training loss: 1.1606 0.4758 sec/batch\n",
      "Epoch 37/40  Iteration 10050/11040 Training loss: 1.1606 0.4905 sec/batch\n",
      "Epoch 37/40  Iteration 10051/11040 Training loss: 1.1609 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 10052/11040 Training loss: 1.1613 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10053/11040 Training loss: 1.1611 0.4756 sec/batch\n",
      "Epoch 37/40  Iteration 10054/11040 Training loss: 1.1612 0.4744 sec/batch\n",
      "Epoch 37/40  Iteration 10055/11040 Training loss: 1.1614 0.4764 sec/batch\n",
      "Epoch 37/40  Iteration 10056/11040 Training loss: 1.1615 0.4731 sec/batch\n",
      "Epoch 37/40  Iteration 10057/11040 Training loss: 1.1615 0.4786 sec/batch\n",
      "Epoch 37/40  Iteration 10058/11040 Training loss: 1.1616 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 10059/11040 Training loss: 1.1619 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10060/11040 Training loss: 1.1624 0.4748 sec/batch\n",
      "Epoch 37/40  Iteration 10061/11040 Training loss: 1.1625 0.4596 sec/batch\n",
      "Epoch 37/40  Iteration 10062/11040 Training loss: 1.1628 0.4734 sec/batch\n",
      "Epoch 37/40  Iteration 10063/11040 Training loss: 1.1628 0.4758 sec/batch\n",
      "Epoch 37/40  Iteration 10064/11040 Training loss: 1.1629 0.4754 sec/batch\n",
      "Epoch 37/40  Iteration 10065/11040 Training loss: 1.1627 0.4754 sec/batch\n",
      "Epoch 37/40  Iteration 10066/11040 Training loss: 1.1630 0.4766 sec/batch\n",
      "Epoch 37/40  Iteration 10067/11040 Training loss: 1.1631 0.4738 sec/batch\n",
      "Epoch 37/40  Iteration 10068/11040 Training loss: 1.1629 0.4756 sec/batch\n",
      "Epoch 37/40  Iteration 10069/11040 Training loss: 1.1628 0.4748 sec/batch\n",
      "Epoch 37/40  Iteration 10070/11040 Training loss: 1.1627 0.4733 sec/batch\n",
      "Epoch 37/40  Iteration 10071/11040 Training loss: 1.1629 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 10072/11040 Training loss: 1.1630 0.4609 sec/batch\n",
      "Epoch 37/40  Iteration 10073/11040 Training loss: 1.1629 0.4594 sec/batch\n",
      "Epoch 37/40  Iteration 10074/11040 Training loss: 1.1629 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 10075/11040 Training loss: 1.1631 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10076/11040 Training loss: 1.1632 0.4763 sec/batch\n",
      "Epoch 37/40  Iteration 10077/11040 Training loss: 1.1632 0.4795 sec/batch\n",
      "Epoch 37/40  Iteration 10078/11040 Training loss: 1.1634 0.4874 sec/batch\n",
      "Epoch 37/40  Iteration 10079/11040 Training loss: 1.1634 0.4752 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40  Iteration 10080/11040 Training loss: 1.1633 0.4750 sec/batch\n",
      "Epoch 37/40  Iteration 10081/11040 Training loss: 1.1633 0.4735 sec/batch\n",
      "Epoch 37/40  Iteration 10082/11040 Training loss: 1.1633 0.4774 sec/batch\n",
      "Epoch 37/40  Iteration 10083/11040 Training loss: 1.1633 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10084/11040 Training loss: 1.1633 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10085/11040 Training loss: 1.1632 0.4839 sec/batch\n",
      "Epoch 37/40  Iteration 10086/11040 Training loss: 1.1633 0.4666 sec/batch\n",
      "Epoch 37/40  Iteration 10087/11040 Training loss: 1.1632 0.4731 sec/batch\n",
      "Epoch 37/40  Iteration 10088/11040 Training loss: 1.1633 0.4744 sec/batch\n",
      "Epoch 37/40  Iteration 10089/11040 Training loss: 1.1635 0.4764 sec/batch\n",
      "Epoch 37/40  Iteration 10090/11040 Training loss: 1.1636 0.4770 sec/batch\n",
      "Epoch 37/40  Iteration 10091/11040 Training loss: 1.1637 0.4737 sec/batch\n",
      "Epoch 37/40  Iteration 10092/11040 Training loss: 1.1637 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10093/11040 Training loss: 1.1636 0.4741 sec/batch\n",
      "Epoch 37/40  Iteration 10094/11040 Training loss: 1.1637 0.4772 sec/batch\n",
      "Epoch 37/40  Iteration 10095/11040 Training loss: 1.1639 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 10096/11040 Training loss: 1.1639 0.4744 sec/batch\n",
      "Epoch 37/40  Iteration 10097/11040 Training loss: 1.1640 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10098/11040 Training loss: 1.1640 0.4609 sec/batch\n",
      "Epoch 37/40  Iteration 10099/11040 Training loss: 1.1641 0.4731 sec/batch\n",
      "Epoch 37/40  Iteration 10100/11040 Training loss: 1.1643 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10101/11040 Training loss: 1.1645 0.4768 sec/batch\n",
      "Epoch 37/40  Iteration 10102/11040 Training loss: 1.1648 0.4736 sec/batch\n",
      "Epoch 37/40  Iteration 10103/11040 Training loss: 1.1648 0.4621 sec/batch\n",
      "Epoch 37/40  Iteration 10104/11040 Training loss: 1.1648 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 10105/11040 Training loss: 1.1649 0.4750 sec/batch\n",
      "Epoch 37/40  Iteration 10106/11040 Training loss: 1.1648 0.4915 sec/batch\n",
      "Epoch 37/40  Iteration 10107/11040 Training loss: 1.1652 0.4748 sec/batch\n",
      "Epoch 37/40  Iteration 10108/11040 Training loss: 1.1653 0.4760 sec/batch\n",
      "Epoch 37/40  Iteration 10109/11040 Training loss: 1.1653 0.4915 sec/batch\n",
      "Epoch 37/40  Iteration 10110/11040 Training loss: 1.1652 0.4751 sec/batch\n",
      "Epoch 37/40  Iteration 10111/11040 Training loss: 1.1652 0.4775 sec/batch\n",
      "Epoch 37/40  Iteration 10112/11040 Training loss: 1.1652 0.4753 sec/batch\n",
      "Epoch 37/40  Iteration 10113/11040 Training loss: 1.1653 0.4749 sec/batch\n",
      "Epoch 37/40  Iteration 10114/11040 Training loss: 1.1653 0.4753 sec/batch\n",
      "Epoch 37/40  Iteration 10115/11040 Training loss: 1.1656 0.4929 sec/batch\n",
      "Epoch 37/40  Iteration 10116/11040 Training loss: 1.1657 0.4912 sec/batch\n",
      "Epoch 37/40  Iteration 10117/11040 Training loss: 1.1658 0.4915 sec/batch\n",
      "Epoch 37/40  Iteration 10118/11040 Training loss: 1.1658 0.4749 sec/batch\n",
      "Epoch 37/40  Iteration 10119/11040 Training loss: 1.1657 0.4765 sec/batch\n",
      "Epoch 37/40  Iteration 10120/11040 Training loss: 1.1659 0.4910 sec/batch\n",
      "Epoch 37/40  Iteration 10121/11040 Training loss: 1.1660 0.4756 sec/batch\n",
      "Epoch 37/40  Iteration 10122/11040 Training loss: 1.1660 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10123/11040 Training loss: 1.1663 0.4760 sec/batch\n",
      "Epoch 37/40  Iteration 10124/11040 Training loss: 1.1663 0.4916 sec/batch\n",
      "Epoch 37/40  Iteration 10125/11040 Training loss: 1.1662 0.4756 sec/batch\n",
      "Epoch 37/40  Iteration 10126/11040 Training loss: 1.1662 0.4766 sec/batch\n",
      "Epoch 37/40  Iteration 10127/11040 Training loss: 1.1661 0.4765 sec/batch\n",
      "Epoch 37/40  Iteration 10128/11040 Training loss: 1.1661 0.4734 sec/batch\n",
      "Epoch 37/40  Iteration 10129/11040 Training loss: 1.1661 0.4742 sec/batch\n",
      "Epoch 37/40  Iteration 10130/11040 Training loss: 1.1661 0.4762 sec/batch\n",
      "Epoch 37/40  Iteration 10131/11040 Training loss: 1.1663 0.4758 sec/batch\n",
      "Epoch 37/40  Iteration 10132/11040 Training loss: 1.1662 0.4921 sec/batch\n",
      "Epoch 37/40  Iteration 10133/11040 Training loss: 1.1663 0.4768 sec/batch\n",
      "Epoch 37/40  Iteration 10134/11040 Training loss: 1.1663 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10135/11040 Training loss: 1.1664 0.4898 sec/batch\n",
      "Epoch 37/40  Iteration 10136/11040 Training loss: 1.1665 0.4752 sec/batch\n",
      "Epoch 37/40  Iteration 10137/11040 Training loss: 1.1667 0.4761 sec/batch\n",
      "Epoch 37/40  Iteration 10138/11040 Training loss: 1.1669 0.4909 sec/batch\n",
      "Epoch 37/40  Iteration 10139/11040 Training loss: 1.1670 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 10140/11040 Training loss: 1.1671 0.4761 sec/batch\n",
      "Epoch 37/40  Iteration 10141/11040 Training loss: 1.1670 0.4756 sec/batch\n",
      "Epoch 37/40  Iteration 10142/11040 Training loss: 1.1670 0.4776 sec/batch\n",
      "Epoch 37/40  Iteration 10143/11040 Training loss: 1.1671 0.4917 sec/batch\n",
      "Epoch 37/40  Iteration 10144/11040 Training loss: 1.1671 0.4746 sec/batch\n",
      "Epoch 37/40  Iteration 10145/11040 Training loss: 1.1669 0.4768 sec/batch\n",
      "Epoch 37/40  Iteration 10146/11040 Training loss: 1.1669 0.4735 sec/batch\n",
      "Epoch 37/40  Iteration 10147/11040 Training loss: 1.1669 0.4964 sec/batch\n",
      "Epoch 37/40  Iteration 10148/11040 Training loss: 1.1669 0.4694 sec/batch\n",
      "Epoch 37/40  Iteration 10149/11040 Training loss: 1.1669 0.4733 sec/batch\n",
      "Epoch 37/40  Iteration 10150/11040 Training loss: 1.1669 0.4766 sec/batch\n",
      "Epoch 37/40  Iteration 10151/11040 Training loss: 1.1668 0.4764 sec/batch\n",
      "Epoch 37/40  Iteration 10152/11040 Training loss: 1.1667 0.4773 sec/batch\n",
      "Epoch 37/40  Iteration 10153/11040 Training loss: 1.1665 0.4903 sec/batch\n",
      "Epoch 37/40  Iteration 10154/11040 Training loss: 1.1665 0.4923 sec/batch\n",
      "Epoch 37/40  Iteration 10155/11040 Training loss: 1.1664 0.4891 sec/batch\n",
      "Epoch 37/40  Iteration 10156/11040 Training loss: 1.1664 0.4757 sec/batch\n",
      "Epoch 37/40  Iteration 10157/11040 Training loss: 1.1663 0.4764 sec/batch\n",
      "Epoch 37/40  Iteration 10158/11040 Training loss: 1.1661 0.4920 sec/batch\n",
      "Epoch 37/40  Iteration 10159/11040 Training loss: 1.1660 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 10160/11040 Training loss: 1.1661 0.4757 sec/batch\n",
      "Epoch 37/40  Iteration 10161/11040 Training loss: 1.1660 0.4767 sec/batch\n",
      "Epoch 37/40  Iteration 10162/11040 Training loss: 1.1661 0.4905 sec/batch\n",
      "Epoch 37/40  Iteration 10163/11040 Training loss: 1.1660 0.4898 sec/batch\n",
      "Epoch 37/40  Iteration 10164/11040 Training loss: 1.1659 0.4764 sec/batch\n",
      "Epoch 37/40  Iteration 10165/11040 Training loss: 1.1660 0.4759 sec/batch\n",
      "Epoch 37/40  Iteration 10166/11040 Training loss: 1.1661 0.4744 sec/batch\n",
      "Epoch 37/40  Iteration 10167/11040 Training loss: 1.1661 0.4905 sec/batch\n",
      "Epoch 37/40  Iteration 10168/11040 Training loss: 1.1662 0.4784 sec/batch\n",
      "Epoch 37/40  Iteration 10169/11040 Training loss: 1.1661 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 10170/11040 Training loss: 1.1660 0.4901 sec/batch\n",
      "Epoch 37/40  Iteration 10171/11040 Training loss: 1.1661 0.4900 sec/batch\n",
      "Epoch 37/40  Iteration 10172/11040 Training loss: 1.1662 0.4766 sec/batch\n",
      "Epoch 37/40  Iteration 10173/11040 Training loss: 1.1663 0.4767 sec/batch\n",
      "Epoch 37/40  Iteration 10174/11040 Training loss: 1.1663 0.4901 sec/batch\n",
      "Epoch 37/40  Iteration 10175/11040 Training loss: 1.1665 0.4920 sec/batch\n",
      "Epoch 37/40  Iteration 10176/11040 Training loss: 1.1666 0.4904 sec/batch\n",
      "Epoch 37/40  Iteration 10177/11040 Training loss: 1.1667 0.4757 sec/batch\n",
      "Epoch 37/40  Iteration 10178/11040 Training loss: 1.1667 0.4769 sec/batch\n",
      "Epoch 37/40  Iteration 10179/11040 Training loss: 1.1667 0.4918 sec/batch\n",
      "Epoch 37/40  Iteration 10180/11040 Training loss: 1.1667 0.4895 sec/batch\n",
      "Epoch 37/40  Iteration 10181/11040 Training loss: 1.1666 0.4754 sec/batch\n",
      "Epoch 37/40  Iteration 10182/11040 Training loss: 1.1666 0.4755 sec/batch\n",
      "Epoch 37/40  Iteration 10183/11040 Training loss: 1.1666 0.4762 sec/batch\n",
      "Epoch 37/40  Iteration 10184/11040 Training loss: 1.1666 0.4740 sec/batch\n",
      "Epoch 37/40  Iteration 10185/11040 Training loss: 1.1666 0.4775 sec/batch\n",
      "Epoch 37/40  Iteration 10186/11040 Training loss: 1.1666 0.4904 sec/batch\n",
      "Epoch 37/40  Iteration 10187/11040 Training loss: 1.1665 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 10188/11040 Training loss: 1.1666 0.4757 sec/batch\n",
      "Epoch 37/40  Iteration 10189/11040 Training loss: 1.1666 0.4757 sec/batch\n",
      "Epoch 37/40  Iteration 10190/11040 Training loss: 1.1667 0.4764 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40  Iteration 10191/11040 Training loss: 1.1667 0.4743 sec/batch\n",
      "Epoch 37/40  Iteration 10192/11040 Training loss: 1.1667 0.4745 sec/batch\n",
      "Epoch 37/40  Iteration 10193/11040 Training loss: 1.1668 0.4764 sec/batch\n",
      "Epoch 37/40  Iteration 10194/11040 Training loss: 1.1668 0.4924 sec/batch\n",
      "Epoch 37/40  Iteration 10195/11040 Training loss: 1.1668 0.4890 sec/batch\n",
      "Epoch 37/40  Iteration 10196/11040 Training loss: 1.1669 0.4879 sec/batch\n",
      "Epoch 37/40  Iteration 10197/11040 Training loss: 1.1669 0.4793 sec/batch\n",
      "Epoch 37/40  Iteration 10198/11040 Training loss: 1.1671 0.4822 sec/batch\n",
      "Epoch 37/40  Iteration 10199/11040 Training loss: 1.1672 0.4807 sec/batch\n",
      "Epoch 37/40  Iteration 10200/11040 Training loss: 1.1673 0.4875 sec/batch\n",
      "Epoch 37/40  Iteration 10201/11040 Training loss: 1.1673 0.4780 sec/batch\n",
      "Epoch 37/40  Iteration 10202/11040 Training loss: 1.1674 0.4979 sec/batch\n",
      "Epoch 37/40  Iteration 10203/11040 Training loss: 1.1676 0.4765 sec/batch\n",
      "Epoch 37/40  Iteration 10204/11040 Training loss: 1.1677 0.4896 sec/batch\n",
      "Epoch 37/40  Iteration 10205/11040 Training loss: 1.1678 0.4752 sec/batch\n",
      "Epoch 37/40  Iteration 10206/11040 Training loss: 1.1679 0.4760 sec/batch\n",
      "Epoch 37/40  Iteration 10207/11040 Training loss: 1.1682 0.4899 sec/batch\n",
      "Epoch 37/40  Iteration 10208/11040 Training loss: 1.1684 0.4787 sec/batch\n",
      "Epoch 37/40  Iteration 10209/11040 Training loss: 1.1685 0.4749 sec/batch\n",
      "Epoch 37/40  Iteration 10210/11040 Training loss: 1.1686 0.4771 sec/batch\n",
      "Epoch 37/40  Iteration 10211/11040 Training loss: 1.1687 0.4747 sec/batch\n",
      "Epoch 37/40  Iteration 10212/11040 Training loss: 1.1688 0.4732 sec/batch\n",
      "Epoch 38/40  Iteration 10213/11040 Training loss: 1.2480 0.4600 sec/batch\n",
      "Epoch 38/40  Iteration 10214/11040 Training loss: 1.2187 0.4762 sec/batch\n",
      "Epoch 38/40  Iteration 10215/11040 Training loss: 1.2141 0.4744 sec/batch\n",
      "Epoch 38/40  Iteration 10216/11040 Training loss: 1.2089 0.4745 sec/batch\n",
      "Epoch 38/40  Iteration 10217/11040 Training loss: 1.2052 0.4748 sec/batch\n",
      "Epoch 38/40  Iteration 10218/11040 Training loss: 1.2015 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10219/11040 Training loss: 1.1922 0.4603 sec/batch\n",
      "Epoch 38/40  Iteration 10220/11040 Training loss: 1.1873 0.4738 sec/batch\n",
      "Epoch 38/40  Iteration 10221/11040 Training loss: 1.1829 0.4755 sec/batch\n",
      "Epoch 38/40  Iteration 10222/11040 Training loss: 1.1808 0.4765 sec/batch\n",
      "Epoch 38/40  Iteration 10223/11040 Training loss: 1.1758 0.4813 sec/batch\n",
      "Epoch 38/40  Iteration 10224/11040 Training loss: 1.1737 0.4827 sec/batch\n",
      "Epoch 38/40  Iteration 10225/11040 Training loss: 1.1704 0.4759 sec/batch\n",
      "Epoch 38/40  Iteration 10226/11040 Training loss: 1.1676 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10227/11040 Training loss: 1.1655 0.4596 sec/batch\n",
      "Epoch 38/40  Iteration 10228/11040 Training loss: 1.1656 0.4616 sec/batch\n",
      "Epoch 38/40  Iteration 10229/11040 Training loss: 1.1624 0.4718 sec/batch\n",
      "Epoch 38/40  Iteration 10230/11040 Training loss: 1.1602 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10231/11040 Training loss: 1.1595 0.4730 sec/batch\n",
      "Epoch 38/40  Iteration 10232/11040 Training loss: 1.1593 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10233/11040 Training loss: 1.1615 0.4913 sec/batch\n",
      "Epoch 38/40  Iteration 10234/11040 Training loss: 1.1621 0.4766 sec/batch\n",
      "Epoch 38/40  Iteration 10235/11040 Training loss: 1.1611 0.4741 sec/batch\n",
      "Epoch 38/40  Iteration 10236/11040 Training loss: 1.1614 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10237/11040 Training loss: 1.1607 0.4902 sec/batch\n",
      "Epoch 38/40  Iteration 10238/11040 Training loss: 1.1603 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10239/11040 Training loss: 1.1594 0.4735 sec/batch\n",
      "Epoch 38/40  Iteration 10240/11040 Training loss: 1.1591 0.4633 sec/batch\n",
      "Epoch 38/40  Iteration 10241/11040 Training loss: 1.1590 0.4715 sec/batch\n",
      "Epoch 38/40  Iteration 10242/11040 Training loss: 1.1595 0.4745 sec/batch\n",
      "Epoch 38/40  Iteration 10243/11040 Training loss: 1.1596 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10244/11040 Training loss: 1.1594 0.4752 sec/batch\n",
      "Epoch 38/40  Iteration 10245/11040 Training loss: 1.1583 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10246/11040 Training loss: 1.1583 0.4755 sec/batch\n",
      "Epoch 38/40  Iteration 10247/11040 Training loss: 1.1584 0.4743 sec/batch\n",
      "Epoch 38/40  Iteration 10248/11040 Training loss: 1.1591 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10249/11040 Training loss: 1.1590 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10250/11040 Training loss: 1.1584 0.4732 sec/batch\n",
      "Epoch 38/40  Iteration 10251/11040 Training loss: 1.1578 0.4744 sec/batch\n",
      "Epoch 38/40  Iteration 10252/11040 Training loss: 1.1580 0.4609 sec/batch\n",
      "Epoch 38/40  Iteration 10253/11040 Training loss: 1.1579 0.4753 sec/batch\n",
      "Epoch 38/40  Iteration 10254/11040 Training loss: 1.1571 0.4742 sec/batch\n",
      "Epoch 38/40  Iteration 10255/11040 Training loss: 1.1573 0.4735 sec/batch\n",
      "Epoch 38/40  Iteration 10256/11040 Training loss: 1.1560 0.4617 sec/batch\n",
      "Epoch 38/40  Iteration 10257/11040 Training loss: 1.1557 0.4734 sec/batch\n",
      "Epoch 38/40  Iteration 10258/11040 Training loss: 1.1549 0.4615 sec/batch\n",
      "Epoch 38/40  Iteration 10259/11040 Training loss: 1.1540 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10260/11040 Training loss: 1.1541 0.4716 sec/batch\n",
      "Epoch 38/40  Iteration 10261/11040 Training loss: 1.1538 0.4765 sec/batch\n",
      "Epoch 38/40  Iteration 10262/11040 Training loss: 1.1541 0.4732 sec/batch\n",
      "Epoch 38/40  Iteration 10263/11040 Training loss: 1.1540 0.4682 sec/batch\n",
      "Epoch 38/40  Iteration 10264/11040 Training loss: 1.1539 0.4850 sec/batch\n",
      "Epoch 38/40  Iteration 10265/11040 Training loss: 1.1543 0.4725 sec/batch\n",
      "Epoch 38/40  Iteration 10266/11040 Training loss: 1.1540 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10267/11040 Training loss: 1.1541 0.4743 sec/batch\n",
      "Epoch 38/40  Iteration 10268/11040 Training loss: 1.1542 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10269/11040 Training loss: 1.1541 0.4752 sec/batch\n",
      "Epoch 38/40  Iteration 10270/11040 Training loss: 1.1540 0.4743 sec/batch\n",
      "Epoch 38/40  Iteration 10271/11040 Training loss: 1.1542 0.4638 sec/batch\n",
      "Epoch 38/40  Iteration 10272/11040 Training loss: 1.1539 0.4720 sec/batch\n",
      "Epoch 38/40  Iteration 10273/11040 Training loss: 1.1538 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10274/11040 Training loss: 1.1538 0.4740 sec/batch\n",
      "Epoch 38/40  Iteration 10275/11040 Training loss: 1.1540 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10276/11040 Training loss: 1.1541 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10277/11040 Training loss: 1.1544 0.4730 sec/batch\n",
      "Epoch 38/40  Iteration 10278/11040 Training loss: 1.1545 0.4738 sec/batch\n",
      "Epoch 38/40  Iteration 10279/11040 Training loss: 1.1546 0.4783 sec/batch\n",
      "Epoch 38/40  Iteration 10280/11040 Training loss: 1.1548 0.4732 sec/batch\n",
      "Epoch 38/40  Iteration 10281/11040 Training loss: 1.1545 0.4767 sec/batch\n",
      "Epoch 38/40  Iteration 10282/11040 Training loss: 1.1543 0.4734 sec/batch\n",
      "Epoch 38/40  Iteration 10283/11040 Training loss: 1.1540 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10284/11040 Training loss: 1.1536 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10285/11040 Training loss: 1.1532 0.4746 sec/batch\n",
      "Epoch 38/40  Iteration 10286/11040 Training loss: 1.1528 0.4746 sec/batch\n",
      "Epoch 38/40  Iteration 10287/11040 Training loss: 1.1527 0.4746 sec/batch\n",
      "Epoch 38/40  Iteration 10288/11040 Training loss: 1.1530 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10289/11040 Training loss: 1.1526 0.4619 sec/batch\n",
      "Epoch 38/40  Iteration 10290/11040 Training loss: 1.1522 0.4726 sec/batch\n",
      "Epoch 38/40  Iteration 10291/11040 Training loss: 1.1522 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10292/11040 Training loss: 1.1523 0.4613 sec/batch\n",
      "Epoch 38/40  Iteration 10293/11040 Training loss: 1.1519 0.4724 sec/batch\n",
      "Epoch 38/40  Iteration 10294/11040 Training loss: 1.1520 0.4753 sec/batch\n",
      "Epoch 38/40  Iteration 10295/11040 Training loss: 1.1520 0.4602 sec/batch\n",
      "Epoch 38/40  Iteration 10296/11040 Training loss: 1.1519 0.4734 sec/batch\n",
      "Epoch 38/40  Iteration 10297/11040 Training loss: 1.1516 0.4605 sec/batch\n",
      "Epoch 38/40  Iteration 10298/11040 Training loss: 1.1516 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10299/11040 Training loss: 1.1509 0.4903 sec/batch\n",
      "Epoch 38/40  Iteration 10300/11040 Training loss: 1.1500 0.4794 sec/batch\n",
      "Epoch 38/40  Iteration 10301/11040 Training loss: 1.1499 0.4869 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40  Iteration 10302/11040 Training loss: 1.1498 0.4751 sec/batch\n",
      "Epoch 38/40  Iteration 10303/11040 Training loss: 1.1498 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10304/11040 Training loss: 1.1497 0.4905 sec/batch\n",
      "Epoch 38/40  Iteration 10305/11040 Training loss: 1.1494 0.4766 sec/batch\n",
      "Epoch 38/40  Iteration 10306/11040 Training loss: 1.1498 0.4879 sec/batch\n",
      "Epoch 38/40  Iteration 10307/11040 Training loss: 1.1502 0.4785 sec/batch\n",
      "Epoch 38/40  Iteration 10308/11040 Training loss: 1.1505 0.4903 sec/batch\n",
      "Epoch 38/40  Iteration 10309/11040 Training loss: 1.1510 0.4747 sec/batch\n",
      "Epoch 38/40  Iteration 10310/11040 Training loss: 1.1515 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10311/11040 Training loss: 1.1518 0.4738 sec/batch\n",
      "Epoch 38/40  Iteration 10312/11040 Training loss: 1.1516 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10313/11040 Training loss: 1.1518 0.4746 sec/batch\n",
      "Epoch 38/40  Iteration 10314/11040 Training loss: 1.1521 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10315/11040 Training loss: 1.1518 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10316/11040 Training loss: 1.1519 0.4738 sec/batch\n",
      "Epoch 38/40  Iteration 10317/11040 Training loss: 1.1518 0.4778 sec/batch\n",
      "Epoch 38/40  Iteration 10318/11040 Training loss: 1.1516 0.4734 sec/batch\n",
      "Epoch 38/40  Iteration 10319/11040 Training loss: 1.1514 0.4769 sec/batch\n",
      "Epoch 38/40  Iteration 10320/11040 Training loss: 1.1514 0.4913 sec/batch\n",
      "Epoch 38/40  Iteration 10321/11040 Training loss: 1.1519 0.4732 sec/batch\n",
      "Epoch 38/40  Iteration 10322/11040 Training loss: 1.1518 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10323/11040 Training loss: 1.1518 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10324/11040 Training loss: 1.1518 0.4921 sec/batch\n",
      "Epoch 38/40  Iteration 10325/11040 Training loss: 1.1519 0.4774 sec/batch\n",
      "Epoch 38/40  Iteration 10326/11040 Training loss: 1.1519 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10327/11040 Training loss: 1.1522 0.4900 sec/batch\n",
      "Epoch 38/40  Iteration 10328/11040 Training loss: 1.1525 0.4743 sec/batch\n",
      "Epoch 38/40  Iteration 10329/11040 Training loss: 1.1524 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10330/11040 Training loss: 1.1526 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10331/11040 Training loss: 1.1527 0.4905 sec/batch\n",
      "Epoch 38/40  Iteration 10332/11040 Training loss: 1.1528 0.4790 sec/batch\n",
      "Epoch 38/40  Iteration 10333/11040 Training loss: 1.1528 0.4725 sec/batch\n",
      "Epoch 38/40  Iteration 10334/11040 Training loss: 1.1529 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10335/11040 Training loss: 1.1532 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10336/11040 Training loss: 1.1537 0.4920 sec/batch\n",
      "Epoch 38/40  Iteration 10337/11040 Training loss: 1.1538 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10338/11040 Training loss: 1.1541 0.4837 sec/batch\n",
      "Epoch 38/40  Iteration 10339/11040 Training loss: 1.1542 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10340/11040 Training loss: 1.1543 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10341/11040 Training loss: 1.1541 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10342/11040 Training loss: 1.1545 0.4761 sec/batch\n",
      "Epoch 38/40  Iteration 10343/11040 Training loss: 1.1545 0.4772 sec/batch\n",
      "Epoch 38/40  Iteration 10344/11040 Training loss: 1.1544 0.4745 sec/batch\n",
      "Epoch 38/40  Iteration 10345/11040 Training loss: 1.1544 0.4919 sec/batch\n",
      "Epoch 38/40  Iteration 10346/11040 Training loss: 1.1543 0.4770 sec/batch\n",
      "Epoch 38/40  Iteration 10347/11040 Training loss: 1.1544 0.4908 sec/batch\n",
      "Epoch 38/40  Iteration 10348/11040 Training loss: 1.1544 0.4822 sec/batch\n",
      "Epoch 38/40  Iteration 10349/11040 Training loss: 1.1542 0.4990 sec/batch\n",
      "Epoch 38/40  Iteration 10350/11040 Training loss: 1.1544 0.4786 sec/batch\n",
      "Epoch 38/40  Iteration 10351/11040 Training loss: 1.1546 0.4729 sec/batch\n",
      "Epoch 38/40  Iteration 10352/11040 Training loss: 1.1547 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10353/11040 Training loss: 1.1547 0.4784 sec/batch\n",
      "Epoch 38/40  Iteration 10354/11040 Training loss: 1.1549 0.4731 sec/batch\n",
      "Epoch 38/40  Iteration 10355/11040 Training loss: 1.1549 0.4776 sec/batch\n",
      "Epoch 38/40  Iteration 10356/11040 Training loss: 1.1549 0.4741 sec/batch\n",
      "Epoch 38/40  Iteration 10357/11040 Training loss: 1.1550 0.4925 sec/batch\n",
      "Epoch 38/40  Iteration 10358/11040 Training loss: 1.1550 0.4901 sec/batch\n",
      "Epoch 38/40  Iteration 10359/11040 Training loss: 1.1550 0.4894 sec/batch\n",
      "Epoch 38/40  Iteration 10360/11040 Training loss: 1.1551 0.4761 sec/batch\n",
      "Epoch 38/40  Iteration 10361/11040 Training loss: 1.1551 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10362/11040 Training loss: 1.1552 0.4832 sec/batch\n",
      "Epoch 38/40  Iteration 10363/11040 Training loss: 1.1552 0.4841 sec/batch\n",
      "Epoch 38/40  Iteration 10364/11040 Training loss: 1.1554 0.4917 sec/batch\n",
      "Epoch 38/40  Iteration 10365/11040 Training loss: 1.1555 0.4927 sec/batch\n",
      "Epoch 38/40  Iteration 10366/11040 Training loss: 1.1557 0.4924 sec/batch\n",
      "Epoch 38/40  Iteration 10367/11040 Training loss: 1.1557 0.4894 sec/batch\n",
      "Epoch 38/40  Iteration 10368/11040 Training loss: 1.1557 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10369/11040 Training loss: 1.1556 0.4742 sec/batch\n",
      "Epoch 38/40  Iteration 10370/11040 Training loss: 1.1557 0.4761 sec/batch\n",
      "Epoch 38/40  Iteration 10371/11040 Training loss: 1.1559 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10372/11040 Training loss: 1.1560 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10373/11040 Training loss: 1.1561 0.4774 sec/batch\n",
      "Epoch 38/40  Iteration 10374/11040 Training loss: 1.1562 0.4738 sec/batch\n",
      "Epoch 38/40  Iteration 10375/11040 Training loss: 1.1564 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10376/11040 Training loss: 1.1565 0.4782 sec/batch\n",
      "Epoch 38/40  Iteration 10377/11040 Training loss: 1.1567 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10378/11040 Training loss: 1.1570 0.4915 sec/batch\n",
      "Epoch 38/40  Iteration 10379/11040 Training loss: 1.1569 0.4898 sec/batch\n",
      "Epoch 38/40  Iteration 10380/11040 Training loss: 1.1569 0.4932 sec/batch\n",
      "Epoch 38/40  Iteration 10381/11040 Training loss: 1.1569 0.4912 sec/batch\n",
      "Epoch 38/40  Iteration 10382/11040 Training loss: 1.1570 0.4903 sec/batch\n",
      "Epoch 38/40  Iteration 10383/11040 Training loss: 1.1572 0.4916 sec/batch\n",
      "Epoch 38/40  Iteration 10384/11040 Training loss: 1.1574 0.4770 sec/batch\n",
      "Epoch 38/40  Iteration 10385/11040 Training loss: 1.1574 0.4762 sec/batch\n",
      "Epoch 38/40  Iteration 10386/11040 Training loss: 1.1574 0.4771 sec/batch\n",
      "Epoch 38/40  Iteration 10387/11040 Training loss: 1.1574 0.4926 sec/batch\n",
      "Epoch 38/40  Iteration 10388/11040 Training loss: 1.1574 0.4894 sec/batch\n",
      "Epoch 38/40  Iteration 10389/11040 Training loss: 1.1575 0.4902 sec/batch\n",
      "Epoch 38/40  Iteration 10390/11040 Training loss: 1.1576 0.4909 sec/batch\n",
      "Epoch 38/40  Iteration 10391/11040 Training loss: 1.1579 0.4924 sec/batch\n",
      "Epoch 38/40  Iteration 10392/11040 Training loss: 1.1580 0.4914 sec/batch\n",
      "Epoch 38/40  Iteration 10393/11040 Training loss: 1.1581 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10394/11040 Training loss: 1.1582 0.4824 sec/batch\n",
      "Epoch 38/40  Iteration 10395/11040 Training loss: 1.1581 0.4701 sec/batch\n",
      "Epoch 38/40  Iteration 10396/11040 Training loss: 1.1582 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10397/11040 Training loss: 1.1583 0.4755 sec/batch\n",
      "Epoch 38/40  Iteration 10398/11040 Training loss: 1.1583 0.4753 sec/batch\n",
      "Epoch 38/40  Iteration 10399/11040 Training loss: 1.1586 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10400/11040 Training loss: 1.1586 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10401/11040 Training loss: 1.1586 0.4915 sec/batch\n",
      "Epoch 38/40  Iteration 10402/11040 Training loss: 1.1585 0.4747 sec/batch\n",
      "Epoch 38/40  Iteration 10403/11040 Training loss: 1.1584 0.4765 sec/batch\n",
      "Epoch 38/40  Iteration 10404/11040 Training loss: 1.1584 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10405/11040 Training loss: 1.1585 0.4904 sec/batch\n",
      "Epoch 38/40  Iteration 10406/11040 Training loss: 1.1585 0.4903 sec/batch\n",
      "Epoch 38/40  Iteration 10407/11040 Training loss: 1.1587 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10408/11040 Training loss: 1.1585 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10409/11040 Training loss: 1.1587 0.4747 sec/batch\n",
      "Epoch 38/40  Iteration 10410/11040 Training loss: 1.1588 0.4771 sec/batch\n",
      "Epoch 38/40  Iteration 10411/11040 Training loss: 1.1588 0.4765 sec/batch\n",
      "Epoch 38/40  Iteration 10412/11040 Training loss: 1.1589 0.4921 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40  Iteration 10413/11040 Training loss: 1.1590 0.4893 sec/batch\n",
      "Epoch 38/40  Iteration 10414/11040 Training loss: 1.1592 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10415/11040 Training loss: 1.1593 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10416/11040 Training loss: 1.1594 0.4763 sec/batch\n",
      "Epoch 38/40  Iteration 10417/11040 Training loss: 1.1593 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10418/11040 Training loss: 1.1593 0.4761 sec/batch\n",
      "Epoch 38/40  Iteration 10419/11040 Training loss: 1.1594 0.4900 sec/batch\n",
      "Epoch 38/40  Iteration 10420/11040 Training loss: 1.1594 0.4910 sec/batch\n",
      "Epoch 38/40  Iteration 10421/11040 Training loss: 1.1593 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10422/11040 Training loss: 1.1592 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10423/11040 Training loss: 1.1592 0.4917 sec/batch\n",
      "Epoch 38/40  Iteration 10424/11040 Training loss: 1.1593 0.4931 sec/batch\n",
      "Epoch 38/40  Iteration 10425/11040 Training loss: 1.1592 0.4888 sec/batch\n",
      "Epoch 38/40  Iteration 10426/11040 Training loss: 1.1593 0.4751 sec/batch\n",
      "Epoch 38/40  Iteration 10427/11040 Training loss: 1.1591 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10428/11040 Training loss: 1.1590 0.4920 sec/batch\n",
      "Epoch 38/40  Iteration 10429/11040 Training loss: 1.1588 0.4745 sec/batch\n",
      "Epoch 38/40  Iteration 10430/11040 Training loss: 1.1588 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10431/11040 Training loss: 1.1587 0.4778 sec/batch\n",
      "Epoch 38/40  Iteration 10432/11040 Training loss: 1.1587 0.4742 sec/batch\n",
      "Epoch 38/40  Iteration 10433/11040 Training loss: 1.1585 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10434/11040 Training loss: 1.1584 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10435/11040 Training loss: 1.1583 0.4797 sec/batch\n",
      "Epoch 38/40  Iteration 10436/11040 Training loss: 1.1583 0.4892 sec/batch\n",
      "Epoch 38/40  Iteration 10437/11040 Training loss: 1.1583 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10438/11040 Training loss: 1.1583 0.4742 sec/batch\n",
      "Epoch 38/40  Iteration 10439/11040 Training loss: 1.1582 0.4911 sec/batch\n",
      "Epoch 38/40  Iteration 10440/11040 Training loss: 1.1581 0.4778 sec/batch\n",
      "Epoch 38/40  Iteration 10441/11040 Training loss: 1.1581 0.4757 sec/batch\n",
      "Epoch 38/40  Iteration 10442/11040 Training loss: 1.1583 0.4745 sec/batch\n",
      "Epoch 38/40  Iteration 10443/11040 Training loss: 1.1583 0.4767 sec/batch\n",
      "Epoch 38/40  Iteration 10444/11040 Training loss: 1.1583 0.4753 sec/batch\n",
      "Epoch 38/40  Iteration 10445/11040 Training loss: 1.1583 0.4850 sec/batch\n",
      "Epoch 38/40  Iteration 10446/11040 Training loss: 1.1582 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10447/11040 Training loss: 1.1583 0.4904 sec/batch\n",
      "Epoch 38/40  Iteration 10448/11040 Training loss: 1.1584 0.4909 sec/batch\n",
      "Epoch 38/40  Iteration 10449/11040 Training loss: 1.1585 0.4869 sec/batch\n",
      "Epoch 38/40  Iteration 10450/11040 Training loss: 1.1586 0.4744 sec/batch\n",
      "Epoch 38/40  Iteration 10451/11040 Training loss: 1.1587 0.4739 sec/batch\n",
      "Epoch 38/40  Iteration 10452/11040 Training loss: 1.1589 0.4767 sec/batch\n",
      "Epoch 38/40  Iteration 10453/11040 Training loss: 1.1590 0.4754 sec/batch\n",
      "Epoch 38/40  Iteration 10454/11040 Training loss: 1.1591 0.4757 sec/batch\n",
      "Epoch 38/40  Iteration 10455/11040 Training loss: 1.1591 0.4928 sec/batch\n",
      "Epoch 38/40  Iteration 10456/11040 Training loss: 1.1592 0.4911 sec/batch\n",
      "Epoch 38/40  Iteration 10457/11040 Training loss: 1.1591 0.4922 sec/batch\n",
      "Epoch 38/40  Iteration 10458/11040 Training loss: 1.1591 0.4917 sec/batch\n",
      "Epoch 38/40  Iteration 10459/11040 Training loss: 1.1592 0.4792 sec/batch\n",
      "Epoch 38/40  Iteration 10460/11040 Training loss: 1.1591 0.4723 sec/batch\n",
      "Epoch 38/40  Iteration 10461/11040 Training loss: 1.1592 0.4752 sec/batch\n",
      "Epoch 38/40  Iteration 10462/11040 Training loss: 1.1592 0.4761 sec/batch\n",
      "Epoch 38/40  Iteration 10463/11040 Training loss: 1.1591 0.4768 sec/batch\n",
      "Epoch 38/40  Iteration 10464/11040 Training loss: 1.1591 0.4751 sec/batch\n",
      "Epoch 38/40  Iteration 10465/11040 Training loss: 1.1592 0.4750 sec/batch\n",
      "Epoch 38/40  Iteration 10466/11040 Training loss: 1.1593 0.4758 sec/batch\n",
      "Epoch 38/40  Iteration 10467/11040 Training loss: 1.1593 0.4752 sec/batch\n",
      "Epoch 38/40  Iteration 10468/11040 Training loss: 1.1593 0.4755 sec/batch\n",
      "Epoch 38/40  Iteration 10469/11040 Training loss: 1.1595 0.4760 sec/batch\n",
      "Epoch 38/40  Iteration 10470/11040 Training loss: 1.1595 0.4775 sec/batch\n",
      "Epoch 38/40  Iteration 10471/11040 Training loss: 1.1595 0.4761 sec/batch\n",
      "Epoch 38/40  Iteration 10472/11040 Training loss: 1.1596 0.4907 sec/batch\n",
      "Epoch 38/40  Iteration 10473/11040 Training loss: 1.1597 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10474/11040 Training loss: 1.1599 0.4764 sec/batch\n",
      "Epoch 38/40  Iteration 10475/11040 Training loss: 1.1600 0.4911 sec/batch\n",
      "Epoch 38/40  Iteration 10476/11040 Training loss: 1.1601 0.4918 sec/batch\n",
      "Epoch 38/40  Iteration 10477/11040 Training loss: 1.1601 0.4899 sec/batch\n",
      "Epoch 38/40  Iteration 10478/11040 Training loss: 1.1602 0.4927 sec/batch\n",
      "Epoch 38/40  Iteration 10479/11040 Training loss: 1.1604 0.4899 sec/batch\n",
      "Epoch 38/40  Iteration 10480/11040 Training loss: 1.1605 0.4929 sec/batch\n",
      "Epoch 38/40  Iteration 10481/11040 Training loss: 1.1606 0.4773 sec/batch\n",
      "Epoch 38/40  Iteration 10482/11040 Training loss: 1.1608 0.4752 sec/batch\n",
      "Epoch 38/40  Iteration 10483/11040 Training loss: 1.1612 0.4769 sec/batch\n",
      "Epoch 38/40  Iteration 10484/11040 Training loss: 1.1614 0.4756 sec/batch\n",
      "Epoch 38/40  Iteration 10485/11040 Training loss: 1.1615 0.4762 sec/batch\n",
      "Epoch 38/40  Iteration 10486/11040 Training loss: 1.1616 0.4897 sec/batch\n",
      "Epoch 38/40  Iteration 10487/11040 Training loss: 1.1617 0.4852 sec/batch\n",
      "Epoch 38/40  Iteration 10488/11040 Training loss: 1.1618 0.4985 sec/batch\n",
      "Epoch 39/40  Iteration 10489/11040 Training loss: 1.2510 0.4688 sec/batch\n",
      "Epoch 39/40  Iteration 10490/11040 Training loss: 1.2137 0.4735 sec/batch\n",
      "Epoch 39/40  Iteration 10491/11040 Training loss: 1.2053 0.4759 sec/batch\n",
      "Epoch 39/40  Iteration 10492/11040 Training loss: 1.2049 0.4754 sec/batch\n",
      "Epoch 39/40  Iteration 10493/11040 Training loss: 1.2009 0.4763 sec/batch\n",
      "Epoch 39/40  Iteration 10494/11040 Training loss: 1.1970 0.4774 sec/batch\n",
      "Epoch 39/40  Iteration 10495/11040 Training loss: 1.1869 0.4897 sec/batch\n",
      "Epoch 39/40  Iteration 10496/11040 Training loss: 1.1828 0.4758 sec/batch\n",
      "Epoch 39/40  Iteration 10497/11040 Training loss: 1.1775 0.4929 sec/batch\n",
      "Epoch 39/40  Iteration 10498/11040 Training loss: 1.1754 0.4903 sec/batch\n",
      "Epoch 39/40  Iteration 10499/11040 Training loss: 1.1708 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10500/11040 Training loss: 1.1696 0.4738 sec/batch\n",
      "Validation loss: 1.23136 Saving checkpoint!\n",
      "Epoch 39/40  Iteration 10501/11040 Training loss: 1.1799 0.4688 sec/batch\n",
      "Epoch 39/40  Iteration 10502/11040 Training loss: 1.1772 0.4755 sec/batch\n",
      "Epoch 39/40  Iteration 10503/11040 Training loss: 1.1749 0.4733 sec/batch\n",
      "Epoch 39/40  Iteration 10504/11040 Training loss: 1.1736 0.4604 sec/batch\n",
      "Epoch 39/40  Iteration 10505/11040 Training loss: 1.1689 0.4587 sec/batch\n",
      "Epoch 39/40  Iteration 10506/11040 Training loss: 1.1660 0.4770 sec/batch\n",
      "Epoch 39/40  Iteration 10507/11040 Training loss: 1.1649 0.4722 sec/batch\n",
      "Epoch 39/40  Iteration 10508/11040 Training loss: 1.1644 0.4766 sec/batch\n",
      "Epoch 39/40  Iteration 10509/11040 Training loss: 1.1657 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10510/11040 Training loss: 1.1658 0.4786 sec/batch\n",
      "Epoch 39/40  Iteration 10511/11040 Training loss: 1.1650 0.4727 sec/batch\n",
      "Epoch 39/40  Iteration 10512/11040 Training loss: 1.1650 0.4722 sec/batch\n",
      "Epoch 39/40  Iteration 10513/11040 Training loss: 1.1640 0.4763 sec/batch\n",
      "Epoch 39/40  Iteration 10514/11040 Training loss: 1.1627 0.4897 sec/batch\n",
      "Epoch 39/40  Iteration 10515/11040 Training loss: 1.1622 0.4730 sec/batch\n",
      "Epoch 39/40  Iteration 10516/11040 Training loss: 1.1617 0.4756 sec/batch\n",
      "Epoch 39/40  Iteration 10517/11040 Training loss: 1.1612 0.4754 sec/batch\n",
      "Epoch 39/40  Iteration 10518/11040 Training loss: 1.1613 0.4732 sec/batch\n",
      "Epoch 39/40  Iteration 10519/11040 Training loss: 1.1611 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10520/11040 Training loss: 1.1606 0.4751 sec/batch\n",
      "Epoch 39/40  Iteration 10521/11040 Training loss: 1.1596 0.4752 sec/batch\n",
      "Epoch 39/40  Iteration 10522/11040 Training loss: 1.1590 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10523/11040 Training loss: 1.1587 0.4606 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40  Iteration 10524/11040 Training loss: 1.1593 0.4770 sec/batch\n",
      "Epoch 39/40  Iteration 10525/11040 Training loss: 1.1592 0.4723 sec/batch\n",
      "Epoch 39/40  Iteration 10526/11040 Training loss: 1.1586 0.4726 sec/batch\n",
      "Epoch 39/40  Iteration 10527/11040 Training loss: 1.1580 0.4608 sec/batch\n",
      "Epoch 39/40  Iteration 10528/11040 Training loss: 1.1579 0.4733 sec/batch\n",
      "Epoch 39/40  Iteration 10529/11040 Training loss: 1.1575 0.4756 sec/batch\n",
      "Epoch 39/40  Iteration 10530/11040 Training loss: 1.1565 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10531/11040 Training loss: 1.1565 0.4734 sec/batch\n",
      "Epoch 39/40  Iteration 10532/11040 Training loss: 1.1555 0.4741 sec/batch\n",
      "Epoch 39/40  Iteration 10533/11040 Training loss: 1.1548 0.4775 sec/batch\n",
      "Epoch 39/40  Iteration 10534/11040 Training loss: 1.1539 0.4737 sec/batch\n",
      "Epoch 39/40  Iteration 10535/11040 Training loss: 1.1529 0.4754 sec/batch\n",
      "Epoch 39/40  Iteration 10536/11040 Training loss: 1.1529 0.4746 sec/batch\n",
      "Epoch 39/40  Iteration 10537/11040 Training loss: 1.1525 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10538/11040 Training loss: 1.1528 0.4703 sec/batch\n",
      "Epoch 39/40  Iteration 10539/11040 Training loss: 1.1524 0.4649 sec/batch\n",
      "Epoch 39/40  Iteration 10540/11040 Training loss: 1.1521 0.4744 sec/batch\n",
      "Epoch 39/40  Iteration 10541/11040 Training loss: 1.1523 0.4744 sec/batch\n",
      "Epoch 39/40  Iteration 10542/11040 Training loss: 1.1521 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10543/11040 Training loss: 1.1519 0.4606 sec/batch\n",
      "Epoch 39/40  Iteration 10544/11040 Training loss: 1.1518 0.4736 sec/batch\n",
      "Epoch 39/40  Iteration 10545/11040 Training loss: 1.1516 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10546/11040 Training loss: 1.1514 0.4755 sec/batch\n",
      "Epoch 39/40  Iteration 10547/11040 Training loss: 1.1517 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10548/11040 Training loss: 1.1511 0.4773 sec/batch\n",
      "Epoch 39/40  Iteration 10549/11040 Training loss: 1.1512 0.4737 sec/batch\n",
      "Epoch 39/40  Iteration 10550/11040 Training loss: 1.1510 0.4766 sec/batch\n",
      "Epoch 39/40  Iteration 10551/11040 Training loss: 1.1512 0.4733 sec/batch\n",
      "Epoch 39/40  Iteration 10552/11040 Training loss: 1.1513 0.4776 sec/batch\n",
      "Epoch 39/40  Iteration 10553/11040 Training loss: 1.1517 0.4739 sec/batch\n",
      "Epoch 39/40  Iteration 10554/11040 Training loss: 1.1516 0.4746 sec/batch\n",
      "Epoch 39/40  Iteration 10555/11040 Training loss: 1.1515 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10556/11040 Training loss: 1.1515 0.4767 sec/batch\n",
      "Epoch 39/40  Iteration 10557/11040 Training loss: 1.1513 0.4739 sec/batch\n",
      "Epoch 39/40  Iteration 10558/11040 Training loss: 1.1512 0.4744 sec/batch\n",
      "Epoch 39/40  Iteration 10559/11040 Training loss: 1.1508 0.4892 sec/batch\n",
      "Epoch 39/40  Iteration 10560/11040 Training loss: 1.1504 0.4810 sec/batch\n",
      "Epoch 39/40  Iteration 10561/11040 Training loss: 1.1499 0.4739 sec/batch\n",
      "Epoch 39/40  Iteration 10562/11040 Training loss: 1.1495 0.4743 sec/batch\n",
      "Epoch 39/40  Iteration 10563/11040 Training loss: 1.1493 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10564/11040 Training loss: 1.1497 0.4753 sec/batch\n",
      "Epoch 39/40  Iteration 10565/11040 Training loss: 1.1491 0.4743 sec/batch\n",
      "Epoch 39/40  Iteration 10566/11040 Training loss: 1.1488 0.4775 sec/batch\n",
      "Epoch 39/40  Iteration 10567/11040 Training loss: 1.1486 0.4890 sec/batch\n",
      "Epoch 39/40  Iteration 10568/11040 Training loss: 1.1488 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10569/11040 Training loss: 1.1486 0.4769 sec/batch\n",
      "Epoch 39/40  Iteration 10570/11040 Training loss: 1.1487 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10571/11040 Training loss: 1.1487 0.4767 sec/batch\n",
      "Epoch 39/40  Iteration 10572/11040 Training loss: 1.1486 0.4726 sec/batch\n",
      "Epoch 39/40  Iteration 10573/11040 Training loss: 1.1482 0.4759 sec/batch\n",
      "Epoch 39/40  Iteration 10574/11040 Training loss: 1.1481 0.4774 sec/batch\n",
      "Epoch 39/40  Iteration 10575/11040 Training loss: 1.1475 0.4893 sec/batch\n",
      "Epoch 39/40  Iteration 10576/11040 Training loss: 1.1465 0.4746 sec/batch\n",
      "Epoch 39/40  Iteration 10577/11040 Training loss: 1.1463 0.4759 sec/batch\n",
      "Epoch 39/40  Iteration 10578/11040 Training loss: 1.1464 0.4738 sec/batch\n",
      "Epoch 39/40  Iteration 10579/11040 Training loss: 1.1464 0.4767 sec/batch\n",
      "Epoch 39/40  Iteration 10580/11040 Training loss: 1.1463 0.4737 sec/batch\n",
      "Epoch 39/40  Iteration 10581/11040 Training loss: 1.1460 0.4778 sec/batch\n",
      "Epoch 39/40  Iteration 10582/11040 Training loss: 1.1463 0.4728 sec/batch\n",
      "Epoch 39/40  Iteration 10583/11040 Training loss: 1.1466 0.4765 sec/batch\n",
      "Epoch 39/40  Iteration 10584/11040 Training loss: 1.1470 0.4748 sec/batch\n",
      "Epoch 39/40  Iteration 10585/11040 Training loss: 1.1473 0.4750 sec/batch\n",
      "Epoch 39/40  Iteration 10586/11040 Training loss: 1.1477 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10587/11040 Training loss: 1.1480 0.4756 sec/batch\n",
      "Epoch 39/40  Iteration 10588/11040 Training loss: 1.1478 0.4766 sec/batch\n",
      "Epoch 39/40  Iteration 10589/11040 Training loss: 1.1480 0.4759 sec/batch\n",
      "Epoch 39/40  Iteration 10590/11040 Training loss: 1.1483 0.4744 sec/batch\n",
      "Epoch 39/40  Iteration 10591/11040 Training loss: 1.1480 0.4743 sec/batch\n",
      "Epoch 39/40  Iteration 10592/11040 Training loss: 1.1482 0.4766 sec/batch\n",
      "Epoch 39/40  Iteration 10593/11040 Training loss: 1.1480 0.4911 sec/batch\n",
      "Epoch 39/40  Iteration 10594/11040 Training loss: 1.1477 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10595/11040 Training loss: 1.1476 0.4779 sec/batch\n",
      "Epoch 39/40  Iteration 10596/11040 Training loss: 1.1477 0.4596 sec/batch\n",
      "Epoch 39/40  Iteration 10597/11040 Training loss: 1.1480 0.4738 sec/batch\n",
      "Epoch 39/40  Iteration 10598/11040 Training loss: 1.1480 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10599/11040 Training loss: 1.1480 0.4712 sec/batch\n",
      "Epoch 39/40  Iteration 10600/11040 Training loss: 1.1481 0.4789 sec/batch\n",
      "Epoch 39/40  Iteration 10601/11040 Training loss: 1.1481 0.4899 sec/batch\n",
      "Epoch 39/40  Iteration 10602/11040 Training loss: 1.1481 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10603/11040 Training loss: 1.1484 0.4747 sec/batch\n",
      "Epoch 39/40  Iteration 10604/11040 Training loss: 1.1486 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10605/11040 Training loss: 1.1484 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10606/11040 Training loss: 1.1485 0.4898 sec/batch\n",
      "Epoch 39/40  Iteration 10607/11040 Training loss: 1.1486 0.4763 sec/batch\n",
      "Epoch 39/40  Iteration 10608/11040 Training loss: 1.1487 0.4743 sec/batch\n",
      "Epoch 39/40  Iteration 10609/11040 Training loss: 1.1487 0.4816 sec/batch\n",
      "Epoch 39/40  Iteration 10610/11040 Training loss: 1.1489 0.4871 sec/batch\n",
      "Epoch 39/40  Iteration 10611/11040 Training loss: 1.1491 0.4890 sec/batch\n",
      "Epoch 39/40  Iteration 10612/11040 Training loss: 1.1496 0.4760 sec/batch\n",
      "Epoch 39/40  Iteration 10613/11040 Training loss: 1.1498 0.4778 sec/batch\n",
      "Epoch 39/40  Iteration 10614/11040 Training loss: 1.1500 0.4907 sec/batch\n",
      "Epoch 39/40  Iteration 10615/11040 Training loss: 1.1500 0.4719 sec/batch\n",
      "Epoch 39/40  Iteration 10616/11040 Training loss: 1.1501 0.4760 sec/batch\n",
      "Epoch 39/40  Iteration 10617/11040 Training loss: 1.1499 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10618/11040 Training loss: 1.1502 0.4760 sec/batch\n",
      "Epoch 39/40  Iteration 10619/11040 Training loss: 1.1502 0.4661 sec/batch\n",
      "Epoch 39/40  Iteration 10620/11040 Training loss: 1.1501 0.4696 sec/batch\n",
      "Epoch 39/40  Iteration 10621/11040 Training loss: 1.1501 0.4726 sec/batch\n",
      "Epoch 39/40  Iteration 10622/11040 Training loss: 1.1501 0.4779 sec/batch\n",
      "Epoch 39/40  Iteration 10623/11040 Training loss: 1.1502 0.4893 sec/batch\n",
      "Epoch 39/40  Iteration 10624/11040 Training loss: 1.1502 0.4768 sec/batch\n",
      "Epoch 39/40  Iteration 10625/11040 Training loss: 1.1502 0.4898 sec/batch\n",
      "Epoch 39/40  Iteration 10626/11040 Training loss: 1.1503 0.4770 sec/batch\n",
      "Epoch 39/40  Iteration 10627/11040 Training loss: 1.1505 0.4911 sec/batch\n",
      "Epoch 39/40  Iteration 10628/11040 Training loss: 1.1506 0.4741 sec/batch\n",
      "Epoch 39/40  Iteration 10629/11040 Training loss: 1.1507 0.4892 sec/batch\n",
      "Epoch 39/40  Iteration 10630/11040 Training loss: 1.1508 0.4782 sec/batch\n",
      "Epoch 39/40  Iteration 10631/11040 Training loss: 1.1508 0.4748 sec/batch\n",
      "Epoch 39/40  Iteration 10632/11040 Training loss: 1.1507 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10633/11040 Training loss: 1.1508 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10634/11040 Training loss: 1.1509 0.4733 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40  Iteration 10635/11040 Training loss: 1.1509 0.4756 sec/batch\n",
      "Epoch 39/40  Iteration 10636/11040 Training loss: 1.1510 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10637/11040 Training loss: 1.1509 0.4891 sec/batch\n",
      "Epoch 39/40  Iteration 10638/11040 Training loss: 1.1509 0.4771 sec/batch\n",
      "Epoch 39/40  Iteration 10639/11040 Training loss: 1.1508 0.4743 sec/batch\n",
      "Epoch 39/40  Iteration 10640/11040 Training loss: 1.1510 0.4752 sec/batch\n",
      "Epoch 39/40  Iteration 10641/11040 Training loss: 1.1511 0.4746 sec/batch\n",
      "Epoch 39/40  Iteration 10642/11040 Training loss: 1.1513 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10643/11040 Training loss: 1.1513 0.4746 sec/batch\n",
      "Epoch 39/40  Iteration 10644/11040 Training loss: 1.1513 0.4617 sec/batch\n",
      "Epoch 39/40  Iteration 10645/11040 Training loss: 1.1512 0.4735 sec/batch\n",
      "Epoch 39/40  Iteration 10646/11040 Training loss: 1.1513 0.4753 sec/batch\n",
      "Epoch 39/40  Iteration 10647/11040 Training loss: 1.1515 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10648/11040 Training loss: 1.1515 0.4734 sec/batch\n",
      "Epoch 39/40  Iteration 10649/11040 Training loss: 1.1515 0.4747 sec/batch\n",
      "Epoch 39/40  Iteration 10650/11040 Training loss: 1.1516 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10651/11040 Training loss: 1.1518 0.4750 sec/batch\n",
      "Epoch 39/40  Iteration 10652/11040 Training loss: 1.1519 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10653/11040 Training loss: 1.1522 0.4747 sec/batch\n",
      "Epoch 39/40  Iteration 10654/11040 Training loss: 1.1524 0.4764 sec/batch\n",
      "Epoch 39/40  Iteration 10655/11040 Training loss: 1.1524 0.4728 sec/batch\n",
      "Epoch 39/40  Iteration 10656/11040 Training loss: 1.1524 0.4759 sec/batch\n",
      "Epoch 39/40  Iteration 10657/11040 Training loss: 1.1524 0.4754 sec/batch\n",
      "Epoch 39/40  Iteration 10658/11040 Training loss: 1.1524 0.4755 sec/batch\n",
      "Epoch 39/40  Iteration 10659/11040 Training loss: 1.1527 0.4750 sec/batch\n",
      "Epoch 39/40  Iteration 10660/11040 Training loss: 1.1528 0.4753 sec/batch\n",
      "Epoch 39/40  Iteration 10661/11040 Training loss: 1.1528 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10662/11040 Training loss: 1.1528 0.4747 sec/batch\n",
      "Epoch 39/40  Iteration 10663/11040 Training loss: 1.1528 0.4761 sec/batch\n",
      "Epoch 39/40  Iteration 10664/11040 Training loss: 1.1528 0.4735 sec/batch\n",
      "Epoch 39/40  Iteration 10665/11040 Training loss: 1.1529 0.4768 sec/batch\n",
      "Epoch 39/40  Iteration 10666/11040 Training loss: 1.1529 0.4926 sec/batch\n",
      "Epoch 39/40  Iteration 10667/11040 Training loss: 1.1531 0.4731 sec/batch\n",
      "Epoch 39/40  Iteration 10668/11040 Training loss: 1.1531 0.4767 sec/batch\n",
      "Epoch 39/40  Iteration 10669/11040 Training loss: 1.1531 0.4760 sec/batch\n",
      "Epoch 39/40  Iteration 10670/11040 Training loss: 1.1532 0.4738 sec/batch\n",
      "Epoch 39/40  Iteration 10671/11040 Training loss: 1.1531 0.4783 sec/batch\n",
      "Epoch 39/40  Iteration 10672/11040 Training loss: 1.1533 0.4730 sec/batch\n",
      "Epoch 39/40  Iteration 10673/11040 Training loss: 1.1534 0.4899 sec/batch\n",
      "Epoch 39/40  Iteration 10674/11040 Training loss: 1.1535 0.4693 sec/batch\n",
      "Epoch 39/40  Iteration 10675/11040 Training loss: 1.1537 0.4740 sec/batch\n",
      "Epoch 39/40  Iteration 10676/11040 Training loss: 1.1537 0.4761 sec/batch\n",
      "Epoch 39/40  Iteration 10677/11040 Training loss: 1.1537 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10678/11040 Training loss: 1.1537 0.4770 sec/batch\n",
      "Epoch 39/40  Iteration 10679/11040 Training loss: 1.1535 0.4900 sec/batch\n",
      "Epoch 39/40  Iteration 10680/11040 Training loss: 1.1535 0.4765 sec/batch\n",
      "Epoch 39/40  Iteration 10681/11040 Training loss: 1.1535 0.4895 sec/batch\n",
      "Epoch 39/40  Iteration 10682/11040 Training loss: 1.1535 0.4761 sec/batch\n",
      "Epoch 39/40  Iteration 10683/11040 Training loss: 1.1537 0.4800 sec/batch\n",
      "Epoch 39/40  Iteration 10684/11040 Training loss: 1.1536 0.4691 sec/batch\n",
      "Epoch 39/40  Iteration 10685/11040 Training loss: 1.1537 0.4760 sec/batch\n",
      "Epoch 39/40  Iteration 10686/11040 Training loss: 1.1537 0.4917 sec/batch\n",
      "Epoch 39/40  Iteration 10687/11040 Training loss: 1.1539 0.4914 sec/batch\n",
      "Epoch 39/40  Iteration 10688/11040 Training loss: 1.1540 0.4898 sec/batch\n",
      "Epoch 39/40  Iteration 10689/11040 Training loss: 1.1542 0.4916 sec/batch\n",
      "Epoch 39/40  Iteration 10690/11040 Training loss: 1.1543 0.4930 sec/batch\n",
      "Epoch 39/40  Iteration 10691/11040 Training loss: 1.1545 0.4759 sec/batch\n",
      "Epoch 39/40  Iteration 10692/11040 Training loss: 1.1546 0.4768 sec/batch\n",
      "Epoch 39/40  Iteration 10693/11040 Training loss: 1.1546 0.4747 sec/batch\n",
      "Epoch 39/40  Iteration 10694/11040 Training loss: 1.1546 0.4768 sec/batch\n",
      "Epoch 39/40  Iteration 10695/11040 Training loss: 1.1546 0.4771 sec/batch\n",
      "Epoch 39/40  Iteration 10696/11040 Training loss: 1.1547 0.4900 sec/batch\n",
      "Epoch 39/40  Iteration 10697/11040 Training loss: 1.1545 0.4918 sec/batch\n",
      "Epoch 39/40  Iteration 10698/11040 Training loss: 1.1544 0.4781 sec/batch\n",
      "Epoch 39/40  Iteration 10699/11040 Training loss: 1.1544 0.4730 sec/batch\n",
      "Epoch 39/40  Iteration 10700/11040 Training loss: 1.1544 0.4815 sec/batch\n",
      "Epoch 39/40  Iteration 10701/11040 Training loss: 1.1543 0.4854 sec/batch\n",
      "Epoch 39/40  Iteration 10702/11040 Training loss: 1.1543 0.4766 sec/batch\n",
      "Epoch 39/40  Iteration 10703/11040 Training loss: 1.1542 0.4732 sec/batch\n",
      "Epoch 39/40  Iteration 10704/11040 Training loss: 1.1540 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10705/11040 Training loss: 1.1539 0.4754 sec/batch\n",
      "Epoch 39/40  Iteration 10706/11040 Training loss: 1.1538 0.4751 sec/batch\n",
      "Epoch 39/40  Iteration 10707/11040 Training loss: 1.1538 0.4748 sec/batch\n",
      "Epoch 39/40  Iteration 10708/11040 Training loss: 1.1537 0.4607 sec/batch\n",
      "Epoch 39/40  Iteration 10709/11040 Training loss: 1.1535 0.4599 sec/batch\n",
      "Epoch 39/40  Iteration 10710/11040 Training loss: 1.1534 0.4580 sec/batch\n",
      "Epoch 39/40  Iteration 10711/11040 Training loss: 1.1533 0.4574 sec/batch\n",
      "Epoch 39/40  Iteration 10712/11040 Training loss: 1.1534 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10713/11040 Training loss: 1.1533 0.4760 sec/batch\n",
      "Epoch 39/40  Iteration 10714/11040 Training loss: 1.1534 0.4592 sec/batch\n",
      "Epoch 39/40  Iteration 10715/11040 Training loss: 1.1533 0.4584 sec/batch\n",
      "Epoch 39/40  Iteration 10716/11040 Training loss: 1.1532 0.4751 sec/batch\n",
      "Epoch 39/40  Iteration 10717/11040 Training loss: 1.1532 0.4773 sec/batch\n",
      "Epoch 39/40  Iteration 10718/11040 Training loss: 1.1533 0.4722 sec/batch\n",
      "Epoch 39/40  Iteration 10719/11040 Training loss: 1.1533 0.4751 sec/batch\n",
      "Epoch 39/40  Iteration 10720/11040 Training loss: 1.1534 0.4588 sec/batch\n",
      "Epoch 39/40  Iteration 10721/11040 Training loss: 1.1533 0.4740 sec/batch\n",
      "Epoch 39/40  Iteration 10722/11040 Training loss: 1.1533 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10723/11040 Training loss: 1.1534 0.4750 sec/batch\n",
      "Epoch 39/40  Iteration 10724/11040 Training loss: 1.1535 0.4747 sec/batch\n",
      "Epoch 39/40  Iteration 10725/11040 Training loss: 1.1536 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10726/11040 Training loss: 1.1536 0.4590 sec/batch\n",
      "Epoch 39/40  Iteration 10727/11040 Training loss: 1.1538 0.4595 sec/batch\n",
      "Epoch 39/40  Iteration 10728/11040 Training loss: 1.1539 0.4740 sec/batch\n",
      "Epoch 39/40  Iteration 10729/11040 Training loss: 1.1540 0.4757 sec/batch\n",
      "Epoch 39/40  Iteration 10730/11040 Training loss: 1.1540 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10731/11040 Training loss: 1.1540 0.4730 sec/batch\n",
      "Epoch 39/40  Iteration 10732/11040 Training loss: 1.1541 0.4606 sec/batch\n",
      "Epoch 39/40  Iteration 10733/11040 Training loss: 1.1540 0.4731 sec/batch\n",
      "Epoch 39/40  Iteration 10734/11040 Training loss: 1.1540 0.4726 sec/batch\n",
      "Epoch 39/40  Iteration 10735/11040 Training loss: 1.1540 0.4779 sec/batch\n",
      "Epoch 39/40  Iteration 10736/11040 Training loss: 1.1540 0.4731 sec/batch\n",
      "Epoch 39/40  Iteration 10737/11040 Training loss: 1.1541 0.4614 sec/batch\n",
      "Epoch 39/40  Iteration 10738/11040 Training loss: 1.1541 0.4763 sec/batch\n",
      "Epoch 39/40  Iteration 10739/11040 Training loss: 1.1540 0.4723 sec/batch\n",
      "Epoch 39/40  Iteration 10740/11040 Training loss: 1.1541 0.4586 sec/batch\n",
      "Epoch 39/40  Iteration 10741/11040 Training loss: 1.1541 0.4594 sec/batch\n",
      "Epoch 39/40  Iteration 10742/11040 Training loss: 1.1542 0.4789 sec/batch\n",
      "Epoch 39/40  Iteration 10743/11040 Training loss: 1.1542 0.4713 sec/batch\n",
      "Epoch 39/40  Iteration 10744/11040 Training loss: 1.1543 0.4754 sec/batch\n",
      "Epoch 39/40  Iteration 10745/11040 Training loss: 1.1544 0.4732 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40  Iteration 10746/11040 Training loss: 1.1544 0.4744 sec/batch\n",
      "Epoch 39/40  Iteration 10747/11040 Training loss: 1.1544 0.4765 sec/batch\n",
      "Epoch 39/40  Iteration 10748/11040 Training loss: 1.1545 0.4742 sec/batch\n",
      "Epoch 39/40  Iteration 10749/11040 Training loss: 1.1546 0.4604 sec/batch\n",
      "Epoch 39/40  Iteration 10750/11040 Training loss: 1.1548 0.4731 sec/batch\n",
      "Epoch 39/40  Iteration 10751/11040 Training loss: 1.1549 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10752/11040 Training loss: 1.1550 0.4604 sec/batch\n",
      "Epoch 39/40  Iteration 10753/11040 Training loss: 1.1550 0.4749 sec/batch\n",
      "Epoch 39/40  Iteration 10754/11040 Training loss: 1.1552 0.4752 sec/batch\n",
      "Epoch 39/40  Iteration 10755/11040 Training loss: 1.1554 0.4751 sec/batch\n",
      "Epoch 39/40  Iteration 10756/11040 Training loss: 1.1554 0.4588 sec/batch\n",
      "Epoch 39/40  Iteration 10757/11040 Training loss: 1.1555 0.4762 sec/batch\n",
      "Epoch 39/40  Iteration 10758/11040 Training loss: 1.1556 0.4733 sec/batch\n",
      "Epoch 39/40  Iteration 10759/11040 Training loss: 1.1560 0.4725 sec/batch\n",
      "Epoch 39/40  Iteration 10760/11040 Training loss: 1.1562 0.4779 sec/batch\n",
      "Epoch 39/40  Iteration 10761/11040 Training loss: 1.1563 0.4745 sec/batch\n",
      "Epoch 39/40  Iteration 10762/11040 Training loss: 1.1563 0.4744 sec/batch\n",
      "Epoch 39/40  Iteration 10763/11040 Training loss: 1.1564 0.4763 sec/batch\n",
      "Epoch 39/40  Iteration 10764/11040 Training loss: 1.1566 0.4756 sec/batch\n",
      "Epoch 40/40  Iteration 10765/11040 Training loss: 1.2386 0.4733 sec/batch\n",
      "Epoch 40/40  Iteration 10766/11040 Training loss: 1.2072 0.4762 sec/batch\n",
      "Epoch 40/40  Iteration 10767/11040 Training loss: 1.2004 0.4804 sec/batch\n",
      "Epoch 40/40  Iteration 10768/11040 Training loss: 1.1974 0.4638 sec/batch\n",
      "Epoch 40/40  Iteration 10769/11040 Training loss: 1.1936 0.4587 sec/batch\n",
      "Epoch 40/40  Iteration 10770/11040 Training loss: 1.1898 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10771/11040 Training loss: 1.1804 0.4747 sec/batch\n",
      "Epoch 40/40  Iteration 10772/11040 Training loss: 1.1757 0.4757 sec/batch\n",
      "Epoch 40/40  Iteration 10773/11040 Training loss: 1.1724 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10774/11040 Training loss: 1.1717 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 10775/11040 Training loss: 1.1669 0.4756 sec/batch\n",
      "Epoch 40/40  Iteration 10776/11040 Training loss: 1.1649 0.4746 sec/batch\n",
      "Epoch 40/40  Iteration 10777/11040 Training loss: 1.1622 0.4733 sec/batch\n",
      "Epoch 40/40  Iteration 10778/11040 Training loss: 1.1591 0.4768 sec/batch\n",
      "Epoch 40/40  Iteration 10779/11040 Training loss: 1.1575 0.4854 sec/batch\n",
      "Epoch 40/40  Iteration 10780/11040 Training loss: 1.1572 0.4766 sec/batch\n",
      "Epoch 40/40  Iteration 10781/11040 Training loss: 1.1542 0.4739 sec/batch\n",
      "Epoch 40/40  Iteration 10782/11040 Training loss: 1.1516 0.4745 sec/batch\n",
      "Epoch 40/40  Iteration 10783/11040 Training loss: 1.1507 0.4747 sec/batch\n",
      "Epoch 40/40  Iteration 10784/11040 Training loss: 1.1509 0.4901 sec/batch\n",
      "Epoch 40/40  Iteration 10785/11040 Training loss: 1.1524 0.4780 sec/batch\n",
      "Epoch 40/40  Iteration 10786/11040 Training loss: 1.1529 0.4727 sec/batch\n",
      "Epoch 40/40  Iteration 10787/11040 Training loss: 1.1520 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10788/11040 Training loss: 1.1528 0.4719 sec/batch\n",
      "Epoch 40/40  Iteration 10789/11040 Training loss: 1.1522 0.4634 sec/batch\n",
      "Epoch 40/40  Iteration 10790/11040 Training loss: 1.1517 0.4767 sec/batch\n",
      "Epoch 40/40  Iteration 10791/11040 Training loss: 1.1511 0.4723 sec/batch\n",
      "Epoch 40/40  Iteration 10792/11040 Training loss: 1.1509 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10793/11040 Training loss: 1.1510 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10794/11040 Training loss: 1.1510 0.4754 sec/batch\n",
      "Epoch 40/40  Iteration 10795/11040 Training loss: 1.1508 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10796/11040 Training loss: 1.1506 0.4755 sec/batch\n",
      "Epoch 40/40  Iteration 10797/11040 Training loss: 1.1496 0.4902 sec/batch\n",
      "Epoch 40/40  Iteration 10798/11040 Training loss: 1.1491 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10799/11040 Training loss: 1.1488 0.4755 sec/batch\n",
      "Epoch 40/40  Iteration 10800/11040 Training loss: 1.1492 0.4749 sec/batch\n",
      "Epoch 40/40  Iteration 10801/11040 Training loss: 1.1491 0.4754 sec/batch\n",
      "Epoch 40/40  Iteration 10802/11040 Training loss: 1.1484 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10803/11040 Training loss: 1.1479 0.4746 sec/batch\n",
      "Epoch 40/40  Iteration 10804/11040 Training loss: 1.1480 0.4605 sec/batch\n",
      "Epoch 40/40  Iteration 10805/11040 Training loss: 1.1477 0.4757 sec/batch\n",
      "Epoch 40/40  Iteration 10806/11040 Training loss: 1.1469 0.4734 sec/batch\n",
      "Epoch 40/40  Iteration 10807/11040 Training loss: 1.1473 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10808/11040 Training loss: 1.1461 0.4866 sec/batch\n",
      "Epoch 40/40  Iteration 10809/11040 Training loss: 1.1456 0.4665 sec/batch\n",
      "Epoch 40/40  Iteration 10810/11040 Training loss: 1.1448 0.4733 sec/batch\n",
      "Epoch 40/40  Iteration 10811/11040 Training loss: 1.1440 0.4745 sec/batch\n",
      "Epoch 40/40  Iteration 10812/11040 Training loss: 1.1439 0.4774 sec/batch\n",
      "Epoch 40/40  Iteration 10813/11040 Training loss: 1.1434 0.4735 sec/batch\n",
      "Epoch 40/40  Iteration 10814/11040 Training loss: 1.1441 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10815/11040 Training loss: 1.1438 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10816/11040 Training loss: 1.1436 0.4765 sec/batch\n",
      "Epoch 40/40  Iteration 10817/11040 Training loss: 1.1439 0.4737 sec/batch\n",
      "Epoch 40/40  Iteration 10818/11040 Training loss: 1.1437 0.4757 sec/batch\n",
      "Epoch 40/40  Iteration 10819/11040 Training loss: 1.1436 0.4745 sec/batch\n",
      "Epoch 40/40  Iteration 10820/11040 Training loss: 1.1436 0.4762 sec/batch\n",
      "Epoch 40/40  Iteration 10821/11040 Training loss: 1.1434 0.4759 sec/batch\n",
      "Epoch 40/40  Iteration 10822/11040 Training loss: 1.1432 0.4774 sec/batch\n",
      "Epoch 40/40  Iteration 10823/11040 Training loss: 1.1436 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10824/11040 Training loss: 1.1430 0.4761 sec/batch\n",
      "Epoch 40/40  Iteration 10825/11040 Training loss: 1.1430 0.4748 sec/batch\n",
      "Epoch 40/40  Iteration 10826/11040 Training loss: 1.1429 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10827/11040 Training loss: 1.1430 0.4764 sec/batch\n",
      "Epoch 40/40  Iteration 10828/11040 Training loss: 1.1431 0.4750 sec/batch\n",
      "Epoch 40/40  Iteration 10829/11040 Training loss: 1.1435 0.4727 sec/batch\n",
      "Epoch 40/40  Iteration 10830/11040 Training loss: 1.1434 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10831/11040 Training loss: 1.1435 0.4757 sec/batch\n",
      "Epoch 40/40  Iteration 10832/11040 Training loss: 1.1435 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 10833/11040 Training loss: 1.1433 0.4728 sec/batch\n",
      "Epoch 40/40  Iteration 10834/11040 Training loss: 1.1431 0.4758 sec/batch\n",
      "Epoch 40/40  Iteration 10835/11040 Training loss: 1.1429 0.4779 sec/batch\n",
      "Epoch 40/40  Iteration 10836/11040 Training loss: 1.1424 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10837/11040 Training loss: 1.1419 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10838/11040 Training loss: 1.1417 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 10839/11040 Training loss: 1.1416 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10840/11040 Training loss: 1.1419 0.4743 sec/batch\n",
      "Epoch 40/40  Iteration 10841/11040 Training loss: 1.1416 0.4773 sec/batch\n",
      "Epoch 40/40  Iteration 10842/11040 Training loss: 1.1413 0.4916 sec/batch\n",
      "Epoch 40/40  Iteration 10843/11040 Training loss: 1.1412 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10844/11040 Training loss: 1.1412 0.4754 sec/batch\n",
      "Epoch 40/40  Iteration 10845/11040 Training loss: 1.1409 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10846/11040 Training loss: 1.1411 0.4747 sec/batch\n",
      "Epoch 40/40  Iteration 10847/11040 Training loss: 1.1411 0.4592 sec/batch\n",
      "Epoch 40/40  Iteration 10848/11040 Training loss: 1.1412 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10849/11040 Training loss: 1.1410 0.4756 sec/batch\n",
      "Epoch 40/40  Iteration 10850/11040 Training loss: 1.1410 0.4902 sec/batch\n",
      "Epoch 40/40  Iteration 10851/11040 Training loss: 1.1403 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10852/11040 Training loss: 1.1393 0.4750 sec/batch\n",
      "Epoch 40/40  Iteration 10853/11040 Training loss: 1.1393 0.4772 sec/batch\n",
      "Epoch 40/40  Iteration 10854/11040 Training loss: 1.1393 0.4725 sec/batch\n",
      "Epoch 40/40  Iteration 10855/11040 Training loss: 1.1393 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10856/11040 Training loss: 1.1392 0.4751 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40  Iteration 10857/11040 Training loss: 1.1390 0.4742 sec/batch\n",
      "Epoch 40/40  Iteration 10858/11040 Training loss: 1.1393 0.4770 sec/batch\n",
      "Epoch 40/40  Iteration 10859/11040 Training loss: 1.1397 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10860/11040 Training loss: 1.1400 0.4723 sec/batch\n",
      "Epoch 40/40  Iteration 10861/11040 Training loss: 1.1406 0.4755 sec/batch\n",
      "Epoch 40/40  Iteration 10862/11040 Training loss: 1.1409 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 10863/11040 Training loss: 1.1412 0.4901 sec/batch\n",
      "Epoch 40/40  Iteration 10864/11040 Training loss: 1.1410 0.4774 sec/batch\n",
      "Epoch 40/40  Iteration 10865/11040 Training loss: 1.1412 0.4722 sec/batch\n",
      "Epoch 40/40  Iteration 10866/11040 Training loss: 1.1415 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 10867/11040 Training loss: 1.1412 0.4613 sec/batch\n",
      "Epoch 40/40  Iteration 10868/11040 Training loss: 1.1415 0.4725 sec/batch\n",
      "Epoch 40/40  Iteration 10869/11040 Training loss: 1.1414 0.4903 sec/batch\n",
      "Epoch 40/40  Iteration 10870/11040 Training loss: 1.1412 0.4748 sec/batch\n",
      "Epoch 40/40  Iteration 10871/11040 Training loss: 1.1410 0.4935 sec/batch\n",
      "Epoch 40/40  Iteration 10872/11040 Training loss: 1.1412 0.4747 sec/batch\n",
      "Epoch 40/40  Iteration 10873/11040 Training loss: 1.1415 0.4772 sec/batch\n",
      "Epoch 40/40  Iteration 10874/11040 Training loss: 1.1415 0.4719 sec/batch\n",
      "Epoch 40/40  Iteration 10875/11040 Training loss: 1.1416 0.4819 sec/batch\n",
      "Epoch 40/40  Iteration 10876/11040 Training loss: 1.1417 0.4872 sec/batch\n",
      "Epoch 40/40  Iteration 10877/11040 Training loss: 1.1418 0.4913 sec/batch\n",
      "Epoch 40/40  Iteration 10878/11040 Training loss: 1.1418 0.4740 sec/batch\n",
      "Epoch 40/40  Iteration 10879/11040 Training loss: 1.1419 0.4769 sec/batch\n",
      "Epoch 40/40  Iteration 10880/11040 Training loss: 1.1422 0.4739 sec/batch\n",
      "Epoch 40/40  Iteration 10881/11040 Training loss: 1.1421 0.4935 sec/batch\n",
      "Epoch 40/40  Iteration 10882/11040 Training loss: 1.1423 0.4740 sec/batch\n",
      "Epoch 40/40  Iteration 10883/11040 Training loss: 1.1425 0.4930 sec/batch\n",
      "Epoch 40/40  Iteration 10884/11040 Training loss: 1.1426 0.4896 sec/batch\n",
      "Epoch 40/40  Iteration 10885/11040 Training loss: 1.1426 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10886/11040 Training loss: 1.1427 0.4758 sec/batch\n",
      "Epoch 40/40  Iteration 10887/11040 Training loss: 1.1430 0.4764 sec/batch\n",
      "Epoch 40/40  Iteration 10888/11040 Training loss: 1.1435 0.4922 sec/batch\n",
      "Epoch 40/40  Iteration 10889/11040 Training loss: 1.1436 0.4892 sec/batch\n",
      "Epoch 40/40  Iteration 10890/11040 Training loss: 1.1439 0.4761 sec/batch\n",
      "Epoch 40/40  Iteration 10891/11040 Training loss: 1.1440 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10892/11040 Training loss: 1.1441 0.4839 sec/batch\n",
      "Epoch 40/40  Iteration 10893/11040 Training loss: 1.1439 0.4779 sec/batch\n",
      "Epoch 40/40  Iteration 10894/11040 Training loss: 1.1443 0.4900 sec/batch\n",
      "Epoch 40/40  Iteration 10895/11040 Training loss: 1.1444 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10896/11040 Training loss: 1.1443 0.4764 sec/batch\n",
      "Epoch 40/40  Iteration 10897/11040 Training loss: 1.1443 0.4776 sec/batch\n",
      "Epoch 40/40  Iteration 10898/11040 Training loss: 1.1442 0.4896 sec/batch\n",
      "Epoch 40/40  Iteration 10899/11040 Training loss: 1.1445 0.4825 sec/batch\n",
      "Epoch 40/40  Iteration 10900/11040 Training loss: 1.1445 0.4864 sec/batch\n",
      "Epoch 40/40  Iteration 10901/11040 Training loss: 1.1444 0.4930 sec/batch\n",
      "Epoch 40/40  Iteration 10902/11040 Training loss: 1.1445 0.4728 sec/batch\n",
      "Epoch 40/40  Iteration 10903/11040 Training loss: 1.1447 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10904/11040 Training loss: 1.1448 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10905/11040 Training loss: 1.1449 0.4764 sec/batch\n",
      "Epoch 40/40  Iteration 10906/11040 Training loss: 1.1450 0.4913 sec/batch\n",
      "Epoch 40/40  Iteration 10907/11040 Training loss: 1.1451 0.4764 sec/batch\n",
      "Epoch 40/40  Iteration 10908/11040 Training loss: 1.1449 0.4754 sec/batch\n",
      "Epoch 40/40  Iteration 10909/11040 Training loss: 1.1450 0.4758 sec/batch\n",
      "Epoch 40/40  Iteration 10910/11040 Training loss: 1.1449 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 10911/11040 Training loss: 1.1450 0.4767 sec/batch\n",
      "Epoch 40/40  Iteration 10912/11040 Training loss: 1.1450 0.4891 sec/batch\n",
      "Epoch 40/40  Iteration 10913/11040 Training loss: 1.1450 0.4755 sec/batch\n",
      "Epoch 40/40  Iteration 10914/11040 Training loss: 1.1451 0.4765 sec/batch\n",
      "Epoch 40/40  Iteration 10915/11040 Training loss: 1.1451 0.4894 sec/batch\n",
      "Epoch 40/40  Iteration 10916/11040 Training loss: 1.1454 0.4931 sec/batch\n",
      "Epoch 40/40  Iteration 10917/11040 Training loss: 1.1454 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10918/11040 Training loss: 1.1455 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10919/11040 Training loss: 1.1455 0.4771 sec/batch\n",
      "Epoch 40/40  Iteration 10920/11040 Training loss: 1.1455 0.4929 sec/batch\n",
      "Epoch 40/40  Iteration 10921/11040 Training loss: 1.1455 0.4884 sec/batch\n",
      "Epoch 40/40  Iteration 10922/11040 Training loss: 1.1456 0.4824 sec/batch\n",
      "Epoch 40/40  Iteration 10923/11040 Training loss: 1.1457 0.4858 sec/batch\n",
      "Epoch 40/40  Iteration 10924/11040 Training loss: 1.1458 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10925/11040 Training loss: 1.1458 0.4750 sec/batch\n",
      "Epoch 40/40  Iteration 10926/11040 Training loss: 1.1459 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10927/11040 Training loss: 1.1461 0.4789 sec/batch\n",
      "Epoch 40/40  Iteration 10928/11040 Training loss: 1.1462 0.4754 sec/batch\n",
      "Epoch 40/40  Iteration 10929/11040 Training loss: 1.1464 0.4887 sec/batch\n",
      "Epoch 40/40  Iteration 10930/11040 Training loss: 1.1468 0.4760 sec/batch\n",
      "Epoch 40/40  Iteration 10931/11040 Training loss: 1.1468 0.4768 sec/batch\n",
      "Epoch 40/40  Iteration 10932/11040 Training loss: 1.1467 0.4749 sec/batch\n",
      "Epoch 40/40  Iteration 10933/11040 Training loss: 1.1468 0.4894 sec/batch\n",
      "Epoch 40/40  Iteration 10934/11040 Training loss: 1.1469 0.4749 sec/batch\n",
      "Epoch 40/40  Iteration 10935/11040 Training loss: 1.1472 0.4773 sec/batch\n",
      "Epoch 40/40  Iteration 10936/11040 Training loss: 1.1473 0.4746 sec/batch\n",
      "Epoch 40/40  Iteration 10937/11040 Training loss: 1.1474 0.4764 sec/batch\n",
      "Epoch 40/40  Iteration 10938/11040 Training loss: 1.1474 0.4898 sec/batch\n",
      "Epoch 40/40  Iteration 10939/11040 Training loss: 1.1475 0.4770 sec/batch\n",
      "Epoch 40/40  Iteration 10940/11040 Training loss: 1.1474 0.4749 sec/batch\n",
      "Epoch 40/40  Iteration 10941/11040 Training loss: 1.1475 0.4726 sec/batch\n",
      "Epoch 40/40  Iteration 10942/11040 Training loss: 1.1475 0.4762 sec/batch\n",
      "Epoch 40/40  Iteration 10943/11040 Training loss: 1.1478 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10944/11040 Training loss: 1.1479 0.4739 sec/batch\n",
      "Epoch 40/40  Iteration 10945/11040 Training loss: 1.1479 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10946/11040 Training loss: 1.1480 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10947/11040 Training loss: 1.1479 0.4759 sec/batch\n",
      "Epoch 40/40  Iteration 10948/11040 Training loss: 1.1480 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10949/11040 Training loss: 1.1481 0.4775 sec/batch\n",
      "Epoch 40/40  Iteration 10950/11040 Training loss: 1.1482 0.4734 sec/batch\n",
      "Epoch 40/40  Iteration 10951/11040 Training loss: 1.1485 0.4733 sec/batch\n",
      "Epoch 40/40  Iteration 10952/11040 Training loss: 1.1484 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10953/11040 Training loss: 1.1484 0.4598 sec/batch\n",
      "Epoch 40/40  Iteration 10954/11040 Training loss: 1.1484 0.4743 sec/batch\n",
      "Epoch 40/40  Iteration 10955/11040 Training loss: 1.1483 0.4602 sec/batch\n",
      "Epoch 40/40  Iteration 10956/11040 Training loss: 1.1483 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10957/11040 Training loss: 1.1483 0.4767 sec/batch\n",
      "Epoch 40/40  Iteration 10958/11040 Training loss: 1.1483 0.4606 sec/batch\n",
      "Epoch 40/40  Iteration 10959/11040 Training loss: 1.1486 0.4729 sec/batch\n",
      "Epoch 40/40  Iteration 10960/11040 Training loss: 1.1484 0.4605 sec/batch\n",
      "Epoch 40/40  Iteration 10961/11040 Training loss: 1.1486 0.4589 sec/batch\n",
      "Epoch 40/40  Iteration 10962/11040 Training loss: 1.1486 0.4622 sec/batch\n",
      "Epoch 40/40  Iteration 10963/11040 Training loss: 1.1487 0.4708 sec/batch\n",
      "Epoch 40/40  Iteration 10964/11040 Training loss: 1.1488 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10965/11040 Training loss: 1.1490 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10966/11040 Training loss: 1.1491 0.4604 sec/batch\n",
      "Epoch 40/40  Iteration 10967/11040 Training loss: 1.1491 0.4741 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40  Iteration 10968/11040 Training loss: 1.1492 0.4595 sec/batch\n",
      "Epoch 40/40  Iteration 10969/11040 Training loss: 1.1492 0.4755 sec/batch\n",
      "Epoch 40/40  Iteration 10970/11040 Training loss: 1.1492 0.4719 sec/batch\n",
      "Epoch 40/40  Iteration 10971/11040 Training loss: 1.1492 0.4770 sec/batch\n",
      "Epoch 40/40  Iteration 10972/11040 Training loss: 1.1493 0.4749 sec/batch\n",
      "Epoch 40/40  Iteration 10973/11040 Training loss: 1.1491 0.4746 sec/batch\n",
      "Epoch 40/40  Iteration 10974/11040 Training loss: 1.1491 0.4639 sec/batch\n",
      "Epoch 40/40  Iteration 10975/11040 Training loss: 1.1491 0.4691 sec/batch\n",
      "Epoch 40/40  Iteration 10976/11040 Training loss: 1.1491 0.4762 sec/batch\n",
      "Epoch 40/40  Iteration 10977/11040 Training loss: 1.1491 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10978/11040 Training loss: 1.1491 0.4740 sec/batch\n",
      "Epoch 40/40  Iteration 10979/11040 Training loss: 1.1490 0.4743 sec/batch\n",
      "Epoch 40/40  Iteration 10980/11040 Training loss: 1.1489 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10981/11040 Training loss: 1.1487 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 10982/11040 Training loss: 1.1487 0.4603 sec/batch\n",
      "Epoch 40/40  Iteration 10983/11040 Training loss: 1.1486 0.4733 sec/batch\n",
      "Epoch 40/40  Iteration 10984/11040 Training loss: 1.1485 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10985/11040 Training loss: 1.1484 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10986/11040 Training loss: 1.1483 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 10987/11040 Training loss: 1.1482 0.4758 sec/batch\n",
      "Epoch 40/40  Iteration 10988/11040 Training loss: 1.1483 0.4900 sec/batch\n",
      "Epoch 40/40  Iteration 10989/11040 Training loss: 1.1482 0.4751 sec/batch\n",
      "Epoch 40/40  Iteration 10990/11040 Training loss: 1.1483 0.4626 sec/batch\n",
      "Epoch 40/40  Iteration 10991/11040 Training loss: 1.1481 0.4880 sec/batch\n",
      "Epoch 40/40  Iteration 10992/11040 Training loss: 1.1480 0.4744 sec/batch\n",
      "Epoch 40/40  Iteration 10993/11040 Training loss: 1.1481 0.4759 sec/batch\n",
      "Epoch 40/40  Iteration 10994/11040 Training loss: 1.1482 0.4741 sec/batch\n",
      "Epoch 40/40  Iteration 10995/11040 Training loss: 1.1482 0.4761 sec/batch\n",
      "Epoch 40/40  Iteration 10996/11040 Training loss: 1.1482 0.4595 sec/batch\n",
      "Epoch 40/40  Iteration 10997/11040 Training loss: 1.1482 0.4745 sec/batch\n",
      "Epoch 40/40  Iteration 10998/11040 Training loss: 1.1481 0.4885 sec/batch\n",
      "Epoch 40/40  Iteration 10999/11040 Training loss: 1.1482 0.4773 sec/batch\n",
      "Epoch 40/40  Iteration 11000/11040 Training loss: 1.1484 0.4773 sec/batch\n",
      "Validation loss: 1.23745 Saving checkpoint!\n",
      "Epoch 40/40  Iteration 11001/11040 Training loss: 1.1492 0.4688 sec/batch\n",
      "Epoch 40/40  Iteration 11002/11040 Training loss: 1.1493 0.4834 sec/batch\n",
      "Epoch 40/40  Iteration 11003/11040 Training loss: 1.1494 0.4700 sec/batch\n",
      "Epoch 40/40  Iteration 11004/11040 Training loss: 1.1496 0.4832 sec/batch\n",
      "Epoch 40/40  Iteration 11005/11040 Training loss: 1.1498 0.4835 sec/batch\n",
      "Epoch 40/40  Iteration 11006/11040 Training loss: 1.1498 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 11007/11040 Training loss: 1.1498 0.4594 sec/batch\n",
      "Epoch 40/40  Iteration 11008/11040 Training loss: 1.1499 0.4640 sec/batch\n",
      "Epoch 40/40  Iteration 11009/11040 Training loss: 1.1498 0.4705 sec/batch\n",
      "Epoch 40/40  Iteration 11010/11040 Training loss: 1.1498 0.4732 sec/batch\n",
      "Epoch 40/40  Iteration 11011/11040 Training loss: 1.1499 0.4619 sec/batch\n",
      "Epoch 40/40  Iteration 11012/11040 Training loss: 1.1499 0.4732 sec/batch\n",
      "Epoch 40/40  Iteration 11013/11040 Training loss: 1.1499 0.4753 sec/batch\n",
      "Epoch 40/40  Iteration 11014/11040 Training loss: 1.1499 0.4748 sec/batch\n",
      "Epoch 40/40  Iteration 11015/11040 Training loss: 1.1499 0.4609 sec/batch\n",
      "Epoch 40/40  Iteration 11016/11040 Training loss: 1.1499 0.4735 sec/batch\n",
      "Epoch 40/40  Iteration 11017/11040 Training loss: 1.1500 0.4742 sec/batch\n",
      "Epoch 40/40  Iteration 11018/11040 Training loss: 1.1500 0.4758 sec/batch\n",
      "Epoch 40/40  Iteration 11019/11040 Training loss: 1.1501 0.4743 sec/batch\n",
      "Epoch 40/40  Iteration 11020/11040 Training loss: 1.1501 0.4759 sec/batch\n",
      "Epoch 40/40  Iteration 11021/11040 Training loss: 1.1502 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 11022/11040 Training loss: 1.1502 0.4756 sec/batch\n",
      "Epoch 40/40  Iteration 11023/11040 Training loss: 1.1502 0.4756 sec/batch\n",
      "Epoch 40/40  Iteration 11024/11040 Training loss: 1.1503 0.4735 sec/batch\n",
      "Epoch 40/40  Iteration 11025/11040 Training loss: 1.1503 0.4745 sec/batch\n",
      "Epoch 40/40  Iteration 11026/11040 Training loss: 1.1506 0.4763 sec/batch\n",
      "Epoch 40/40  Iteration 11027/11040 Training loss: 1.1507 0.4755 sec/batch\n",
      "Epoch 40/40  Iteration 11028/11040 Training loss: 1.1508 0.4731 sec/batch\n",
      "Epoch 40/40  Iteration 11029/11040 Training loss: 1.1508 0.4628 sec/batch\n",
      "Epoch 40/40  Iteration 11030/11040 Training loss: 1.1510 0.4714 sec/batch\n",
      "Epoch 40/40  Iteration 11031/11040 Training loss: 1.1512 0.4742 sec/batch\n",
      "Epoch 40/40  Iteration 11032/11040 Training loss: 1.1513 0.4769 sec/batch\n",
      "Epoch 40/40  Iteration 11033/11040 Training loss: 1.1514 0.4893 sec/batch\n",
      "Epoch 40/40  Iteration 11034/11040 Training loss: 1.1515 0.4748 sec/batch\n",
      "Epoch 40/40  Iteration 11035/11040 Training loss: 1.1518 0.4612 sec/batch\n",
      "Epoch 40/40  Iteration 11036/11040 Training loss: 1.1521 0.4675 sec/batch\n",
      "Epoch 40/40  Iteration 11037/11040 Training loss: 1.1522 0.4856 sec/batch\n",
      "Epoch 40/40  Iteration 11038/11040 Training loss: 1.1523 0.4864 sec/batch\n",
      "Epoch 40/40  Iteration 11039/11040 Training loss: 1.1524 0.4752 sec/batch\n",
      "Epoch 40/40  Iteration 11040/11040 Training loss: 1.1525 0.4741 sec/batch\n",
      "Validation loss: 1.23291 Saving checkpoint!\n",
      "40 epoch done in 92.62816666666666 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # always measure the time\n",
    "epochs = 40\n",
    "save_every_n = 500\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/{}/train'.format(folder_name), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/{}/test'.format(folder_name))\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/lyr20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1 \n",
    "                # because this is validation, not training, so we need everything\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/{}/i{}_l{}_{:.3f}.ckpt\".format(folder_name, iteration, lstm_size, np.mean(val_loss)))\n",
    "\n",
    "print(epochs, \"epoch done in\", round(time.time() - start_time, 2)/60, \"mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the loss graph,\n",
    "<img src=\"assets/loss_graph.JPG\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "No test here, because I'm not predicting house prices. The model is to generate, not to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i11040_l512_1.233.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i500_l512_1.944.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i1000_l512_1.637.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i1500_l512_1.501.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i2000_l512_1.428.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i2500_l512_1.378.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i3000_l512_1.353.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i3500_l512_1.333.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i4000_l512_1.313.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i4500_l512_1.298.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i5000_l512_1.285.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i5500_l512_1.276.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i6000_l512_1.270.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i6500_l512_1.263.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i7000_l512_1.256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i7500_l512_1.248.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i8000_l512_1.247.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i8500_l512_1.246.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i9000_l512_1.241.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i9500_l512_1.242.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i10000_l512_1.236.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i10500_l512_1.231.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i11000_l512_1.237.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/memesljuirmoblteovanpaicmaanexdr\\\\i11040_l512_1.233.ckpt\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Skin And The Stand\n",
      "The Painkiller\n",
      "The core was all the truth\n",
      "\n",
      "There's no tomorrow, will never live to stop\n",
      "The world is a conscience\n",
      "\n",
      "\n",
      "\n",
      "Trapped us to the blood and the son and stare\n",
      "The sun is calling to you\n",
      "And the street of tomorrow's bleeding and darkness\n",
      "Where the cores of the desires too much\n",
      "I would be a shot or start to stone by the sky\n",
      "\n",
      "I walk the earth to strength when the world's so falling\n",
      "In the stars too long to the streets\n",
      "And the fallen calls come for the same as they say the will to see\n",
      "\n",
      "In my heart, I was a lie\n",
      "The face that I see to save my sanity\n",
      "\n",
      "I can't see my hands around me\n",
      "\n",
      "I'm a most soul is all I want and I don't wanna see me anymore\n",
      "\n",
      "\n",
      "There are the same and then the\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/{}/i10500_l512_1.231.ckpt\".format(folder_name)\n",
    "samp = sample(checkpoint, 700, lstm_size, len(vocab), vocab_to_int, vocab, int_to_vocab, prime=\"The \", top_n=3)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highway to Hell\n",
      "I am running ran away\n",
      "\n",
      "In the night that search out\n",
      "And the life I do is real\n",
      "The sin I bring you where they soul\n",
      "That he's suck a shame of someone else\n",
      "I did to go down the fight\n",
      "But they arconged straight one disgrace\n",
      "\n",
      "I am the face that I'm going to\n",
      "The fire in bed with someone else around\n",
      "\n",
      "I walk always that you don't know it\n",
      "If I can't take it any more again\n",
      "The world is goena look behind all the storm\n",
      "Why are you're feelings\n",
      "In my sense that someone should haad\n",
      "I'll never live the other day\n",
      "It's my godna take that wounds\n",
      "In the heart and that's all the way\n",
      "This is the feot your sins of minutes\n",
      "This is my war is all you'd die\n",
      "\n",
      "And I'm going to be tile the proce is where you call\n",
      "All thi\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/{}/i10020_l512_0.682.ckpt\".format(folder_name)\n",
    "samp = sample(checkpoint, 700, lstm_size, len(vocab), vocab_to_int, vocab, int_to_vocab, prime=\"Highway to Hell\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "- [Udacity Deep Learning](https://github.com/udacity/deep-learning)  \n",
    "- [Mat Leonard](https://github.com/mcleonard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
