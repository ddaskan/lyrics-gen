{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle # to save/load objects\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../helper/\") # go to parent dir\n",
    "from functions_preprocess import *\n",
    "from functions_model import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Here, I'm getting, cleaning and processing the data. I intentionally mixed the bands with different characteristics. I thought this may prevent overfitting and provide better word pool. Data has the almost entire discography of the bands, even demos for some.  \n",
    "\n",
    "I'm also creating two dictionaries here, `vocab_to_int` and `int_to_vocab`. The model need numerical representation of the characters to be able to compute weights, biases, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw ready with 6292269 chars\n"
     ]
    }
   ],
   "source": [
    "# prepare training data\n",
    "artist_list = next(os.walk('../data/'))[1] # just need folder names, each named folders has albums\n",
    "\n",
    "text = ''\n",
    "folder_name = ''\n",
    "for i in artist_list:\n",
    "    text = text + combine_songs(i)\n",
    "    folder_name = folder_name + i[:1] # I know this's not the greatest folder name\n",
    "    \n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(folder_name, \"ready with\", len(chars), \"chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving\n",
    "I need to save the objects I created so far. Remember, no orders in Python dictionaries. This means this notebook creates different dictionaries in each session. Because I need the use my model later, I have to save these lookup dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the objects\n",
    "directory = 'checkpoints/{}'.format(folder_name)\n",
    "if not os.path.exists(directory): os.makedirs(directory) # create folder first\n",
    "f = open('checkpoints/{}/vars.pckl'.format(folder_name), 'wb')\n",
    "pickle.dump([vocab, vocab_to_int, int_to_vocab, artist_list], f, protocol=2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob_train = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly encourage you to check the function `split_data` in `functions_model.py`. The target is the next character in the sequence.  \n",
    "`x = chars[: n_batches*slice_size]`  \n",
    "`y = chars[1: n_batches*slice_size + 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main graph of the following model from <a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\">TensorBoard</a>,\n",
    "<img src=\"assets/main_graph.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100  Iteration 1/56600 Training loss: 4.4892 0.8543 sec/batch\n",
      "Epoch 1/100  Iteration 2/56600 Training loss: 4.4489 0.4667 sec/batch\n",
      "Epoch 1/100  Iteration 3/56600 Training loss: 4.2830 0.4863 sec/batch\n",
      "Epoch 1/100  Iteration 4/56600 Training loss: 4.4983 0.4636 sec/batch\n",
      "Epoch 1/100  Iteration 5/56600 Training loss: 4.4387 0.4756 sec/batch\n",
      "Epoch 1/100  Iteration 6/56600 Training loss: 4.3224 0.4835 sec/batch\n",
      "Epoch 1/100  Iteration 7/56600 Training loss: 4.2203 0.4727 sec/batch\n",
      "Epoch 1/100  Iteration 8/56600 Training loss: 4.1332 0.4716 sec/batch\n",
      "Epoch 1/100  Iteration 9/56600 Training loss: 4.0553 0.4653 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100  Iteration 48899/56600 Training loss: 0.9747 0.4630 sec/batch\n",
      "Epoch 87/100  Iteration 48900/56600 Training loss: 0.9748 0.4683 sec/batch\n",
      "Epoch 87/100  Iteration 48901/56600 Training loss: 0.9750 0.4738 sec/batch\n",
      "Epoch 87/100  Iteration 48902/56600 Training loss: 0.9751 0.4800 sec/batch\n",
      "Epoch 87/100  Iteration 48903/56600 Training loss: 0.9753 0.4710 sec/batch\n",
      "Epoch 87/100  Iteration 48904/56600 Training loss: 0.9752 0.4912 sec/batch\n",
      "Epoch 87/100  Iteration 48905/56600 Training loss: 0.9752 0.4788 sec/batch\n",
      "Epoch 87/100  Iteration 48906/56600 Training loss: 0.9754 0.4788 sec/batch\n",
      "Epoch 87/100  Iteration 48907/56600 Training loss: 0.9756 0.4905 sec/batch\n",
      "Epoch 87/100  Iteration 48908/56600 Training loss: 0.9757 0.4844 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100  Iteration 56585/56600 Training loss: 0.9589 0.4788 sec/batch\n",
      "Epoch 100/100  Iteration 56586/56600 Training loss: 0.9589 0.4794 sec/batch\n",
      "Epoch 100/100  Iteration 56587/56600 Training loss: 0.9589 0.4902 sec/batch\n",
      "Epoch 100/100  Iteration 56588/56600 Training loss: 0.9589 0.4791 sec/batch\n",
      "Epoch 100/100  Iteration 56589/56600 Training loss: 0.9588 0.4787 sec/batch\n",
      "Epoch 100/100  Iteration 56590/56600 Training loss: 0.9588 0.4844 sec/batch\n",
      "Epoch 100/100  Iteration 56591/56600 Training loss: 0.9588 0.4716 sec/batch\n",
      "Epoch 100/100  Iteration 56592/56600 Training loss: 0.9589 0.4811 sec/batch\n",
      "Epoch 100/100  Iteration 56593/56600 Training loss: 0.9589 0.4704 sec/batch\n",
      "Epoch 100/100  Iteration 56594/56600 Training loss: 0.9589 0.4692 sec/batch\n",
      "Epoch 100/100  Iteration 56595/56600 Training loss: 0.9590 0.4716 sec/batch\n",
      "Epoch 100/100  Iteration 56596/56600 Training loss: 0.9590 0.4817 sec/batch\n",
      "Epoch 100/100  Iteration 56597/56600 Training loss: 0.9590 0.4730 sec/batch\n",
      "Epoch 100/100  Iteration 56598/56600 Training loss: 0.9590 0.4863 sec/batch\n",
      "Epoch 100/100  Iteration 56599/56600 Training loss: 0.9591 0.4787 sec/batch\n",
      "Epoch 100/100  Iteration 56600/56600 Training loss: 0.9592 0.4799 sec/batch\n",
      "Validation loss: 1.20624 Saving checkpoint!\n",
      "100 epoch done in 466.36116666666663 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # always measure the time\n",
    "epochs = 100\n",
    "save_every_n = 2000\n",
    "# Split training and validation sets\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps) # default fraction for trainning is 0.9\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/{}/train'.format(folder_name), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/{}/test'.format(folder_name))\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/lyr20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob_train,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1 \n",
    "                # because this is validation, not training, so we need everything\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/{}/i{}_l{}_{:.3f}.ckpt\".format(folder_name, iteration, lstm_size, np.mean(val_loss)))\n",
    "\n",
    "print(epochs, \"epoch done in\", round(time.time() - start_time, 2)/60, \"mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the loss graph,\n",
    "<img src=\"assets/loss_graph2.JPG\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "No test here, because I'm not predicting house prices. The model is to generate, not to predict. However, the validation loss is a good indicater about how well the model behaves. And, looks like I already got the best model almost in the middle, I waited for ~200 minutes just to know no need go more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i56600_l512_1.206.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i2000_l512_1.425.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i4000_l512_1.303.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i6000_l512_1.260.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i8000_l512_1.236.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i10000_l512_1.227.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i12000_l512_1.218.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i14000_l512_1.213.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i16000_l512_1.211.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i18000_l512_1.204.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i20000_l512_1.202.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i22000_l512_1.204.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i24000_l512_1.207.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i26000_l512_1.215.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i28000_l512_1.208.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i30000_l512_1.201.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i32000_l512_1.202.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i34000_l512_1.195.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i36000_l512_1.204.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i38000_l512_1.207.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i40000_l512_1.207.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i42000_l512_1.210.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i44000_l512_1.207.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i46000_l512_1.201.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i48000_l512_1.208.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i50000_l512_1.199.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i52000_l512_1.205.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i54000_l512_1.209.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i56000_l512_1.208.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/aaaaabbcdddeeefhiiijkkmmmmnnoopssssssttw\\\\i56600_l512_1.206.ckpt\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints/{}'.format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Stench Of With Hell\n",
      "\n",
      "I am your land\n",
      "I am the path to save me\n",
      "The sorrow, I hear your fate\n",
      "I will never know,\n",
      "I cannot hope you I will die again\n",
      "The one to the other side\n",
      "\n",
      "As I still defend the writings\n",
      "On my soul to search for me\n",
      "And in a dream I'm the masquerade\n",
      "This is the final scene\n",
      "\n",
      "Too many stories of the lies\n",
      "They're too many shouts to see\n",
      "They set in our holy lead\n",
      "And the checks of summer waits\n",
      "\n",
      "The chosen cast in soul of an our own divide\n",
      "The stench of far to stark on far away\n",
      "And who's a thousand to be free again\n",
      "\n",
      "When all the thoughts won't haunt me\n",
      "And I should be trying to give up\n",
      "And then this is my last\n",
      "I can never see\n",
      "And now I feel so far\n",
      "They come along the way\n",
      "They will sur\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/{}/i34000_l512_1.195.ckpt\".format(folder_name)\n",
    "samp = sample(checkpoint, 700, lstm_size, len(vocab), vocab_to_int, vocab, int_to_vocab, prime=\"The \", top_n=4)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "- [Udacity Deep Learning](https://github.com/udacity/deep-learning)  \n",
    "- [Mat Leonard](https://github.com/mcleonard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
